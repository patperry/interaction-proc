\documentclass[aoas,preprint]{imsart}
\pdfoutput=1    % Make arXiv happy

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath,amssymb}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{hypernat}
\RequirePackage{graphicx}
\RequirePackage{enumerate}
\RequirePackage{booktabs}
\belowbottomsep=0.5em

\setattribute{journal}{name}{}  % Suppress text "Submitted to..."

\startlocaldefs
\usepackage{iproc-macros}
\endlocaldefs


\begin{document}

\begin{frontmatter}

\title{
    A model for repeated interactions with applications to email traffic
    analysis\protect\thanksref{T1}
}

\runtitle{A point process model for graphs}
\thankstext{T1}{Supported by grant\ldots}

\begin{aug}
    \author{%
        \fnms{Patrick O.} \snm{Perry}\corref{}%
        \ead[label=e1]{patperry@seas.harvard.edu}%
    }%
    \and%
    \author{%
        \fnms{Patrick J.} \snm{Wolfe}%
        \ead[label=e2]{wolfe@stat.harvard.edu}%
    }%

    \runauthor{P.\ O.\ Perry and P.\ J.\ Wolfe}

    \affiliation{Harvard University}

    \address{
        Statistics and Information Sciences Laboratory \\
        Harvard University \\
        33 Oxford Street \\
        Cambridge, MA 02138 \\
        \printead{e1}\\
        \phantom{E-mail:\ }\printead*{e2}
    }
\end{aug}

\begin{abstract}
Abstract.
\end{abstract}

\begin{keyword}[class=AMS]
    \kwd[Primary ]{62M30}     % Statistics:Inference from stochastic processes:Spatial processes
    \kwd[; secondary ]{62N01} % Statistics:Survival analysis and censored data:Censored data models
\end{keyword}

\begin{keyword}
    \kwd{random graphs}
    \kwd{networks}
    \kwd{point processes}
    \kwd{inference}
\end{keyword}

\end{frontmatter}


\section{Introduction}

Data derived from repeated pairwise interactions are becoming more common. 
The canonical examples are communications networks: either all of the phone
calls or the emails sent between individuals in a community
\cite{eagle2006reality,tyler2005email}. More broadly,
``pairwise interaction'' has been construed to include the following:

\begin{itemize}
    \item\textbf{animal association patterns:} zebras congregate at
    locations in their habitat \cite{sundaresan2007network};
    
    \item\textbf{gang homicide:} gangs in Chicago murder members of rival factions
    \cite{papachristos2009murder};

    \item\textbf{legislation cosponsorship:} a congressperson authors a bill, which
    gets sponsored by other legislators \cite{fowler2006connecting};

    \item\textbf{migration patterns:} families migrate between communities in
    Mexico \cite{mckenzie2007network};
\end{itemize}

For the current treatment, we consider directed interactions, with each
interaction having an initiator (sender) and at least one recipient
(receiver).  The above examples fall into this category.  To simplify
the development, initially we focus on the single-recipient case, so that
each interaction involves one sender-receiver pair, called a dyad.  We
borrow terminology from the communications setting, referring to
interactions as messages.

The data is a set of messages.  Each message is represented by a triple,
where $(t,i,j)$ denotes that at time $t$, sender $i$ sent a message to
receiver $j$.  Given a set of messages and covariate information about the
senders and receivers, our goal is to determine which characteristics and
behaviors are predictive of interaction.  At least two questions are
relevant:

\begin{enumerate}
    \item Is a shared attribute between individuals predictive of increased
    interaction?  In studying human behavior, Sociologists repeatedly find
    evidence of homophily, the tendency of individuals to associate with
    similar others \cite{mcpherson2001birds}. A typical inquiry is
    to see if people with identical genders interact more often then
    people with differing genders.

    \item If individual $i$ sends a message to individual $j$, is this
    associated with $j$ being more likely to send to $i$ in the future?
    If so, how does this effect decay with time?
\end{enumerate}

This report presents a modeling framework to facilitate inquiry into
these questions and others like them. The framework is flexible enough to
allow various forms of time-inhomogeneity and dependency, and includes the
ability to handle interactions with multiple recipients. 


\section{A Motivating Example}

Our motivating example is to test for gender bias in a corporate email
network.  The dataset we use is a large collection of email messages sent
within the Enron corporation during the last four years of the company's
existence.  Appendix~\ref{S:enron-corpus} describes the data collection and
preprocessing.  Briefly, there are 21,635 messages in the dataset, which
were sent between 156 executives in the company.  We know the genders,
seniorities, and departments of the employees (summarized in
Table~\ref{T:employee-summary}).

\begin{table}[h]
    \input{tables/employee-summary}
    \caption{
        Characteristics of the 156 employees in the Enron dataset.
    }
    \label{T:employee-summary}
\end{table}

Roughly 28\% of the employees are female, and 72\% are male.  If there were
no gender bias in the email behavior, we would expect these proportions to
agree with the counts of messages exchanged between gender groups.  This
is not the case, as Table~\ref{T:gender-send-counts} demonstrates.  
Apparently, bias is present in each group, associated with higher
within-group sending rates.

\begin{table}[h]
    \input{tables/gender-send-counts}
    \caption{
        Counts of the 21,635 messages sent between gender groups.  Messages
        with multiple recipients are duplicated so that, for example,
        a messages with five recipients counts as five single-recipient
        messages.  This increases the total count to 38,388.
    }
    \label{T:gender-send-counts}
\end{table}

If we treat the messages as independent identically-distributed observations,
then any reasonable test will find gender bias in both group-level behaviors.
Given that a female is sending a message, the probability that the recipient
is male will be estimated to be about 48\%, with a standard error around
0.4\%; given that a male is sending a message, the estimate will be about
23\% with a standard error around 0.2\%.

There are at least three problems with this simple analysis.
\begin{enumerate}
    \item \textbf{Senders never send messages to themselves.} In network
    terminology, there are no ``self-loops.'' In other email datasets, people
    do sometimes email themselves, but arguably these messages should be
    handled differently then messages exchanged between different individuals.
    
    \item \textbf{Messages are not independent of each other.} If I send
    you a message today, then you are more likely to send me a message
    tomorrow.  Any inherent gender bias will be exaggerated by
    reciprocation effects.
    
    \item \textbf{Messages have multiple recipients.} In the Enron dataset,
    more than 30\% of the messages have more than one recipient.  As noted in
    the caption to Table~\ref{T:gender-send-counts}, for the analysis above
    we have treated a message with two or more recipients as two or more
    single-recipient messages.
\end{enumerate}
The first issue is easy to fix, but the other two require significant
attention.

Another feature of the data potentially requiring special consideration is
that the employees have inhomogeneous sending patterns.  Fig~BLAH shows how
the volume varies with sender and time.

\section{A Point Process Model}\label{S:point-process-model}

Every interaction process can be encoded by a multivariate counting measure.
For sender, $i$, receiver, $j$, and positive time, $t$, we define
\[
    N_t(i,j)
        =
        \#\{\text{messages sent from $i$ to $j$ in time interval $[0,t]$}\}.
\]
We model $N$ through its stochastic intensity, $\lambda$, assumed to
exist and be predictable and continuous (``predictable'' means roughly that
it only depends on the immediate past).  Heuristically,
\[
    \lambda_t(i,j) \, dt
        =
        \mathbb{P}\{
            \text{$i$ sends $j$ a message in time interval $[t,t+dt)$}
        \}.
\]
The stochastic intensity exists
under mild conditions, the most important of which is that no two interactions
happen simultaneously.
We will use a version of the Cox Proportional Hazards model for modeling
the interaction process \cite{cox1972regression}.
For this section, assume that each message has a single recipient.



Let $\mathcal{I}$ be a set of senders and $\mathcal{J}$ be a set of
receivers.  
For each sender $i$, let $\bar \lambda_t(i)$ be a non-negative predictable
process called the baseline intensity of sender $i$; let
$\mathcal{J}_t(i)$ be a predictable subset of $\mathcal{J}$ called the 
recipient set of sender $i$.
For each sender-receiver pair $(i,j)$, let $x_t(i,j)$ be a locally bounded
predictable vector of covariates in $\reals^p$.  Let $\beta_0$ in
be an unknown vector of coefficients in  $\reals^p$.

Suppose $N$ is a multivariate counting process on 
$\reals_+ \times \mathcal{I} \times \mathcal{J}$,
with stochastic intensity
\begin{equation}\label{E:cox-intensity}
    \lambda_t(i,j)
        =
        \bar \lambda_t(i)
        \exp\{ \beta_0^\trans x_t(i, j) \}
        \cdot
        1{\{j \in \mathcal{J}_t(i)\}}.
\end{equation}
Assume $N_0(i,j) = 0$.  Define
\(
    N_t(i) = \sum_{j \in \mathcal{J}} N_t(i,j)
\)
and
\(
    N_t = \sum_{i \in \mathcal{I}} N_t(i).
\)

  
If $\{ (t_m, i_m, j_m) \}$ is the set of observed messages, then
the log partial likelihood at time $t$, evaluated at $\beta$, is
\begin{equation}\label{E:log-pl}
    \log
    \mathit{PL}_t(\beta)
        =
        \sum_{t_m \leq t}
        \bigg\{
            \beta^\trans x_{t_m}\!(i_m, j_m)
            -
            \log\big[
                \!\!\!\!
                \sum_{j \in \mathcal{J}_{t_m}\!(i_m)}
                    \exp\{ \beta^\trans x_{t_m}\!(i_m, j)\}
            \big]
        \bigg\}.
\end{equation}
Define weights
\begin{gather}
    w_{t}(\beta, i,j)
        =
        \exp\{ \beta^\trans x_t(i,j) \}
        \cdot
        1\{ j \in \mathcal{J}_t(i)\}, \\
    W_{t}(\beta, i)
        =
        \sum_{j \in \mathcal{J}} w_{t}(\beta, i,j).
\end{gather}
Note the inner sum in $\log \mathit{PL}_t(\beta)$ is
$W_{t_m}\!(\beta, i_m)$.  The function 
$\log W_{t}(\cdot, i)$ has gradient $E_{t}(\cdot, i)$ and Hessian 
$V_{t}(\cdot, i)$, given by
\begin{gather}
    E_{t}(\beta, i)
        =
        \frac{1}{W_{t}(\beta, i)}
        \sum_{j \in \mathcal{J}_t(i)}
            w_{t}(\beta, i,j) \, x_{t}(i,j), \\
    V_{t}(\beta, i)
        =
        \frac{1}{W_{t}(\beta, i)}
        \sum_{j \in \mathcal{J}_t(i)}
            w_{t}(\beta, i,j) 
            \Big[ x_{t}(i,j) - E_{t}(\beta, i)\Big]^{\otimes 2},
\end{gather}
where $a^{\otimes 2} = a \otimes a = a a^\trans$.
The gradient and negative Hessian of $\log \mathit{PL}_t(\cdot)$ are
\begin{gather}
    \label{E:log-pl-gradient}
    U_t(\beta)
        =
        \nabla \big[ \log \mathit{PL}_t(\beta) \big]
        =
        \sum_{t_m \leq t}
            x_{t_m}(i_m, j_m) - E_{t_m}(\beta, i_m), \\
    \label{E:log-pl-neg-hessian}            
    I_t(\beta)
        =
        -\nabla^2 \big[ \log \mathit{PL}_t(\beta) \big]
        =
        \sum_{t_m \leq t}
            V_{t_m}(\beta, i_m).
\end{gather}
We call $U_t(\beta_0)$ the unnormalized score and $I_t(\beta_0)$
the Fisher information matrix.  Let $\hat \beta_t$ be the value that
maximizes $\log \mathit{PL}_t(\cdot)$, called the maximum partial
likelihood estimator (MPLE). 

Typically, one proves consistency of $\hat \beta_t$ by keeping $t$ fixed
and letting $|\mathcal{I}|$ go to infinity
\cite{andersen1993statistical,andersen1982cox,cook2007statistical,fleming1991counting,
martinussen2006dynamic}.  This makes sense in the context of clinical trial
data, where $\mathcal{I}$ corresponds to the set of patients.  In interaction
data, $|\mathcal{I}|$ is either fixed, or $t$ and $|\mathcal{I}|$ increase
simultaneously.  It is more natural to look at what happens as time increases.
Increasing sender sets can be handled by assuming $|\mathcal{I}|$ is infinite
(for most senders $\bar \lambda_t(i)$ is zero during the initial
part of the observation period); increasing receiver sets can be
handled similarly.

The main idea of the consistency proof is to rescale time to make the message
arrivals uniform.  This is the main difference between our proof and
Andersen and Gill's original proof \cite{andersen1982cox}; otherwise, the
outline of the two are roughly the same.  Define the sequence of stopping
times
\begin{equation}\label{E:message-times}
    \tau_n = \sup\{ t : N_t < n \},
\end{equation}
and let $\mathcal{F}_{\tau_n}$ be the $\sigma$-algebra of events prior to
$\tau_n$. The idea is to change time from the original scale to a scale on
which $\tau_{n} - \tau_{n'}$ is proportional to $n - n'$.

Let $\mathcal{B}$ be a neighborhood of $\beta_0$.  For a vector, $a$, let
$\| a \|$ denote its Euclidean norm; for a matrix, $A$, let $\| A \|$ denote
its spectral norm (largest eigenvalue).  We will make the following assumptions:
\begin{enumerate}[{A}1.]
    \item \label{A:square-int}
    \textbf{The covariates are uniformly square-integrable.}  That is,
    \[
        \E\left[
            \sup_{t, i, j} \| x_{t}(i,j) \|^2
        \right]
        \,\,\text{is bounded.}
    \]
    \item \label{A:message-times-finite}
    \textbf{The message arrival times are finite.}  For each $n$,
    \[
        \mathbb{P}\{\tau_n < \infty\} = 1.
    \]
    \item \label{A:integrated-cov-limit}
    \textbf{The integrated covariance function is well-behaved.}
    When $\beta \in \mathcal{B}$ and $\alpha \in [0,1]$, as $ n \to \infty$,
    \[
        \frac{1}{n}
        \sum_{i \in \mathcal{I}}
        \int_0^{\tau_{\lfloor \alpha n \rfloor}}
            V_s(\beta,i)
            \, W_s(\beta,i)
            \, \bar \lambda(i)
            \, ds
        \toP
        \Sigma_\alpha(\beta).
    \]
    
    \item \label{A:var-equicont}
    \textbf{The variance function is equicontinuous.}
    More precisely,
    \[
        \Big\{
            V_{\tau_n}(\cdot, i)
            :
            n \geq 1, i \in \mathcal{I}
        \Big\}
    \]
    is equicontinuous.
\end{enumerate}

\begin{theorem}\label{T:score-fisher}
    Let $N$ be a multivariate counting process on with stochastic
    intensity as given in~\eqref{E:cox-intensity}, with true parameter
    vector $\beta_0$.  Let $\tau_n$ be the sequence of message arrival times
    defined in~\eqref{E:message-times}, set $U_t(\beta)$ and $I_t(\beta)$ to
    be the gradient and negative Hessian of the partial likelihood function
    as given in~(\ref{E:log-pl-gradient}--\ref{E:log-pl-neg-hessian}).  If
    assumptions A\ref{A:square-int}--A\ref{A:var-equicont} hold, then the
    following are true as $n \to \infty$:
    \begin{enumerate}[(i)]
        \item \label{I:score-part}
        $n^{-1/2} \, U_{\lfloor \alpha \tau_n \rfloor}(\beta_0)$
        converges weakly to a Gaussian process on $[0,1]$ with
        covariance function $\Sigma_\alpha(\beta_0)$;
        
        \item \label{I:fisher-part}
        if $\hat \beta_t$ is any consistent estimator of $\beta_0$,
        then
        \[
            \sup_{\alpha \in [0,1]}
            \left\|
                \tfrac{1}{n}
                I_{\lfloor \alpha \tau_n \rfloor}(\hat \beta_{\tau_n})
                -
                \Sigma_\alpha(\beta_0)
            \right\|
            \toP
            0.
        \]
    \end{enumerate}
\end{theorem}

\begin{proof}[Proof of Part \textit{(\ref{I:score-part})}]
The process $N_t(i,j)$ has compensator
\begin{equation}
    \Lambda_t(i,j)
        =
            \int_0^t \lambda_s(i,j) \, ds;
\end{equation}
similarly, processes $N_t(i)$ and $N_t$ have compensators
$\Lambda_t(i) = \sum_{j \in \mathcal{J}} \Lambda_t(i,j)$
and $\Lambda_t = \sum_{i \in \mathcal{I}} \Lambda_t(i)$.  Define local
martintagles $M_t(i,j) = N_t(i,j) - \Lambda_t(i,j)$,
$M_t(i) = N_t(i) - \Lambda_t(i)$, and 
$M_t = N_t - \Lambda_t$.  

The score function evaluated at the true parameter vector has a simple
representation in terms of these martingales.  Set
$H_t(i,j) = x_t(i,j) - E_t(\beta_0,i)$.  Since $x$ is uniformly
bounded, $H$ is as well.  Using the identity
\(
    \sum_{j \in \mathcal{J}}
    \int_0^t
        H_s(i,j) \,
        d\Lambda_s(i,j)
    =
    0,
\)
the score can be written as
\begin{align*}
    U_t(\beta_0)
        &=
        \sum_{i \in \mathcal{I}}
        \sum_{j \in \mathcal{J}}
        \int_0^t
            H_s(i,j) \, dN_s(i,j) \\
        &=
        \sum_{i \in \mathcal{I}}
        \sum_{j \in \mathcal{J}}
        \int_0^t
            H_s(i,j) \, dM_s(i,j).
\end{align*}
Each term in the sum is locally square integrable, with predictable
covariation
\begin{align*}
    \begin{split}
        \bigg\langle
            \int
                H_s(i,j) \, dM_s(i,j)
        &, \, \,
            \int
                H_s(i',j') \, dM_s(i',j')
        \bigg\rangle_t \\
        &=
            \int_0^t
                H_s(i,j) \otimes H_s(i',j') \,
                d\big\langle M(i,j), M(i',j')\big\rangle_s
    \end{split} \\
        &=
            \int_0^t
                \big[ H_s(i,j) \big]^{\otimes 2} \,
                d\Lambda_s(i,j)
            \cdot
            1\{ i = i', j = j' \}
\end{align*}
\cite[Thm.~2.4.3]{fleming1991counting}.  There exists a sequence
of stopping times localizing all $M(i,j)$ simultaneously, so $U(\beta_0)$ is
locally square integrable with predictable variation
\begin{align*}
    \big\langle U(\beta_0) \big\rangle_t
        &=
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \big[ H_s(i,j) \big]^{\otimes 2} \,
                d\Lambda_s(i,j) \\
        &=
            \sum_{i \in \mathcal{I}}
            \int_0^t
                V_s(\beta_0, i) \,
                d\Lambda_s(i). \\
\intertext{Note}
    I_t(\beta)
        &=
            \sum_{i \in \mathcal{I}}
            \int_0^t
                V_s(\beta, i) \,
                dN_s(i).
\end{align*}

Now we rescale time.
For each positive $n$ define a discretized time-scaled version of the score.  
The process is defined for times in $[0,1]$; between times in
$[\tfrac{k}{n}, \tfrac{k+1}{n})$, it takes the value $U_{\tau_k}$.  That is,
\[
    \tilde U_{\alpha}^{(n)}(\beta)
        = U_{\lfloor \alpha \tau_n \rfloor}(\beta).
\]
This is right-continuous with limits from the left.

We claim $\tilde U_\alpha^{(n)}(\beta_0)$ is a square-integrable martingale
adapted to
\(
    \mathcal{\tilde F}^{(n)}_\alpha
        =
        \mathcal{F}_{\lfloor \alpha \tau_n \rfloor}.
\)
The conditional expectation property holds provided
\(
    \E[ U_{\tau_{n}}(\beta_0) \mid \mathcal{F}_{\tau_{n-1}} ]
        = U_{\tau_{n-1}}(\beta_0).
\)
Define $K = \sup_{t,i,j} \| x_{t}(i,j) \|$.
Note that $\|H_{t}(i,j)\| \leq 2 K$.  Thus,
\begin{align*}
    \| U_{t \wedge \tau_n} (\beta_0) \|
        &\leq
            2 K
            \big(
                N_{t \wedge \tau_n}
                +
                \Lambda_{t \wedge \tau_n}
            \big), \\
\intertext{so that}
    \E \left[
        \sup_t
        \| U_{t \wedge \tau_n} (\beta_0) \|^2
    \right]
        &\leq
            8 \cdot \big(\E K^2\big)^{1/2} \cdot
            \big(
              \E N_{\tau_n}^2
              +
              \E \Lambda_{\tau_n}^2              
            \big)^{1/2}.
\end{align*}
By assumption, $\E K^2$ is finite, and by construction, $N_{\tau_n}$ is
bounded.  Since $N_{t \wedge \tau_n}$ is a counting process,
$\E \Lambda_{\tau_n}^2$ is finite, too
(this follows from results in Section 2.3 of Fleming and Harrington
\cite{fleming1991counting}).  Thus, $U_{t \wedge \tau_n}(\beta_0)$
is uniformly integrable.  The Optional Sampling Theorem now applies to
give the conditional expectation property of $\tilde U^{(n)}(\beta_0)$.  For
square integrability, note
\(
    \sup_{1 \leq m \leq n}
    \E \| U_{\tau_m} \|^2
        \leq
        \E \left[
           \sup_t
           \| U_{t \wedge \tau_n} (\beta_0) \|^2
        \right].
\)

Since it only depends on values at jump times, the
quadratic variation of $\tilde U^{(n)}(\beta_0)$ at time $\alpha$ is
equal to the quadratic variation of $U(\beta_0)$ at time
$\tau_{\lfloor \alpha n \rfloor}$.  Therefore, since quadratic and predictable
variation have the same limit when it exists
\cite[Prop. 1]{rebolledo1980central},
\(
    \langle \frac{1}{\sqrt{n}} \tilde U^{(n)}(\beta_0) \rangle_\alpha
        \toP
            \Sigma_\alpha(\beta_0).
\)
When $\frac{1}{\sqrt{n}} \tilde U^{(n)}(\beta_0)$ satisfies a Lindeberg condition,
the process converges in distribution to a Gaussian process with covariance
function $\Sigma(\beta_0).$

The Lindeberg condition is that for any positive $\varepsilon$,
\[
    \frac{1}{n}
    \sum_{i,j}
    \int_{0}^{\tau_n}
        \| H_s(i,j) \|^2
        \, 1\{\| H_s(i,j) \| > \sqrt{n} \varepsilon \}
        \, d\Lambda_s(i,j)
        \toP
        0.
\]
With $K = \sup_{t,i,j} \| x_t(i,j)\|$ as above, the integral is bounded by
\(
    4 \, K^2 \, 1\{n^{-1/2} K > \varepsilon / 2\}
    \cdot
    \frac{\Lambda_{\tau_n}}{n}.
\)
Since $\E K^2 < \infty$, the first term converges to zero in probability.
Since $\E \Lambda_{\tau_n} = \E N_{\tau_n} = n$, the product of the two also
converges to zero in probability.  Thus, the Lindeberg condition is satisfied.
The weak convergence result now follows from Rebolledo's Martingale Central
Limit Theorem~\cite{rebolledo1980central}.
\end{proof}

\begin{proof}[Proof of Part \textit{(\ref{I:fisher-part})}]
Part~\textit{(\ref{I:fisher-part})} is a consequence of equicontinuity
and Lenglart's Inequality~\cite{lenglart1977relation}.  As in the proof of
part~\textit{(\ref{I:score-part})}, set $K = \sup_{t,i,j} \| x_t(i,j) \|$.

When $\alpha \in [0, 1]$,
\begin{align*}
    \left\|
        \tfrac{1}{n} I_{\lfloor \alpha \tau_n \rfloor}(\hat \beta_{\tau_n})
        -
        \Sigma_{\alpha} (\beta_0)
    \right\|
        &\leq
        \left\|
            \frac{1}{n}
            \int_0^{\lfloor \alpha \tau_n \rfloor}
                \{
                    V_s(\hat \beta_{\tau_n}, i)
                    -
                    V_s(\beta_0, i)
                \} \, dN_s(i)
        \right\| \\
        &\quad+
        \left\|
            \frac{1}{n}
            \sum_i
            \int_0^{\lfloor \alpha \tau_n \rfloor}
                V_s(\beta_0, i) \, dM_s(i)
        \right\| \\
        &\quad+
        \left\|
            \frac{1}{n}
            \sum_i
            \int_0^{\lfloor \alpha \tau_n \rfloor}
                V_s(\beta_0)
                \, d\Lambda_s(i)
            -
            \Sigma_{\alpha}(\beta_0)
        \right\|.
\end{align*}
The first term is uniformly bounded by 
\(
    \sup_{n',i}
        \|
            V_{\tau_{n'}}(\hat \beta_{\tau_n}, i)
            -
            V_{\tau_{n'}}(\beta_0, i)
        \|,
\)
which converges to zero since $\hat \beta_t \toP \beta_0$ and
$\{ V_{\tau_{n'}}(\cdot, i) \}$ is an equicontinuous family
(assumption~A\ref{A:var-equicont}).
The third term converges to zero by assumption~A\ref{A:integrated-cov-limit}.
Lenglart's Inequality and assumption~A\ref{A:message-times-finite} imply that for
any postive $\rho$ and $\delta$,
\begin{multline*}
    \mathbb{P}\left\{
        \sup_{t \in [0,\tau_n]}
        \left\|
            \frac{1}{n}
            \sum_{i}
            \int_{0}^{t}
                V_s(\beta_0, i) \, dM_s(i)
        \right\|
        \geq \rho
    \right\} \\
    \leq
    \frac{\delta}{\rho^2}
    +
    \mathbb{P}\left\{
        \frac{1}{n^2}
        \sum_{i}
        \int_{0}^{\tau_n}
            \| V_s (\beta_0, i) \|^2
            \, d\Lambda_s(i)
        \geq
        \delta
    \right\}.
\end{multline*}
(see Fleming and Harrington's Cor.~3.4.1 for a related proof
\cite{fleming1991counting}).  The sum in the second term is bounded by
$\frac{K^4}{n} \cdot \frac{\Lambda_{\tau_n}}{n}$.  Since $n^{-1/2} K^2 \toP 0$
and $\E \Lambda_{\tau_n} = n$, the right hand side of the inequality converges
to $\frac{\delta}{\rho^2}$.  Since $\delta$ is arbitrary, the right hand side
must converge to zero.  This proves that the second term in the bound on
\(
    \left\|
        \tfrac{1}{n} I_{\lfloor \alpha \tau_n \rfloor}(\hat \beta_{\tau_n})
        -
        \Sigma_{\alpha} (\beta_0)
    \right\|
\)
converges to zero uniformly in $\alpha$, concluding the proof.
\end{proof}


\begin{theorem}\label{T:consistency}
    Let $N$ be a multivariate counting process on with stochastic
    intensity as given in~\eqref{E:cox-intensity}, with true parameter
    vector $\beta_0$.  Let the log partial likelihood,
    $\log \mathit{PL}_t(\cdot)$, be as defined in \eqref{E:log-pl}.
    Let $\tau_n$ be the sequence of message arrival times defined
    in~\eqref{E:message-times}.
    
    Assume for $\beta$ in a
    neighborhood of $\beta_0$ that
    \(
        -\tfrac{1}{n} \nabla^2 [ \log \mathit{PL}_{\tau_n}(\beta)]
            \toP \Sigma_1(\beta),
    \)
    where $\| \Sigma_1(\cdot) \|$ is locally Lipschitz and bounded
    away from zero.
    If $\hat \beta_n$ maximizes $\log \mathit{PL}_{\tau_n}(\cdot)$
    and the conclusions of Theorem~\ref{T:score-fisher} hold, then
    the following are true as $n\to\infty$:
    \begin{enumerate}[(i)]
        \item $\hat \beta_n$ is a consistent estimator of $\beta_0$;
        \item $\sqrt{n} \, (\hat \beta_n - \beta_0)$ converges weakly
            to a mean-zero Gaussian random variable with covariance
            $[\Sigma_1(\beta_0)]^{-1}$.
    \end{enumerate}
\end{theorem}

We follow Haberman's approach to proving
consistency, which relies on Kantorovich's analysis
of Newton's method.  Tapia gives an elementary proof of the Kantorovich
Thereom~\cite{haberman1977maximum,kantorovich1952functional,tapia1971kantorovich}.
We state a weak form of the the result as a lemma.

\begin{lemma}[Kantorovich Theorem]
    Let $P(x) = 0$ be a general system of nonlinear equations, where $P$ is
    a map between two Banach spaces.  Let $P'(x)$ denote the Jacobian
    (Fr\'echet differential) of $P$ at $x$, assumed to exist in $D_0$,
    a convex open neighborhood of $x_0$.  Assume the following:
    \begin{enumerate}[(i)]
        \item $\| [P'(x_0)]^{-1} \| \leq B$,
        \item $\| [P'(x_0)]^{-1} P(x_0) \| \leq \eta$,
        \item $\| P'(x) - P'(y) \| \leq K \| x - y \|$,\quad
            for all $x$ and $y$ in $D_0$,
    \end{enumerate}
    with $h = B K \eta \leq \tfrac{1}{2}$.
    
    Let $\Omega_\ast = \{ x : \| x - x_0 \| \leq 2 \eta \}$.
    If $\Omega_\ast \subset D_0$, then the Newton iterates,
    $x_{k+1} = x_k - [P'(x_k)]^{-1} P(x_k)$, are well defined, remain
    in $\Omega_\ast$, and converge to $x^\ast$ in $\Omega_\ast$ such
    that $P(x^\ast) = 0$.  In addition,
    \[
        \| x^\ast - x_k \|
            \leq
                \frac{\eta}{h}
                \frac{(2h)^{2^k}}{2^k},
        \qquad
        k = 0, 1, 2, \ldots.
    \]
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{T:consistency}]

Set $U_t(\cdot)$ and $I_t(\cdot)$ to be the gradient and negative
Hessian of the log partial likelihood, as defined in
(\ref{E:log-pl-gradient}--\ref{E:log-pl-neg-hessian}).  Since
$I_t(\beta)$ is a sum of rank-one matrices with positive weights,
it is positive semi-definite, and $\log \mathit{PL}_t(\cdot)$ is
a concave function.  By the assumption that $\| \Sigma_1(\cdot) \|$
is bounded away from zero in a neighborhood of $\beta_0$, for $n$
sufficiently large, if $\log \mathit{PL}_t(\cdot)$ has a local minimum
in that neighborhood then it must be the unique global maximum.

We find the local maximum by applying Newton's method to the
gradient of $\tfrac{1}{n} \log \mathit{PL}_{\tau_n}(\cdot)$, taking
$\beta_0$ as the initial iterate.  Define
\[
    Z_n = -[I_{\tau_n}(\beta_0)]^{-1} U_{\tau_n}(\beta_0).
\]
Theorem~\ref{T:score-fisher} and the
assumptions of the theorem imply that $[I_{\tau_n}(\beta_0)]^{-1}$
exists for $n$ large enough so that $Z_n$ is well-defined.  Moreover,
(i) $Z_n \toP 0$, and
(ii) $\sqrt{n} \, Z_n \tod \Normal(0, \, [\Sigma_1(\beta_0)]^{-1})$.
The first Newton iterate, $\beta_{n,1}$ is equal to $\beta_0 - Z_n$.

Apply Kantorovich's Theorem to bound $\| \hat \beta_n - \beta_0 \|$
and $\| \hat \beta_n - \beta_{n,1} \|$.  By assumption, there exists
a neighborhood of $\beta_0$, say $D_0$, and finite $K$ and $B$, such that 
\(
    \|
        \frac{1}{n} I_{\tau_n} (\beta)
        -
        \frac{1}{n} I_{\tau_n}(\beta')
    \|
    \leq
    K
    \|
        \beta
        -
        \beta'
    \|
\)
and
\(
    \| \frac{1}{n} [I_{\tau_n}(\beta_0)]^{-1} \| \leq B
\)
for $\beta, \beta' \in D_0$.
Define $\eta_n = \| Z_n \|$ and $h_n = B K \eta_n$, noting that $h_n$ and
$\eta_n$ are size $\OhP(n^{-1/2})$.  Thus, for $n$ large enough, the
Newton iterates converge to $\hat \beta_n$.  Moreover,
\begin{enumerate}[(i)]
    \item $\| \hat \beta_n - \beta_0 \| \leq 2 \, \eta_n \toP 0$,
    \item 
        \(
            \sqrt{n} \, \| \hat \beta_n - (\beta_0 - Z_n) \|
            \leq
            2 \sqrt{n} \, \eta_n \, h_n
            \toP 0.
        \)
\end{enumerate}
Thus, $\hat \beta_n \toP \beta_0$, and $\sqrt{n} (\hat \beta_n - \beta_0)$
and $\sqrt{n} \, Z_n$ converge weakly to the same limit.

\end{proof}


\section{Multiple recipients}

To this point, we have assumed that each message has a single recipient.
Often, messages have multiple recipients.  In the Enron dataset,
approximately 30\% of messages have more than one recipient, with
a few messages having more than fifty recipients.  One way of dealing
with multiple recipients is, for example, treating a message with three
recipients as three separate single-recipient messages.  A less ad-hoc
approach is to explicitly take multiple recipients into account at the
modeling stage.

Introduce $\bar \lambda_t(i ; R)$ as the baseline intensity at time $t$ for
messages sent by sender $i$ with $R$ recipients.  Let $J$ be any
subset of $\mathcal{J}$ and model the rate at which $i$ sends messages to
recipient set $J$ as
\begin{equation}\label{E:intensity-multiple}
    \lambda_t(i,J)
        =
        \bar \lambda_t(i ; |J|)
        \cdot
        \exp\Big\{
            \sum_{j \in J}
                \beta_0^\trans  x_t(i,j)
        \Big\}
        \cdot
        \prod_{j \in J}
        1\{ j \in \mathcal{J}_t(i) \}
        .
\end{equation}
We have chosen this form for ease of interpretability.  At time
$t$, changing receiver $j$'s covariates from $x_t(i,j)$ to
$x'_t(i,j)$ results in scaling the the rate at
which $i$ sends him messages by
\(
    \exp\{ \langle x'_t(i,j) - x_t(i,j), \beta \rangle \};
\)
importantly, this does not depend on $\lambda_t(i; R)$.

The partial likelihood is nearly the same as in the single-recipient
setting from Section~\ref{S:point-process-model}.
Let $(t_1, i_1, J_1), \ldots, (t_M, i_M, J_M)$ be the sequence of
observed messages, with tuple $(t, i, J)$ indicating that at time $t$
sender $i$ sent a message with recipient set $J$.


\clearpage 

Define sets
\begin{align*}
    \mathcal{M}_t(i, J)
        &= \{ m : t_m \leq t, i_m = i, J_m = J \}, \\
    \mathcal{M}_t(i)
        &= \cup_{J \subseteq \mathcal{J}} \mathcal{M}_t(i, J), \\
    \mathcal{M}_t
        &= \cup_{i \in \mathcal{I}} \mathcal{M}_t(i).
\end{align*}
When $i \in \mathcal{I}$, $j \in \mathcal{J}(i)$, and $R \geq 1$, define
\[
    w_{\beta,t}(i,j) = \exp\{ \langle x_t(i,j), \, \beta \rangle \},
    \quad
    W_{\beta,t}(i; R)
    =
    \!\!
    \sum_{\substack{J \subseteq \mathcal{J}(i)\\ |J| = R}}
        \prod_{j \in J}
            w_{\beta,t}(i,j).
\]
The log partial likelihood at time $t$ is
\begin{equation}\label{E:log-pl-multiple}
    \log \mathit{PL}_t(\beta)
        =
        \sum_{i \in \mathcal{I}}
        \log \mathit{PL}_t(\beta, i),
\end{equation}
where the sender-specific part is
\begin{equation}\label{E:sender-log-pl-multiple}
    \log \mathit{PL}_t(\beta, i)
        =
        \sum_{m \in \mathcal{M}_t(i)}
        \Big[
            \sum_{j \in J_m}
                \langle x_{t_m}\!(i, j), \, \beta \rangle
        \Big]
            -
            \log W_{\beta,t_m} (i ; |J_m|).
\end{equation}
Consistency and asymptotic normality of the maximum partial likelihood
estimator follow from standard arguments.  We state this result and
sketch its proof below.

Let $J_n$ be an increasing sequence of positive integers, set
$\mathcal{J}_n = \{ 1, \ldots, J_n \}$, and
let $x_{n,j}$ such that $1 \leq j \leq J_n$ be a triangular array of
$p$-dimensional covariate vectors.  Let $l_m$ be a sequence of positive
integers and $\mathcal{R}_{n,m}$ such that
$1 \leq m \leq n$ be another triangular array, where
$\mathcal{R}_{n,m} \in \mathcal{J}_n^{l_m}$.  The vector
\(
    \mathcal{R}_{n,m}
    =
    \big(
        \mathcal{R}_{n,m}(1), \ldots, \mathcal{R}_{n,m}(l_m)
    \big)
\)
can be considered an ordered sample of size $l_m$ from $\mathcal{J}_n$.

Consider three probability laws for $\mathcal{R}_{n,m}$ indexed by
$\beta$.  For all three, the variables $\mathcal{R}_{n,m}$ are independent.
First, when $j \in \mathcal{J}_n$ define
\[
    \pi_{\beta, n}(j)
    =
    \frac{\exp\big\{\langle x_{n,j}, \beta \rangle\big\}}
         {\sum_{j' \in \mathcal{J}_n}
            \exp\big\{\langle x_{n,j'}, \beta \rangle\big\}}.
\]
Under $\tilde{\mathbb{P}}_\beta$, the components of $\mathcal{R}_{n,m}$ are
drawn with replacement according to $\pi_{\beta, n}(\cdot)$:
\begin{equation}
    \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
    =
    \prod_{k = 1}^{l_m}
        \pi_{\beta, n}(R(k)).
\end{equation}
Under both $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$, the components
of $\mathcal{R}_{n,m}$ are unique.
The law $\mathbb{P}_\beta$ designates that 
$\mathcal{R}_{n,m}$ is drawn from $\tilde{\mathbb{P}}_\beta$
conditionally on having unique components;
the law $\mathbb{P}^1_\beta$ designates that the components of
$\mathcal{R}_{n,m}$ are drawn one at a time without replacement according to
weights $\pi_{\beta,n}(\cdot)$.  Formally, define $|\cdot|$ to be the number
of unique components in a vector.  Then,
\begin{align}
    \mathbb{P}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
        &=
        \frac{
            \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
            I_{\{|R| = l_m\}}
        }{
            \sum_{R' \in \mathcal{J}_n^{l_m}}
                \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R' \big\}
                I_{\{|R'| = l_m\}}                
        } \\
\intertext{and}
    \mathbb{P}^1_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
        &=
        \frac{1}{l_m !}
        \sum_{\sigma}
        \prod_{k=1}^{l_m}
            \frac{
                \pi_{\beta,n}\big(R(\sigma(k))\big)
            }{
                1 - \sum_{k' = 1}^{k - 1} \pi_{\beta,n}\big(R(\sigma(k'))\big)
            },
\end{align}
where $\sigma$ ranges over all permutations of $\{ 1, ..., l_m \}$.


We wish to estimate $\beta$ after observing covariates
$\{ x_{n,j} : 1 \leq j \leq J_n \}$ and samples
$\{\mathcal{R}_{n,m} : 1 \leq m \leq n\}$.  In the setting driven by
$\tilde{\mathbb{P}}_\beta$, the maximum likelihood estimate is easy to
obtain.  In $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$, though, the
likelihood and its derivatives have combinatorially many terms
and can be prohibitively expensive to optimize.

The likelihood corresponding to $\tilde{\mathbb{P}}_\beta$ is given by
\[
    \tilde{L}_n(\beta ; R_1, \ldots, R_n)
    =
    \prod_{m=1}^{n}
        \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R_m \}.
\]
Let $\hat \beta_n$ be the value that maximizes
\(
    \tilde{L}_n( \cdot\, ; \mathcal{R}_{n,1}, \ldots, \mathcal{R}_{n,n}).
\)
As $n$ increases, under $\tilde{\mathbb{P}}_\beta$ standard likelihood
theory shows that $\hat \beta_n$ converges in probability to $\beta$ and
that $\sqrt{n}(\beta - \hat \beta_n)$ converges in distribution to a
mean-zero Gaussian random variable.  This fails to hold generally under
$\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$.  However, it turns out
that if $J_n$ diverges sufficiently fast, then the bias in $\hat \beta_n$ is
of order $\tfrac{1}{\sqrt{n}}$, and $\sqrt{n}(\beta - \hat \beta_n)$ still
has a Gaussian limit.

\begin{theorem}
    Under $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$,
    if $\max_{j \in \mathcal{J}_n} \|x_{n,j}\|$,
    \(
        \frac{1}{n} \sum_{m=1}^n l_m^2,
    \)
    and
    \(
        \frac{n}{J_n^2}
    \)
    are uniformly bounded in $n$, then $\hat \beta_n$ is a
    $\sqrt{n}$-consistent estimate of $\beta$.
\end{theorem}

\clearpage

\appendix

\section{The Enron Email Corpus}\label{S:enron-corpus}

The Enron email corpus is a large subset of the email messages sent within the
corporation between November 1998 and June 2002. Enron was an energy company
based in Texas that became notorious in late 2001 for its fraudulent
accounting practices. These practices eventually lead to the resignation of
its CEO, to an external audit into its accounting procedures, to massive
readjustments to its earnings statements, and eventually to its bankruptcy.
Later, many of the top executives were prosecuted and convicted of criminal
fraud and insider trading. As part of the investigation, the Federal Energy
Regulatory Commission (FERC) subpoenaed the e-mail correspondences of the top
employees and posted those not related to the criminal trial to their website
(619,446 messages, roughly 92\% of those subpoenaed).

Leslie Kaelbling, a computer scientist at MIT, purchased the corpus from the
FERC with the intention of making it available to the research community. A
team of researchers at SRI International worked to fix a number of apparent
integrity problems with the data, and then it was released to the public by
William Cohen, at CMU. This is the ``March 2, 2004 Version'' of the dataset.
Later, a former Enron employee requested that one message be removed from the
data; this removal prompted the ``August 21, 2009 Version,'' widely regarded
to be the authoritative version \cite{cohen2009enron}.

Zhou et al. have gone through substantial work to preprocess the Enron corpus
into a useful form \cite{zhou2007strategies}. We rely on their preprocessed
version for our analysis. They reduce the data to the set of e-mails sent by
high-ranking Enron executives, eliminating messages sent by non-employees,
support staff and administrative assistants. They also filter out messages
with missing timestamps. After this culling process, there are 21,635 messages
sent by 156 employees between November 13, 1998 and June 21, 2002. Each email
message has a message body consisting of text, along with header fields
including \texttt{Date}, \texttt{From}, \texttt{To}, \texttt{CC},
\texttt{BCC}, and \texttt{Subject}. The \texttt{From} field always lists a
unique e-mail address, but the \texttt{To}, \texttt{CC}, and \texttt{BCC}
fields sometimes specify multiple recipients. We make no distinction between
\texttt{To}, \texttt{CC}, and \texttt{BCC}, combining them all to determine
the recipients of a message.

We can associate a set of static covariates for each of the 156 individuals in
the dataset. Zhou et al. have extracted the employees' names, departments, and
titles. We use the forenames of the employees to associate genders to the
employees (e.g, John is male, Susan is female). For ambiguously-gendered names
like Dana, Robin, and Sean, we use personal pronouns in the messages to help
code the genders. We code the department as one of three categories: Legal,
Trading, or Other. Finally, we code the position as Senior (CEO, CFO, COO,
Director, Managing Director, VP, President) or Junior (Administrator, Analyst,
Assistant, Attorney, Counsel, Employee, Manager, Specialist, Trader).



\section{Implementation}\label{S:implementaiton}


The likelihood of the interaction process is a function of the unknown
parameter vector $\beta$ and the sender-specific baseline
intensities $\lambda(1), \ldots, \lambda(I)$.  After observing the process
up to time $t$, the likelihood is
\begin{multline*}
    L_t\big(\beta, \lambda(1), \ldots, \lambda(I)\big) \\
        =
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \lambda_s(i,j) \, dN_s(i,j)
            -
            \int_0^t
                \lambda_s(i,j) \, ds
        \bigg\}.
\end{multline*}
This factors into two terms, one which depends on all of the parameters,
and one which depends on $\beta$ alone.  We estimate $\beta$ by maximizing
the latter term, the partial likelihood, denoted $\mathit{PL}_t(\beta)$.
Define
\begin{gather*}
    w_{\beta,t}(i,j) = \exp\{ \langle x_t(i,j), \, \beta \rangle \},
    \qquad
    W_{\beta,t}(i) = \sum_{j \in \mathcal{J}(i)} w_{\beta,t}(i,j), \\
    \tilde \lambda_{\beta,t}(i)
        = \lambda_t(i) \cdot W_{\beta,t}(i).
\end{gather*}
The factorization is
\begin{align*}
    \begin{split}
    L_t\big(\beta, \lambda(1), \ldots&, \lambda(I)\big) \\
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \int_0^t
                \log \tilde \lambda_{\beta,s}(i) \, dN_s(i,j)
            -
            \int_0^t
                \tilde \lambda_{\beta,s}(i) \, ds
        \bigg\}
        \cdot
        \mathit{PL}_t(\beta),
    \end{split} \\
    \mathit{PL}_t(\beta)
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \frac{w_{\beta,s}(i,j)}
                          {W_{\beta,s}(i)}
                \, dN_s(i,j)
        \bigg\}.
\end{align*}

Suppose $(t_1, i_1, j_1), \ldots, (t_M, i_M, j_M)$ is the sequence of observed
messages, where tuple $(t,i,j)$ indicates that at time $t$, sender $i$ sent a
message to receiver $j$.  Define the following message sets:
\begin{align*}
  \mathcal{M}_t(i,j)
    &= \{ m : t_m \leq t, \, i_m = i, \, j_m = j \}, \\
  \mathcal{M}_t(i)
    &= \cup_{j \in \mathcal{J}} \mathcal{M}_t(i,j), \\
  \mathcal{M}_t
    &= \cup_{i \in \mathcal{I}} \mathcal{M}_t(i).
\end{align*}
The partial likelihood factors a a product of terms, one for each sender:
\begin{align*}
    \mathit{PL}_t(\beta)
        &=
        \,\,
        \prod_{i \in \mathcal{I}}
            \,\,
            \mathit{PL}_t(\beta, i), \\
    \mathit{PL}_t(\beta, i)
        &=
        \!\!\!\!
        \prod_{m \in \mathcal{M}_t(i)}
            \!\!\!
            \frac{w_{\beta, t_m} (i, j_m)}
                 {W_{\beta, t_m}}.
\end{align*}
This factorization allows us to compute $\log \mathit{PL}_t(\beta)$ and
its derivatives by computing the sender-specific terms in parallel and
then adding them together.

The sender-specific log-partial likelihood and its derivatives are
\begin{align*}
    \log \mathit{PL}_t(\beta, i)
        &=
        \!\!\!\!
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!\!        
            \langle x_{t_m}\!(i, j_m), \, \beta \rangle
            \,
            -
            \,
            \log W_{\beta, t_m}\!(i), \\
    \nabla [ \log \mathit{PL}_t(\beta, i) ]
        &=
        \!\!\!\!        
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!\!
            x_{t_m}\!(i,j_m)
            \,
            -
            \,
            \frac{1}{W_{t_m}\!(i)}
                \sum_{j \in \mathcal{J}(i)}
                    w_{t_m}\!(i,j) \,
                    x_{t_m}\!(i,j), \\
    \begin{split}
    \nabla^2 [ \log \mathit{PL}_t(\beta, i) ]
        &=
        -
        \!\!\!\!
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!
            \bigg\{
            \frac{1}{W_{t_m}\!(i)}
            \sum_{j \in \mathcal{J}(i)}
                w_{t_m}\!(i,j) \,
                \big[
                    x_{t_m}\!(i,j)
                \big]^{\otimes 2} \\
        &\qquad\qquad\qquad-
            \Big[
                \frac{1}{W_{t_m}\!(i)}
                \sum_{j \in \mathcal{J}(i)}
                    w_{t_m}\!(i,j) \,
                    x_{t_m}\!(i,j)
            \Big]^{\otimes 2}
            \bigg\},
    \end{split}
\end{align*}
where $a^{\otimes 2} = a a^\trans$.  When $x_t(i,j)$ is constant over time,
there are sufficient statistics for $\beta$ and these formulas reduce.
Otherwise, computing $\log \mathit{PL}_t(\beta,i)$ and its derivatives
requires iterating over all messages, potentially taking time
$\Oh(|\mathcal{M}_t(i)| \cdot |\mathcal{J}(i)| \cdot p^2)$.  
The serial computation time for computing the full partial likelihood
and its derivatives is $\Oh(M \cdot J \cdot p^2)$.
For the Enron data, $M \approx 20,\!000$, $J \approx 150$,
and $p \approx 200$.  This is relatively small by modern standards.  Often
$M$ on the order of $10^9$ and $J$ on the order of $10^6$.  Computations
taking time $\Oh(M \cdot J)$ are prohibitive for large datasets.

Taking advantage of sparsity, the serial computation time reduces to
$\Oh(M \cdot p^2 + I \cdot J)$.  Decompose $x$ into its static
(non time-varying) and dynamic parts:
\[
    x_t(i,j)
        = x_0(i,j) + d_t(i,j).
\]
Typically, the dynamic part, $d_t(i,j)$, is sparse, with at most $\bar p$
nonzero components.  Moreover $d_t(i,j)$ is zero for most $(i,j)$
pairs---often $d_t(i,j)$ is zero unless $i$ and $j$ have exchanged
messages in the past.  Let
\[
    \mathcal{\bar J}(i)
        =
        \{
            j \in \mathcal{J}(i) : d_t(i,j) \neq 0 
            \,\,
            \text{for some $t$}
        \}.
\]
Assume $|\mathcal{\bar J}(i)| \ll |\mathcal{J}(i)|$ and that 
computing $d_t(i,j)$ takes amortized time $\Oh(\bar p)$ for each
$(t,i,j)$ triple.

Notice
\begin{align*}
    w_{\beta,t}(i,j)
        &=
            w_{\beta,0}(i,j)
            \cdot
            \exp\{ \langle d_t(i,j), \beta \rangle \}, \\
    W_{\beta,t}(i)
        &=
            W_{\beta,0}(i)
            +
            \sum_{j \in \mathcal{\bar J}(i)}
                w_{\beta,t}(i,j) - w_{\beta,0}(i,j)
\end{align*}
It takes time $\Oh(|\mathcal{J}(i)| \cdot p)$ to compute the values of
$W_{\beta,0}(i)$ and $w_{\beta,0}(i,j)$ for any $i$ and all
$j$ in $\mathcal{J}(i)$; once these values are known, computing the values
of $W_{\beta,t}(i)$ and $w_{\beta,t}(i,j)$ takes time
\(
    \Oh(|\mathcal{\bar J}(i)| \cdot \bar p).
\)
Since
\[
    \sum_{m\in \mathcal{M}_t(i)}
        x_{t_m}\!(i,j_m)
        =
            \sum_{j \in \mathcal{J}(i)}
                \mathcal{M}_t(i,j) \, x_0(i,j)
            \,\,\,
            +
            \sum_{m \in \mathcal{M}_t(i)}
                d_{t_m}\!(i,j),
\]
%computing sum on the left hand side takes time
%\(
%    \Oh(|\mathcal{M}_t(i)| \cdot \bar p + |\mathcal{J}(i)| \cdot p);
%\)
computing $\log \mathit{PL}_t(\beta, i)$ takes time
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p
        +
        |\mathcal{J}(i)| \cdot p
    ).
\)

The second term in the gradient is
\begin{multline*}
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta,t_m}\!(i)}
        \sum_{j \in \mathcal{J}(i)}
            w_{\beta,t_m}\!(i,j)
            \,
            x_{t_m}\!(i,j) \\
    =
    \sum_{j \in \mathcal{J}(i)}
    \bigg[
        \sum_{m \in \mathcal{M}_t(i)}
            \frac{w_{\beta,t_m}\!(i,j)}{W_{\beta,t_m}\!(i)}
    \bigg]
    \cdot
    x_0(i,j) \\
    +
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta, t_m}\!(i)}
        \sum_{j \in \mathcal{\bar J}(i)}
            w_{\beta,t_m}\!(i,j)
            \,
            d_{t_m}\!(i,j).
\end{multline*}
Also,
\[
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{w_{\beta,t_m}\!(i,j)}{W_{\beta,t_m}\!(i)}
    =
        w_{\beta,0}(i,j)
        \cdot
        \sum_{m \in \mathcal{M}_t(i)}
            \frac{
                \exp\{\langle d_{t_m}\!(i,j), \, \beta \rangle \}
            }{
                W_{\beta,t_m}\!(i)
            };
\]
the latter term is constant when $j \notin \mathcal{\bar J}(i)$.
Organizing the computations as suggested by these formula,
the additional overhead for computing the gradient of
$\log \mathit{PL}_t(\beta,i)$ is
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p
        +
        |\mathcal{J}(i)| \cdot p
    )
\).
This is the same computational complexity as for computing
$\log \mathit{PL}_t(\beta,i)$.


Suppose $|\mathcal{J}(i)| \leq J$, $|\mathcal{\bar J}(i)| \leq \bar J$,
and $|\mathcal{M}_t(i)| \leq M$.  By organizing the computations as described
above, we can compute $\log \mathit{PL}_t(\beta)$ and its gradient in serial
time
\(
    \Oh(
        | \mathcal{I} |
        \cdot
        M
        \cdot
        \bar J
        \cdot
        \bar p
        +
        |\mathcal{I}| \cdot J \cdot p
    ).
\)
With $\Oh(|\mathcal{I}|)$ processors, we can compute these quantities in time
\(
    \Oh(
        M \cdot \bar J \cdot \bar p
        +
        J \cdot p
        +
        |\mathcal{I}| \cdot p
    ).
\)


\bibliographystyle{imsart-number}
\bibliography{iproc-sources}

\end{document}
