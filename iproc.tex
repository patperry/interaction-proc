\documentclass[aoas,preprint]{imsart}
\pdfoutput=1    % Make arXiv happy

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath,amssymb}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{hypernat}
\RequirePackage{graphicx}
\RequirePackage{enumerate}
\RequirePackage{booktabs}
\RequirePackage{subfigure}
\belowbottomsep=0.5em

\setattribute{journal}{name}{}  % Suppress text "Submitted to..."

\startlocaldefs
\usepackage{iproc-macros}
\endlocaldefs


\begin{document}

\begin{frontmatter}

\title{
    A model for repeated interactions with applications to email traffic
    analysis\protect\thanksref{T1}
}

\runtitle{A point process model for graphs}
\thankstext{T1}{Supported by grant\ldots}

\begin{aug}
    \author{%
        \fnms{Patrick O.} \snm{Perry}\corref{}%
        \ead[label=e1]{patperry@seas.harvard.edu}%
    }%
    \and%
    \author{%
        \fnms{Patrick J.} \snm{Wolfe}%
        \ead[label=e2]{wolfe@stat.harvard.edu}%
    }%

    \runauthor{P.\ O.\ Perry and P.\ J.\ Wolfe}

    \affiliation{Harvard University}

    \address{
        Statistics and Information Sciences Laboratory \\
        Harvard University \\
        33 Oxford Street \\
        Cambridge, MA 02138 \\
        \printead{e1}\\
        \phantom{E-mail:\ }\printead*{e2}
    }
\end{aug}

\begin{abstract}
Abstract.
\end{abstract}

\begin{keyword}[class=AMS]
    \kwd[Primary ]{62M30}     % Statistics:Inference from stochastic processes:Spatial processes
    \kwd[; secondary ]{62N01} % Statistics:Survival analysis and censored data:Censored data models
\end{keyword}

\begin{keyword}
    \kwd{random graphs}
    \kwd{networks}
    \kwd{point processes}
    \kwd{inference}
\end{keyword}

\end{frontmatter}


\section{Introduction}

Data derived from repeated pairwise interactions are becoming more common. 
The canonical examples are communications networks: either all of the phone
calls or the emails sent between individuals in a community
\cite{eagle2006reality,tyler2005email}. More broadly,
``pairwise interaction'' has been construed to include the following:

\begin{itemize}
    \item\textbf{animal association patterns:} zebras congregate at
    locations in their habitat \cite{sundaresan2007network};
    
    \item\textbf{gang homicide:} gangs in Chicago murder members of rival factions
    \cite{papachristos2009murder};

    \item\textbf{legislation cosponsorship:} a congressperson authors a bill, which
    gets sponsored by other legislators \cite{fowler2006connecting};

    \item\textbf{migration patterns:} families migrate between communities in
    Mexico \cite{mckenzie2007network};
\end{itemize}

For the current treatment, we consider directed interactions, with each
interaction having an initiator (sender) and at least one recipient
(receiver).  The above examples fall into this category.  To simplify
the development, initially we focus on the single-recipient case, so that
each interaction involves one sender-receiver pair, called a dyad.  We
borrow terminology from the communications setting, referring to
interactions as messages.

The data is a set of messages.  Each message is represented by a triple,
where $(t,i,j)$ denotes that at time $t$, sender $i$ sent a message to
receiver $j$.  Given a set of messages and covariate information about the
senders and receivers, our goal is to determine which characteristics and
behaviors are predictive of interaction.  At least two questions are
relevant:

\begin{enumerate}
    \item Is a shared attribute between individuals predictive of increased
    interaction?  In studying human behavior, Sociologists repeatedly find
    evidence of homophily, the tendency of individuals to associate with
    similar others \cite{mcpherson2001birds}. A typical inquiry is
    to see if people with identical genders interact more often then
    people with differing genders.

    \item If individual $i$ sends a message to individual $j$, is this
    associated with $j$ being more likely to send to $i$ in the future?
    If so, how does this effect decay with time?
\end{enumerate}

This report presents a modeling framework to facilitate inquiry into
these questions and others like them. The framework is flexible enough to
allow various forms of time-inhomogeneity and dependency, and includes the
ability to handle interactions with multiple recipients. 


\section{A Motivating Example}

Our motivating example is to test for gender bias in a corporate email
network.  The dataset we use is a large collection of email messages sent
within the Enron corporation during the last four years of the company's
existence.  Appendix~\ref{S:enron-corpus} describes the data collection and
preprocessing.  Briefly, there are 21,635 messages in the dataset, which
were sent between 156 executives in the company.  We know the genders,
seniorities, and departments of the employees (summarized in
Table~\ref{T:employee-summary}).

\begin{table}[h]
    \input{tables/employee-summary}
    \caption{
        Characteristics of the 156 employees in the Enron dataset.
    }
    \label{T:employee-summary}
\end{table}

Roughly 28\% of the employees are female, and 72\% are male.  If there were
no gender bias in the email behavior, we would expect these proportions to
agree with the counts of messages exchanged between gender groups.  This
is not the case, as Table~\ref{T:gender-send-counts} demonstrates.  
Apparently, bias is present in each group, associated with higher
within-group sending rates.

\begin{table}[h]
    \input{tables/gender-send-counts}
    \caption{
        Counts of the 21,635 messages sent between gender groups.  Messages
        with multiple recipients are duplicated so that, for example,
        a messages with five recipients counts as five single-recipient
        messages.  This increases the total count to 38,388.
    }
    \label{T:gender-send-counts}
\end{table}

If we treat the messages as independent identically-distributed observations,
then any reasonable test will find gender bias in both group-level behaviors.
Given that a female is sending a message, the probability that the recipient
is male will be estimated to be about 0.48, with a standard error around
0.004; given that a male is sending a message, the estimate will be about
0.23 with a standard error around 0.002.

There are at least three problems with this simple analysis.
\begin{enumerate}
    \item \textbf{Senders never send messages to themselves.} In network
    terminology, there are no ``self-loops.'' In other email datasets, people
    do sometimes email themselves, but arguably these messages should be
    handled differently then messages exchanged between different individuals.
    
    \item \textbf{Messages are not independent of each other.} If I send
    you a message today, then you are more likely to send me a message
    tomorrow.  Any inherent gender bias will be exaggerated by
    reciprocation effects.
    
    \item \textbf{Messages have multiple recipients.} In the Enron dataset,
    more than 30\% of the messages have more than one recipient.  As noted in
    the caption to Table~\ref{T:gender-send-counts}, for the analysis above
    we have treated a message with two or more recipients as two or more
    single-recipient messages.
\end{enumerate}
The first issue is easy to fix, but the other two require significant
attention.

Another feature of the data potentially requiring special consideration is
that the employees have inhomogeneous sending patterns. 
Fig.~\ref{F:send-intensities} shows how the message volume varies with
sender and time.  The individuals have idiosyncratic sending behaviors.

\begin{figure}
    \includegraphics[scale=0.6]{figures/kernel-intensity}
    \caption{
        \textsc{Messages sent by the first employee in the Enron dataset.}
        Each of the 413 messages sent by the employee is represented by a point
        whose $x$ value is the time of the message and whose $y$ value is a
        random jitter around zero.  The curve above the points is a kernel
        intensity estimate of the sending rate with a Gaussian kernel
        and bandwidth chosen by Silverman's rule of thumb.  The area
        under the curve is 413, the number of messages sent by the
        employee.
    }
    \label{F:kernel-intensity}
\end{figure}

\begin{figure}
    \includegraphics[scale=0.6]{figures/send-intensities}
    \caption{
        \textsc{Send intensities of all 156 employees in the Enron
        dataset.}  The curves in the right panel are constructed as in
        Fig.~\ref{F:kernel-intensity}, except normalized so that the
        area under each curve is one.   The left panel shows a quantile plot
        of the number of the normalization constants,
        the messages sent per sender (numbers range from 1 to 3,844).
        Colors are consistent across the two panels. 
        The two panels reveal inhomogeneity of sending volumes and of
        activity behaviors.
    }
    \label{F:send-intensities}
\end{figure}

We propose using a Cox proportional intensity model with history-dependent
covariates to address the first two issues mentioned above, and a parametric
bootstrap to address the third.

\section{A Point Process Model}\label{S:point-process-model}

Every interaction process can be encoded by a multivariate counting measure.
For sender, $i$, receiver, $j$, and positive time, $t$, define
\[
    N_t(i,j)
        =
        \#\{\text{messages sent from $i$ to $j$ in time interval $[0,t]$}\}.
\]
We model $N$ through its stochastic intensity, $\lambda$, assumed to
exist and be predictable and continuous (``predictable'' means roughly that
it only depends on the immediate past).  Heuristically,
\[
    \lambda_t(i,j) \, dt
        =
        \mathbb{P}\{
            \text{$i$ sends $j$ a message in time interval $[t,t+dt)$}
        \}.
\]
The stochastic intensity exists under mild conditions, the most important of
which is that no two interactions happen simultaneously (in practical
applications simultaneous events exist and are an annoyance; a few 
work-arounds have been proposed {TODO: references}).  We will use a version
of the Cox proportional intensity model for modeling the interaction process
\cite{cox1972regression}.  For this section, assume each message has a
single recipient.

Let $\mathcal{I}$ be a set of senders and $\mathcal{J}$ be a set of receivers.
For each sender $i$, let $\bar \lambda_t(i)$ be a non-negative predictable
process called the baseline intensity of sender $i$; let
$\mathcal{J}_t(i)$ be a predictable subset of $\mathcal{J}$ called the 
recipient set of sender $i$.
For each sender-receiver pair $(i,j)$, let $x_t(i,j)$ be a locally bounded
predictable vector of covariates in $\reals^p$.  Let $\beta_0$ in
be an unknown vector of coefficients in  $\reals^p$.

Suppose $N$ is a multivariate counting process on 
$\reals_+ \times \mathcal{I} \times \mathcal{J}$,
with stochastic intensity
\begin{equation}\label{E:cox-intensity}
    \lambda_t(i,j)
        =
        \bar \lambda_t(i)
        \exp\{ \beta_0^\trans x_t(i, j) \}
        \cdot
        1{\{j \in \mathcal{J}_t(i)\}}.
\end{equation}
For technical reasons, assume $N_0(i,j) = 0$.  If
$(t_1, i_1, j_1), \ldots, (t_n, i_n, j_n)$ is the sequence of observed
messages, then the log partial likelihood at time $t$, evaluated at $\beta$,
is
\begin{equation}\label{E:log-pl}
    \log
    \mathit{PL}_t(\beta)
        =
        \sum_{t_m \leq t}
        \bigg\{
            \beta^\trans x_{t_m}\!(i_m, j_m)
            -
            \log\big[
                \!\!\!\!
                \sum_{j \in \mathcal{J}_{t_m}\!(i_m)}
                    \exp\{ \beta^\trans x_{t_m}\!(i_m, j)\}
            \big]
        \bigg\}.
\end{equation}
The maximum partial likelihood estimator (MPLE) is a natural
estimate of $\beta_0$; the inverse Hessian of $\log \mathit{PL}_t(\cdot)$
evaluated at the MPLE is a natural estimate of its covariance matrix.
Section~\ref{S:MPLE-consistency} gives conditions under which these
estimators are consistent, and Appendix~\ref{S:implementaiton} presents
an efficient algorithm for computing them.  In practice, the
Hessian of $\log \mathit{PL}_t(\cdot)$ may be singular or ill-conditioned;
Appendix~\ref{S:regularization} addresses the issue.

\section{Modeling the Enron Data}\label{S:enron-modeling}

We use the proportional intensity model of
Section~\ref{S:point-process-model} to model the Enron data.
The senders and receivers are the 156 employees in the dataset, so
$\mathcal{I} = \mathcal{J} = \{ 1, 2, \ldots, 156 \}$.  To disallow
self-loops, we set $\mathcal{J}_t(i) = \mathcal{I} \setminus \{ i \}$.
We use two types of covariates: static and dynamic.  The static covariates
don't change over time, and are determined by the sender- and
receiver-specific group memberships summarized in
Table~\ref{T:employee-summary}.  The dynamic covariates depend on the
history of the process, and are chosen to allow adjusting for reciprocation
effects.  The rest of this section discusses the covariates in depth.

Considering gender (2 levels), seniority (2 levels), and department
(3 levels), each individual belongs to one of 12 groups.  Each
sender-receiver pair involves 2 of these groups, for $12^2 = 144$ possible
group-group combinations.  We introduce a static binary covariate for each
combination.  The dyad covariate vector $x_t(i,j)$ has 144 components
corresponding to these covariates.  For concreteness, abbreviate the codes
for gender, department, and seniority by F/M, L/T/O, and J/S.  For
individual $i$, define $s(i)$ in $\reals^{12}$ by
\begin{align*}
    s(i)
    =
    \big(
        &\text{1\{$i$ is FLJ\}},
        \text{1\{$i$ is FLS\}},
        \text{1\{$i$ is FTJ\}},
        \text{1\{$i$ is FTS\}}, \\
        &\,\,\text{1\{$i$ is FOJ\}},
        \text{1\{$i$ is FOS\}},
        \text{1\{$i$ is MLJ\}},
        \text{1\{$i$ is MLS\}}, \\
        &\,\,\,\,\text{1\{$i$ is MTJ\}},
        \text{1\{$i$ is MTS\}},
        \text{1\{$i$ is MOJ\}},
        \text{1\{$i$ is MOS\}}
    \big).
\end{align*}
The 144 static covariates of $x_t(i,j)$ are the interactions,
the components of $\vecm\big(s(i) \otimes s(j)\big)$, where
$a \otimes b = a b^\trans$ and $\vecm(\cdot)$ maps a matrix to a vector
by stacking columns.  More generally, we recommend forming
dyad covariates by taking Kronecker products of sender and receiver
covariates.

Notably, the components of $\beta_0$ corresponding to the static
components are not identifiable.  Any $\tilde \beta_0$ satisfying
$\tilde \beta_0^\trans x_t(i,j) = \beta_0^\trans x_t(i,j) + \alpha_t(i)$
gives rise to the same partial likelihood.  We identify $\beta_0$ by
forcing it to have minimum $\mathcal{L}^2$ norm.  This reduces the
degrees of freedom in the static coefficients from 144 to 132 (effectively,
it imposes ``sum-to-zero'' constraints within receiver classes).

We use history-dependent covariates to capture reciprocation effects.
Specifically, we introduce 21 covariates of the form
\[
    1\{\text{$j$'s most recent message to $i$ was sent in $[t-\delta, t)$\}}.
\]
If, at time $t$, individual $j$ has never sent a message to $i$, then
these covariates are all zero.  The 21 values for the time interval
$\delta$ are of the form ``$2^k$ hours,'' where $k$ is an integer ranging
from $-6$ to $14$.  The shortest time interval is $56.25$ seconds, and the
longest is $1.87$ years.



\section{MPLE Consistency}\label{S:MPLE-consistency}

Let $N_t(i,j)$ be a multivariate counting process with stochastic
intensity $\lambda_t(i,j)$ as defined in~\eqref{E:cox-intensity}.
As before, the log-partial likelihood is given in~\eqref{E:log-pl}.
The MPLE is an obvious estimate of the true parameter vector, $\beta_0$.

Typically, one proves consistency of the MPLE by keeping $t$ fixed
and letting $|\mathcal{I}|$ go to infinity
\cite{andersen1993statistical,andersen1982cox,cook2007statistical,fleming1991counting,
martinussen2006dynamic}.  This makes sense in the context of clinical trial
data, where $\mathcal{I}$ corresponds to the set of patients.  In interaction
data, $|\mathcal{I}|$ is either fixed, or $t$ and $|\mathcal{I}|$ increase
simultaneously.  It is more natural to look at what happens as time increases.
Increasing sender sets can be handled by assuming $|\mathcal{I}|$ is infinite
(for most senders $\bar \lambda_t(i)$ is zero during the initial
part of the observation period); increasing receiver sets can be
handled similarly.


The main idea of the consistency proof is to rescale time to make the message
arrivals uniform.  This is the biggest difference between our proof and
Andersen and Gill's original proof \cite{andersen1982cox}; otherwise, the
outline of the two are roughly the same.  
Define marginal processes
\(
    N_t(i) = \sum_{j \in \mathcal{J}} N_t(i,j)
\)
and
\(
    N_t = \sum_{i \in \mathcal{I}} N_t(i).
\)
Define the sequence of stopping
times
\begin{equation}\label{E:message-times}
    \tau_n = \sup\{ t : N_t < n \},
\end{equation}
and let $\mathcal{F}_{\tau_n}$ be the $\sigma$-algebra of events prior to
$\tau_n$. The idea is to change time from the original scale to a scale on
which $\tau_{n} - \tau_{n'}$ is proportional to $n - n'$.

The first two derivatives of $\log \mathit{PL}_t(\cdot)$ are a weighted
mean and covariance of the covariates.  The weights are
\begin{subequations}
\begin{align}
    w_{t}(\beta, i,j)
        &=
        \exp\{ \beta^\trans x_t(i,j) \}
        \cdot
        1\{ j \in \mathcal{J}_t(i)\}, \\
    \text{and}\qquad
    W_{t}(\beta, i)
        &=
        \sum_{j \in \mathcal{J}} w_{t}(\beta, i,j).
\end{align}
\end{subequations}
The inner sum in $\log \mathit{PL}_t(\beta)$ is
$W_{t_m}\!(\beta, i_m)$.  The function 
$\log W_{t}(\cdot, i)$ has gradient $E_{t}(\cdot, i)$ and Hessian 
$V_{t}(\cdot, i)$, given by
\begin{subequations}
\begin{gather}
    E_{t}(\beta, i)
        =
        \frac{1}{W_{t}(\beta, i)}
        \sum_{j \in \mathcal{J}_t(i)}
            w_{t}(\beta, i,j) \, x_{t}(i,j), \\
    V_{t}(\beta, i)
        =
        \frac{1}{W_{t}(\beta, i)}
        \sum_{j \in \mathcal{J}_t(i)}
            w_{t}(\beta, i,j) 
            \Big[ x_{t}(i,j) - E_{t}(\beta, i)\Big]^{\otimes 2},
\end{gather}
\end{subequations}
where $a^{\otimes 2} = a \otimes a = a a^\trans$.
Consequently, the gradient and negative Hessian of
$\log \mathit{PL}_t(\cdot)$ are
\begin{subequations}
\begin{gather}
    \label{E:log-pl-gradient}
    U_t(\beta)
        =
        \nabla \big[ \log \mathit{PL}_t(\beta) \big]
        =
        \sum_{t_m \leq t}
            x_{t_m}(i_m, j_m) - E_{t_m}(\beta, i_m), \\
    \label{E:log-pl-neg-hessian}            
    I_t(\beta)
        =
        -\nabla^2 \big[ \log \mathit{PL}_t(\beta) \big]
        =
        \sum_{t_m \leq t}
            V_{t_m}(\beta, i_m).
\end{gather}
\end{subequations}
We call $U_t(\beta_0)$ the unnormalized score and $I_t(\beta_0)$
the Fisher information matrix.

Let $\mathcal{B}$ be a neighborhood of $\beta_0$.  For a vector, $a$, let
$\| a \|$ denote its Euclidean norm; for a matrix, $A$, let $\| A \|$ denote
its spectral norm (largest eigenvalue).  We need the following assumptions:
\begin{enumerate}[{A}1.]
    \item \label{A:square-int}
    \textbf{The covariates are uniformly square-integrable.}  That is,
    \[
        \E\left[
            \sup_{t, i, j} \| x_{t}(i,j) \|^2
        \right]
        \,\,\text{is bounded.}
    \]

    \item \label{A:integrated-cov-limit}
    \textbf{The integrated covariance function is well-behaved.}
    When $\beta \in \mathcal{B}$ and $\alpha \in [0,1]$, as $ n \to \infty$,
    \[
        \frac{1}{n}
        \sum_{i \in \mathcal{I}}
        \int_0^{\tau_{\lfloor \alpha n \rfloor}}
            V_s(\beta,i)
            \, W_s(\beta,i)
            \, \bar \lambda(i)
            \, ds
        \toP
        \Sigma_\alpha(\beta).
    \]

    \item \label{A:message-times-finite}
    \textbf{The message arrival times are finite.}  For each $n$,
    \[
        \mathbb{P}\{\tau_n < \infty\} = 1.
    \]
    
    \item \label{A:var-equicont}
    \textbf{The variance function is equicontinuous.}
    More precisely,
    \[
        \Big\{
            V_{\tau_n}(\cdot, i)
            :
            n \geq 1, i \in \mathcal{I}
        \Big\}
        \,\,\text{is an equicontinuous family of functions.}
    \]
\end{enumerate}
These assumptions imply that the MPLE is consistent and asymptotically
Gaussian.  Anderson and Gill show related assumptions hold in some
specific settings~\cite{andersen1982cox}.

\begin{theorem}\label{T:score-fisher}
    Let $N$ be a multivariate counting process on with stochastic
    intensity as given in~\eqref{E:cox-intensity}, with true parameter
    vector $\beta_0$.  Let $\tau_n$ be the sequence of message arrival times
    defined in~\eqref{E:message-times}, set $U_t(\beta)$ and $I_t(\beta)$ to
    be the gradient and negative Hessian of the partial likelihood function
    as given in~(\ref{E:log-pl-gradient}--\ref{E:log-pl-neg-hessian}).  If
    assumptions A\ref{A:square-int}--A\ref{A:integrated-cov-limit} hold, then
    the following are true as $n \to \infty$:
    \begin{enumerate}[(i)]
        \item \label{I:score-part}
        $n^{-1/2} \, U_{\lfloor \alpha \tau_n \rfloor}(\beta_0)$
        converges weakly to a Gaussian process on $[0,1]$ with
        covariance function $\Sigma_\alpha(\beta_0)$;
        
        \item \label{I:fisher-part}
        if in addition assumptions
        A\ref{A:message-times-finite}--A\ref{A:var-equicont} hold and
        if $\hat \beta_n$ is any consistent estimator of $\beta_0$,
        then
        \[
            \sup_{\alpha \in [0,1]}
            \left\|
                \tfrac{1}{n}
                I_{\lfloor \alpha \tau_n \rfloor}(\hat \beta_{n})
                -
                \Sigma_\alpha(\beta_0)
            \right\|
            \toP
            0.
        \]
    \end{enumerate}
\end{theorem}

We don't actually need convergence of the whole sample path, but
it turns out to be just as difficult to prove as convergence of the
endpoint.  Consistency is a direct consequence of
Theorem~\ref{T:score-fisher}.

\begin{theorem}\label{T:consistency}
    Let $N$ be a multivariate counting process on with stochastic
    intensity as given in~\eqref{E:cox-intensity}, with true parameter
    vector $\beta_0$.  Let the log partial likelihood,
    $\log \mathit{PL}_t(\cdot)$, be as defined in \eqref{E:log-pl}.
    Let $\tau_n$ be the sequence of message arrival times defined
    in~\eqref{E:message-times}.
    
    Assume for $\beta$ in a
    neighborhood of $\beta_0$ that
    \(
        -\tfrac{1}{n} \nabla^2 [ \log \mathit{PL}_{\tau_n}(\beta)]
            \toP \Sigma_1(\beta),
    \)
    where $\| \Sigma_1(\cdot) \|$ is locally Lipschitz and bounded
    away from zero.
    If $\hat \beta_n$ maximizes $\log \mathit{PL}_{\tau_n}(\cdot)$ and
    conclusion~(\ref{I:score-part}) of Theorem~\ref{T:score-fisher} holds,
    then the following are true as $n\to\infty$:
    \begin{enumerate}[(i)]
        \item $\hat \beta_n$ is a consistent estimator of $\beta_0$;
        \item $\sqrt{n} \, (\hat \beta_n - \beta_0)$ converges weakly
            to a mean-zero Gaussian random variable with covariance
            $[\Sigma_1(\beta_0)]^{-1}$.
    \end{enumerate}
\end{theorem}

Appendix~\ref{S:MPLE-consistency-proofs} contains the proofs of
these two theorems.  For the data application in this report,
the covariates are bounded and the message count is high, so 
assumptions A\ref{A:square-int}--A\ref{A:var-equicont} and
the asymptotic framework are natural.  


\section{Enron Parameter Estimates}\label{S:enron-estimates}

We fit the model described in Section~\ref{S:enron-modeling} to
the messages in the Enron corpus with 10 or fewer recipients by
maximizing the partial likelihood, and then get Wold-style standard
error estimates using Theorem~\ref{T:consistency}.  Our analysis excludes
messages with more than 10 recipients---a subjectively-chosen cutoff that
includes 99\% of the corpus---because many of these are ``broadcast'' and
we are only interested in inter-personal communication.  For this section,
we treat messages with multiple recipients as multiple single-recipient
messages.  

\begin{table}[h]
    \tiny
    \input{tables/group-effects}
    \caption{
        Estimated group-level effects and their standard errors for the
        model described in Section~\ref{S:enron-modeling}.  Gender,
        Department, and Seniority codes are abbreviated as F/M, L/T/O,
        and J/S.  Standard errors are computed from
        Theorem~\ref{T:consistency} using the delta method and are shown in
        parentheses.  The group-level effect of 2.46 for sender FLJ 
        and receiver FLS means that a female legal junior executive sends
        emails to a female legal senior executive at 2.46 times her baseline
        activity rate.  The highest effect in each column is shown in
        boldface; note that for females there is a strong within-group
        (homophily) effect.
    }
    \label{T:group-effects}
\end{table}

\begin{figure}[h]
    \includegraphics[scale=0.6]{figures/reciprocation}
    \caption{
        \textsc{Reciprocation Effect.}
        Estimated reciprocation effect and standard errors for the
        model described in Section~\ref{S:enron-modeling}.  The solid
        (orange) curve is determined from the 21 estimated coefficients
        for the reciprocation effects at intervals of ``\,$2^l$ hours''
        for $l$ ranging from $-6$ to $14$.  The dashed (purple) curve gives
        one standard error around the estimate as computed from 
        Theorem~\ref{T:consistency}.  Read the figure as follows: if employee
        $j$'s most recent message to employee $i$ was 30 minutes ago, then
        $i$ sends to $j$ at about 200 times his baseline rate.
        The reciprocation effect decays at a super-exponential rate, but
        persists for $1.87$ years.
    }\label{F:reciprocation}
\end{figure}

Table~\ref{T:group-effects} shows the estimated group-level
(static) effects, and Fig.~\ref{F:reciprocation} shows the reciprocation
(dynamic) effects.  Reciprocation is stronger by orders of magnitude,
amplifying the sending rate by between about $10^1$ and $10^3$.  Group level
effects, on the other hand are on the order of $10^{-1}$ to $10^0$.

\begin{table}[h]
    \input{tables/deviance.tex}
    \caption{
        Ad-hoc analysis of deviance for the model described in
        Section~\ref{S:enron-modeling}.  Residual deviance is defined as twice
        the negative log-partial likelihood when messages with multiple
        recipients are treated as multiple single-recipient messages.  The
        ``Static'' term contains the group-level effects, and the ``Dynamic''
        term contains the reciprocation effects.   Degrees of freedom for the
        model terms are the number of identifiable parameters.
    }
    \label{T:deviance}
\end{table}

Table~\ref{T:deviance} gives an ad-hoc analysis of deviance for the fitted
model, showing group-level (static) effects account for 18\% of the
residual deviance and reciprocation (dynamic) effects account for 24\%.
If the model fit perfectly, we would expect the residual deviance to be
about equal to the residual degrees of freedom; here, the residual deviance
is about $5.9$ times what we would expect.  We can account for this by
scaling the reported standard errors by $\sqrt{5.9}$.

We use the estimated group level effects to quantify the gender bias of
females and males.  This is defined as follows:
\begin{enumerate}
    \item go to the company on the day that it opens, so that there are no
          reciprocation effects;
    \item choose a random female employee proportional to how many
          messages she sent in the dataset;
    \item \label{I:bias-description-block}
          block all messages sent to this employee;
    \item wait for the first message she sends and record the gender
          of its recipient;
    \item if the probability of the recipient being female does not
          agree with the proportion of females in the company ($43 / 156$),
          then declare there to be gender bias;
    \item do the same for the first message sent by a male.
\end{enumerate}
Step \ref{I:bias-description-block} is there because if the chosen
employee receives a message before she sends one, then her sending
probabilities will change.  

\begin{table}[h]
    \label{T:gender-effects}    
    \subtable[Adjusted for Reciprocation]{
        \input{tables/gender-effects-dynamic}
        \label{T:gender-effects-dynamic}
    }
    \subtable[No Adjustment]{
        \input{tables/gender-effects-static}
        \label{T:gender-effects-static}
    }
    \caption{
        Estimated gender-level sending preferences.  Probabilities
        should be compared to the proportions of females and males
        at the company ($0.276$ and $0.724$, respectively).  See
        main text for a more thorough description.
    }
\end{table}

According to Table~\ref{T:gender-effects-dynamic}, females are biased
towards sending to females, and males are biased towards sending to
males, even after adjusting for reciprocation effects.
Table~\ref{T:gender-effects-static} shows that when we don't
adjust for reciprocation, apparent gender bias is higher in females
but about the same in males.  This suggests that females are more likely
to respond to a message than males are (though we have not formally
analyzed interactions between gender and reciprocation).

The analyses in this section all have a serious caveat, which is that
we have sidestepped the issue of messages with multiple recipients.  By
treating messages with multiple recipients as multiple single-recipient
messages, we have introduced bias in the estimates.  It turns out that
after correcting for the bias, the group-level effects are about the
same, and the reciprocation effects is slightly stronger.  The next two
sections address the multiple recipient issue and remove the caveat.


\section{Multiple recipients}

To this point, we have assumed that each message has a single recipient.
Often, messages have multiple recipients.  In the Enron dataset,
approximately 30\% of messages have more than one recipient, with
a few messages having more than fifty recipients
(see Fig.~\ref{F:recipient-counts}).  One way of dealing with multiple
recipients is, for example, treating a message with three recipients as three
separate single-recipient messages.  A less ad-hoc approach is to explicitly
take multiple recipients into account at the modeling stage.

Introduce $\bar \lambda_t(i ; L)$ as the baseline intensity at time $t$ for
messages sent by sender $i$ with $L$ recipients.  Let $J$ be any
subset of $\mathcal{J}$ and model the rate at which $i$ sends messages to
recipient set $J$ as
\begin{equation}\label{E:intensity-multiple}
    \lambda_t(i,J)
        =
        \bar \lambda_t(i ; |J|)
        \cdot
        \exp\Big\{
            \sum_{j \in J}
                \beta_0^\trans  x_t(i,j)
        \Big\}
        \cdot
        \prod_{j \in J}
        1\{ j \in \mathcal{J}_t(i) \}
        .
\end{equation}
We have chosen this form for ease of interpretability.  At time
$t$, changing receiver $j$'s covariates from $x_t(i,j)$ to
$x'_t(i,j)$ results in scaling the the rate at
which $i$ sends him messages by
\(
    \exp\big\{ \beta_0^\trans \big(x'_t(i,j) - x_t(i,j)\big) \big\};
\)
importantly, this does not depend on $\bar \lambda_t(i; L)$.

\begin{figure}
    \includegraphics[scale=0.6]{figures/recipient-counts}
    \caption{
        \textsc{Message recipient counts.}
        Each of the 21,635 points corresponds to a message, with the
        $y$-value equal to the log (base 2) of the number of recipients
        for that message.  The message sent to the most people
        had 57 recipients.  In our analysis, we exclude messages with more
        than 10 recipients.
    }\label{F:recipient-counts}
\end{figure}

The partial likelihood is nearly the same as in the single-recipient
setting from Section~\ref{S:point-process-model}.
Let $(t_1, i_1, J_1), \ldots, (t_n, i_n, J_n)$ be the sequence of
observed messages, with tuple $(t, i, J)$ indicating that at time $t$
sender $i$ sent a message with recipient set $J$.  The log partial likelihood
at time $t$, evaluated at $\beta$, is
\begin{equation}\label{E:log-pl-multiple}
    \log
    \mathit{PL}_t(\beta)
        =
        \sum_{t_m \leq t}\!
        \bigg\{\!
            \sum_{j \in J_m}\!
                \beta^\trans x_{t_m}\!(i_m, j)
            -
            \log\big[
                \!\!\!\!
                \sum_{\substack{J \subseteq \mathcal{J}_{t_m}(i_m) \\
                               |J| = |J_m|}}\!\!\!\!\!\!\!
                    \exp\big\{
                        \sum_{j \in J}
                            \beta^\trans x_{t_m}\!(i_m, j)
                    \big\}
            \big]
        \bigg\}.
\end{equation}
Compare this with the single recipient case in~\eqref{E:log-pl}.

The multiple recipient case can actually be considered as a special case of
the single recipient one if we take $\mathcal{I} \times \mathbb{N}_+$
to be the sender set and power set $\mathcal{P}(\mathcal{J})$ to be the
receiver set.  For sender $(i,L)$, take $\bar \lambda(i ; L)$ to be the
baseline send intensity and take
$\{ J \subseteq \mathcal{J}_t(i) : |J| = L\}$ to be the recipient set; for
sender-receiver pair $\big((i,L), J\big)$, take $\sum_{j \in J} x_t(i,j)$ to
be the covariate vector.  Consistency of the MPLE now follows from
Theorems~\ref{T:score-fisher}~and~\ref{T:consistency}.

In practice, \eqref{E:log-pl-multiple} involves combinatorially many terms
and is difficult to optimize.  When we treat messages with multiple recipients
as multiple single-recipient messages, we get an approximate partial
likelihood:
\begin{equation}\label{E:log-pl-multiple-approx}
    \log
    \widetilde{\mathit{PL}}_t(\beta)
        =
        \sum_{t_m \leq t}
        \bigg\{
            \sum_{j \in J_m}\!
                \beta^\trans x_{t_m}\!(i_m, j)
            -
            \log\big[
                \!\!\!\!
                \sum_{j \in \mathcal{J}_{t_m}\!(i_m)}
                    \exp\{ \beta^\trans x_{t_m}\!(i_m, j)\}
            \big]^{|J_m|}
        \bigg\}.
\end{equation}
Maximizing $\log \widetilde{\mathit{PL}}_t(\cdot)$ instead of
$\log \mathit{PL}_t(\cdot)$ introduces bias in the estimate of $\beta_0$.
Fortunately, in many situations the bias in the estimate is of the same order
as the variance. Define
\begin{equation}\label{E:growth-constant}
    G_n
        =
            \frac{1}{n}
            \sum_{t_m \leq \tau_n}
                \frac{1}{|\mathcal{J}_{t_m}(i_m)|},
\end{equation}
the receiver set growth sequence.  This sequence turns out to be critical
in bounding the error from approximating $\log \mathit{PL}$ by
$\log \widetilde{\mathit{PL}}.$

\begin{theorem}\label{T:log-pl-multiple-approx-error}
    Let $(t_m, i_m, J_m)$ be a sequence of observations from a multivariate
    point processes with intensity as given in~\eqref{E:intensity-multiple}.
    Set $\tau_n = t_n$.  Assume
    \(
        \sup_t \| x_t (i,j) \|
    \)
    and
    \(
        \sup_m | J_m |
    \)
    are bounded in probability.  
    If $\log \mathit{PL}$ and $\log \widetilde{\mathit{PL}}$ are as
    defined in
    \textnormal{(}\ref{E:log-pl-multiple}--\ref{E:log-pl-multiple-approx}\textnormal{)},
    and $G_n$ is as defined in \eqref{E:growth-constant},
    then for $\beta$ in a neighborhood of $\beta_0$,
    \[
        \Big\|
        \tfrac{1}{n}
        \nabla [\log \mathit{PL}_{\tau_n}(\beta) ]
        -
        \tfrac{1}{n}
        \nabla [\log \widetilde{\mathit{PL}}_{\tau_n}(\beta) ]
        \Big\|
            =
            \OhP(G_n),
    \]
    and
    \[
        \Big\|
        \tfrac{1}{n}
        \nabla^2 [\log \mathit{PL}_{\tau_n}(\beta) ]
        -
        \tfrac{1}{n}
        \nabla^2 [\log \widetilde{\mathit{PL}}_{\tau_n}(\beta) ]
        \Big\|
            =
            \OhP(G_n).
    \]
\end{theorem}
\begin{corollary}\label{C:mple-approx-error}
    Assume the setup of Theorem~\ref{T:log-pl-multiple-approx-error}.
    Suppose for $\beta$ in a neighborhood of $\beta_0$ that
    \(
        -
        \tfrac{1}{n}
        \nabla^2
        [ \log \mathit{PL}_{\tau_n}(\beta)]
        \toP \Sigma_1(\beta)
    \)
    and
    \(
        -
        \tfrac{1}{n}
        \nabla^2
        [ \log \widetilde{\mathit{PL}}_{\tau_n}(\beta)]
        \toP \tilde{\Sigma}_1(\beta),
    \)
    where $\| \Sigma_1 (\cdot) \|$ and $\| \tilde \Sigma_1( \cdot ) \|$
    are locally Lipschitz and bounded away from zero.
    If $\hat \beta_n$ maximizes $\log \mathit{PL}_{\tau_n}(\cdot)$
    and $\tilde \beta_n$ maximizes
    $\log \widetilde{\mathit{PL}}_{\tau_n}(\cdot)$, then
    \[
        \| \tilde \beta_{\tau_n} - \hat \beta_{\tau_n} \|
            =
            \OhP(G_n).
    \]
\end{corollary}

Corollary~\ref{C:mple-approx-error} implies that if $|\mathcal{J}_{t_m}(i_m)|$
grows sufficiently quickly and if the true MPLE, $\hat \beta_n$, is
a $\sqrt{n}$-consistent estimate of $\beta_0$, then
the approximate MPLE, $\tilde \beta_n$ is \emph{also} $\sqrt{n}$-consistent.
Growth at rate $|\mathcal{J}_{t_m}(i_m)| = \omega(\sqrt{m})$ suffices.
Moreover, if $\sqrt{n}(\hat \beta_n - \beta_0)$ is asymptotically Gaussian, 
then $\sqrt{n}(\tilde \beta_n - \beta_0)$ is asymptotically Gaussian with
the same covariance matrix (but possibly a different mean).  
Theorem~\ref{T:log-pl-multiple-approx-error} implies that under enough
regularity,
\(
    -\tfrac{1}{n} [
        \nabla^2 \log \widetilde{\mathit{PL}}_{\tau_n}(\tilde \beta_n)
    ]
\)
consistently estimates the covariance matrix.  To get the mean, we use
a parametric bootstrap simulation.



\clearpage 

\appendix

\section{The Enron Email Corpus}\label{S:enron-corpus}

The Enron email corpus is a large subset of the email messages sent within the
corporation between November 1998 and June 2002. Enron was an energy company
based in Texas that became notorious in late 2001 for its fraudulent
accounting practices. These practices eventually lead to the resignation of
its CEO, to an external audit into its accounting procedures, to massive
readjustments to its earnings statements, and eventually to its bankruptcy.
Later, many of the top executives were prosecuted and convicted of criminal
fraud and insider trading. As part of the investigation, the Federal Energy
Regulatory Commission (FERC) subpoenaed the e-mail correspondences of the top
employees and posted those not related to the criminal trial to their website
(619,446 messages, roughly 92\% of those subpoenaed).

Leslie Kaelbling, a computer scientist at MIT, purchased the corpus from the
FERC with the intention of making it available to the research community. A
team of researchers at SRI International worked to fix a number of apparent
integrity problems with the data, and then it was released to the public by
William Cohen, at CMU. This is the ``March 2, 2004 Version'' of the dataset.
Later, a former Enron employee requested that one message be removed from the
data; this removal prompted the ``August 21, 2009 Version,'' widely regarded
to be the authoritative version \cite{cohen2009enron}.

Zhou et al. have gone through substantial work to preprocess the Enron corpus
into a useful form \cite{zhou2007strategies}. We rely on their preprocessed
version for our analysis. They reduce the data to the set of e-mails sent by
high-ranking Enron executives, eliminating messages sent by non-employees,
support staff and administrative assistants. They also filter out messages
with missing timestamps. After this culling process, there are 21,635 messages
sent by 156 employees between November 13, 1998 and June 21, 2002. Each email
message has a message body consisting of text, along with header fields
including \texttt{Date}, \texttt{From}, \texttt{To}, \texttt{CC},
\texttt{BCC}, and \texttt{Subject}. The \texttt{From} field always lists a
unique e-mail address, but the \texttt{To}, \texttt{CC}, and \texttt{BCC}
fields sometimes specify multiple recipients. We make no distinction between
\texttt{To}, \texttt{CC}, and \texttt{BCC}, combining them all to determine
the recipients of a message.

We can associate a set of static covariates for each of the 156 individuals in
the dataset. Zhou et al. have extracted the employees' names, departments, and
titles. We use the forenames of the employees to associate genders to the
employees (e.g, John is male, Susan is female). For ambiguously-gendered names
like Dana, Robin, and Sean, we use personal pronouns in the messages to help
code the genders. We code the department as one of three categories: Legal,
Trading, or Other. Finally, we code the position as Senior (CEO, CFO, COO,
Director, Managing Director, VP, President) or Junior (Administrator, Analyst,
Assistant, Attorney, Counsel, Employee, Manager, Specialist, Trader).



\section{Implementation}\label{S:implementaiton}


The likelihood of the interaction process is a function of the unknown
parameter vector $\beta$ and the sender-specific baseline
intensities $\lambda(1), \ldots, \lambda(I)$.  After observing the process
up to time $t$, the likelihood is
\begin{multline*}
    L_t\big(\beta, \lambda(1), \ldots, \lambda(I)\big) \\
        =
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \lambda_s(i,j) \, dN_s(i,j)
            -
            \int_0^t
                \lambda_s(i,j) \, ds
        \bigg\}.
\end{multline*}
This factors into two terms, one which depends on all of the parameters,
and one which depends on $\beta$ alone.  We estimate $\beta$ by maximizing
the latter term, the partial likelihood, denoted $\mathit{PL}_t(\beta)$.
Define
\begin{gather*}
    w_{\beta,t}(i,j) = \exp\{ \langle x_t(i,j), \, \beta \rangle \},
    \qquad
    W_{\beta,t}(i) = \sum_{j \in \mathcal{J}(i)} w_{\beta,t}(i,j), \\
    \tilde \lambda_{\beta,t}(i)
        = \lambda_t(i) \cdot W_{\beta,t}(i).
\end{gather*}
The factorization is
\begin{align*}
    \begin{split}
    L_t\big(\beta, \lambda(1), \ldots&, \lambda(I)\big) \\
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \int_0^t
                \log \tilde \lambda_{\beta,s}(i) \, dN_s(i,j)
            -
            \int_0^t
                \tilde \lambda_{\beta,s}(i) \, ds
        \bigg\}
        \cdot
        \mathit{PL}_t(\beta),
    \end{split} \\
    \mathit{PL}_t(\beta)
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \frac{w_{\beta,s}(i,j)}
                          {W_{\beta,s}(i)}
                \, dN_s(i,j)
        \bigg\}.
\end{align*}

Suppose $(t_1, i_1, j_1), \ldots, (t_M, i_M, j_M)$ is the sequence of observed
messages, where tuple $(t,i,j)$ indicates that at time $t$, sender $i$ sent a
message to receiver $j$.  Define the following message sets:
\begin{align*}
  \mathcal{M}_t(i,j)
    &= \{ m : t_m \leq t, \, i_m = i, \, j_m = j \}, \\
  \mathcal{M}_t(i)
    &= \cup_{j \in \mathcal{J}} \mathcal{M}_t(i,j), \\
  \mathcal{M}_t
    &= \cup_{i \in \mathcal{I}} \mathcal{M}_t(i).
\end{align*}
The partial likelihood factors a a product of terms, one for each sender:
\begin{align*}
    \mathit{PL}_t(\beta)
        &=
        \,\,
        \prod_{i \in \mathcal{I}}
            \,\,
            \mathit{PL}_t(\beta, i), \\
    \mathit{PL}_t(\beta, i)
        &=
        \!\!\!\!
        \prod_{m \in \mathcal{M}_t(i)}
            \!\!\!
            \frac{w_{\beta, t_m} (i, j_m)}
                 {W_{\beta, t_m}}.
\end{align*}
This factorization allows us to compute $\log \mathit{PL}_t(\beta)$ and
its derivatives by computing the sender-specific terms in parallel and
then adding them together.

The sender-specific log-partial likelihood and its derivatives are
\begin{align*}
    \log \mathit{PL}_t(\beta, i)
        &=
        \!\!\!\!
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!\!        
            \langle x_{t_m}\!(i, j_m), \, \beta \rangle
            \,
            -
            \,
            \log W_{\beta, t_m}\!(i), \\
    \nabla [ \log \mathit{PL}_t(\beta, i) ]
        &=
        \!\!\!\!        
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!\!
            x_{t_m}\!(i,j_m)
            \,
            -
            \,
            \frac{1}{W_{t_m}\!(i)}
                \sum_{j \in \mathcal{J}(i)}
                    w_{t_m}\!(i,j) \,
                    x_{t_m}\!(i,j), \\
    \begin{split}
    \nabla^2 [ \log \mathit{PL}_t(\beta, i) ]
        &=
        -
        \!\!\!\!
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!
            \bigg\{
            \frac{1}{W_{t_m}\!(i)}
            \sum_{j \in \mathcal{J}(i)}
                w_{t_m}\!(i,j) \,
                \big[
                    x_{t_m}\!(i,j)
                \big]^{\otimes 2} \\
        &\qquad\qquad\qquad-
            \Big[
                \frac{1}{W_{t_m}\!(i)}
                \sum_{j \in \mathcal{J}(i)}
                    w_{t_m}\!(i,j) \,
                    x_{t_m}\!(i,j)
            \Big]^{\otimes 2}
            \bigg\},
    \end{split}
\end{align*}
where $a^{\otimes 2} = a a^\trans$.  When $x_t(i,j)$ is constant over time,
there are sufficient statistics for $\beta$ and these formulas reduce.
Otherwise, computing $\log \mathit{PL}_t(\beta,i)$ and its derivatives
requires iterating over all messages, potentially taking time
$\Oh(|\mathcal{M}_t(i)| \cdot |\mathcal{J}(i)| \cdot p^2)$.  
The serial computation time for computing the full partial likelihood
and its derivatives is $\Oh(M \cdot J \cdot p^2)$.
For the Enron data, $M \approx 20,\!000$, $J \approx 150$,
and $p \approx 200$.  This is relatively small by modern standards.  Often
$M$ on the order of $10^9$ and $J$ on the order of $10^6$.  Computations
taking time $\Oh(M \cdot J)$ are prohibitive for large datasets.

Taking advantage of sparsity, the serial computation time reduces to
$\Oh(M \cdot p^2 + I \cdot J)$.  Decompose $x$ into its static
(non time-varying) and dynamic parts:
\[
    x_t(i,j)
        = x_0(i,j) + d_t(i,j).
\]
Typically, the dynamic part, $d_t(i,j)$, is sparse, with at most $\bar p$
nonzero components.  Moreover $d_t(i,j)$ is zero for most $(i,j)$
pairs---often $d_t(i,j)$ is zero unless $i$ and $j$ have exchanged
messages in the past.  Let
\[
    \mathcal{\bar J}(i)
        =
        \{
            j \in \mathcal{J}(i) : d_t(i,j) \neq 0 
            \,\,
            \text{for some $t$}
        \}.
\]
Assume $|\mathcal{\bar J}(i)| \ll |\mathcal{J}(i)|$ and that 
computing $d_t(i,j)$ takes amortized time $\Oh(\bar p)$ for each
$(t,i,j)$ triple.

Notice
\begin{align*}
    w_{\beta,t}(i,j)
        &=
            w_{\beta,0}(i,j)
            \cdot
            \exp\{ \langle d_t(i,j), \beta \rangle \}, \\
    W_{\beta,t}(i)
        &=
            W_{\beta,0}(i)
            +
            \sum_{j \in \mathcal{\bar J}(i)}
                w_{\beta,t}(i,j) - w_{\beta,0}(i,j)
\end{align*}
It takes time $\Oh(|\mathcal{J}(i)| \cdot p)$ to compute the values of
$W_{\beta,0}(i)$ and $w_{\beta,0}(i,j)$ for any $i$ and all
$j$ in $\mathcal{J}(i)$; once these values are known, computing the values
of $W_{\beta,t}(i)$ and $w_{\beta,t}(i,j)$ takes time
\(
    \Oh(|\mathcal{\bar J}(i)| \cdot \bar p).
\)
Since
\[
    \sum_{m\in \mathcal{M}_t(i)}
        x_{t_m}\!(i,j_m)
        =
            \sum_{j \in \mathcal{J}(i)}
                \mathcal{M}_t(i,j) \, x_0(i,j)
            \,\,\,
            +
            \sum_{m \in \mathcal{M}_t(i)}
                d_{t_m}\!(i,j),
\]
%computing sum on the left hand side takes time
%\(
%    \Oh(|\mathcal{M}_t(i)| \cdot \bar p + |\mathcal{J}(i)| \cdot p);
%\)
computing $\log \mathit{PL}_t(\beta, i)$ takes time
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p
        +
        |\mathcal{J}(i)| \cdot p
    ).
\)

The second term in the gradient is
\begin{multline*}
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta,t_m}\!(i)}
        \sum_{j \in \mathcal{J}(i)}
            w_{\beta,t_m}\!(i,j)
            \,
            x_{t_m}\!(i,j) \\
    =
    \sum_{j \in \mathcal{J}(i)}
    \bigg[
        \sum_{m \in \mathcal{M}_t(i)}
            \frac{w_{\beta,t_m}\!(i,j)}{W_{\beta,t_m}\!(i)}
    \bigg]
    \cdot
    x_0(i,j) \\
    +
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta, t_m}\!(i)}
        \sum_{j \in \mathcal{\bar J}(i)}
            w_{\beta,t_m}\!(i,j)
            \,
            d_{t_m}\!(i,j).
\end{multline*}
Also,
\[
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{w_{\beta,t_m}\!(i,j)}{W_{\beta,t_m}\!(i)}
    =
        w_{\beta,0}(i,j)
        \cdot
        \sum_{m \in \mathcal{M}_t(i)}
            \frac{
                \exp\{\langle d_{t_m}\!(i,j), \, \beta \rangle \}
            }{
                W_{\beta,t_m}\!(i)
            };
\]
the latter term is constant when $j \notin \mathcal{\bar J}(i)$.
Organizing the computations as suggested by these formula,
the additional overhead for computing the gradient of
$\log \mathit{PL}_t(\beta,i)$ is
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p
        +
        |\mathcal{J}(i)| \cdot p
    )
\).
This is the same computational complexity as for computing
$\log \mathit{PL}_t(\beta,i)$.


Suppose $|\mathcal{J}(i)| \leq J$, $|\mathcal{\bar J}(i)| \leq \bar J$,
and $|\mathcal{M}_t(i)| \leq M$.  By organizing the computations as described
above, we can compute $\log \mathit{PL}_t(\beta)$ and its gradient in serial
time
\(
    \Oh(
        | \mathcal{I} |
        \cdot
        M
        \cdot
        \bar J
        \cdot
        \bar p
        +
        |\mathcal{I}| \cdot J \cdot p
    ).
\)
With $\Oh(|\mathcal{I}|)$ processors, we can compute these quantities in time
\(
    \Oh(
        M \cdot \bar J \cdot \bar p
        +
        J \cdot p
        +
        |\mathcal{I}| \cdot p
    ).
\)

\section{Estimation with an Ill-conditioned Hessian}\label{S:regularization}

When the Hessian of $\log \mathit{PL}_t(\cdot)$ is singular or
ill-conditioned, its inverse may not exist, and it is necessary to modify
the MPLE-based estimators.

\section{Proofs of MPLE Consistency Results}\label{S:MPLE-consistency-proofs}

\begin{proof}[Proof of Theorem~\ref{T:score-fisher}, Part \textit{(\ref{I:score-part})}]
The process $N_t(i,j)$ has compensator
\begin{equation}
    \Lambda_t(i,j)
        =
            \int_0^t \lambda_s(i,j) \, ds;
\end{equation}
similarly, processes $N_t(i)$ and $N_t$ have compensators
$\Lambda_t(i) = \sum_{j \in \mathcal{J}} \Lambda_t(i,j)$
and $\Lambda_t = \sum_{i \in \mathcal{I}} \Lambda_t(i)$.  Define local
martintagles $M_t(i,j) = N_t(i,j) - \Lambda_t(i,j)$,
$M_t(i) = N_t(i) - \Lambda_t(i)$, and 
$M_t = N_t - \Lambda_t$.  

The score function evaluated at the true parameter vector has a simple
representation in terms of these martingales.  Set
$H_t(i,j) = x_t(i,j) - E_t(\beta_0,i)$.  Since $x$ is uniformly
bounded, $H$ is as well.  Using the identity
\(
    \sum_{j \in \mathcal{J}}
    \int_0^t
        H_s(i,j) \,
        d\Lambda_s(i,j)
    =
    0,
\)
the score can be written as
\begin{align*}
    U_t(\beta_0)
        &=
        \sum_{i \in \mathcal{I}}
        \sum_{j \in \mathcal{J}}
        \int_0^t
            H_s(i,j) \, dN_s(i,j) \\
        &=
        \sum_{i \in \mathcal{I}}
        \sum_{j \in \mathcal{J}}
        \int_0^t
            H_s(i,j) \, dM_s(i,j).
\end{align*}
Each term in the sum is locally square integrable, with predictable
covariation
\begin{align*}
    \begin{split}
        \bigg\langle
            \int
                H_s(i,j) \, dM_s(i,j)
        &, \, \,
            \int
                H_s(i',j') \, dM_s(i',j')
        \bigg\rangle_t \\
        &=
            \int_0^t
                H_s(i,j) \otimes H_s(i',j') \,
                d\big\langle M(i,j), M(i',j')\big\rangle_s
    \end{split} \\
        &=
            \int_0^t
                \big[ H_s(i,j) \big]^{\otimes 2} \,
                d\Lambda_s(i,j)
            \cdot
            1\{ i = i', j = j' \}
\end{align*}
\cite[Thm.~2.4.3]{fleming1991counting}.  There exists a sequence
of stopping times localizing all $M(i,j)$ simultaneously, so $U(\beta_0)$ is
locally square integrable with predictable variation
\begin{align}
\begin{split}\label{E:score-compensator}
    \big\langle U(\beta_0) \big\rangle_t
        &=
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \big[ H_s(i,j) \big]^{\otimes 2} \,
                d\Lambda_s(i,j) \\
        &=
            \sum_{i \in \mathcal{I}}
            \int_0^t
                V_s(\beta_0, i) \,
                d\Lambda_s(i).
\end{split} \\
\intertext{Note}\label{E:var-estimate}
    I_t(\beta)
        &=
            \sum_{i \in \mathcal{I}}
            \int_0^t
                V_s(\beta, i) \,
                dN_s(i).
\end{align}

Now we rescale time.
For each positive $n$ define a discretized time-scaled version of the score.  
The process is defined for times in $[0,1]$; between times in
$[\tfrac{k}{n}, \tfrac{k+1}{n})$, it takes the value $U_{\tau_k}$.  That is,
\[
    \tilde U_{\alpha}^{(n)}(\beta)
        = U_{\lfloor \alpha \tau_n \rfloor}(\beta).
\]
This is right-continuous with limits from the left.

We claim $\tilde U_\alpha^{(n)}(\beta_0)$ is a square-integrable martingale
adapted to
\(
    \mathcal{\tilde F}^{(n)}_\alpha
        =
        \mathcal{F}_{\lfloor \alpha \tau_n \rfloor}.
\)
The conditional expectation property holds provided
\(
    \E[ U_{\tau_{n}}(\beta_0) \mid \mathcal{F}_{\tau_{n-1}} ]
        = U_{\tau_{n-1}}(\beta_0).
\)
Define $K = \sup_{t,i,j} \| x_{t}(i,j) \|$.
Note that $\|H_{t}(i,j)\| \leq 2 K$.  Thus,
\begin{align*}
    \| U_{t \wedge \tau_n} (\beta_0) \|
        &\leq
            2 K
            \big(
                N_{t \wedge \tau_n}
                +
                \Lambda_{t \wedge \tau_n}
            \big), \\
\intertext{so that}
    \E \left[
        \sup_t
        \| U_{t \wedge \tau_n} (\beta_0) \|^2
    \right]
        &\leq
            8 \cdot \big(\E K^2\big)^{1/2} \cdot
            \big(
              \E N_{\tau_n}^2
              +
              \E \Lambda_{\tau_n}^2              
            \big)^{1/2}.
\end{align*}
By assumption A\ref{A:square-int}, $\E K^2$ is finite, and by construction,
$N_{\tau_n}$ is bounded.  Since $N_{t \wedge \tau_n}$ is a counting process,
$\E \Lambda_{\tau_n}^2$ is finite, too
(this follows from results in Section 2.3 of Fleming and Harrington
\cite{fleming1991counting}).  Thus, $U_{t \wedge \tau_n}(\beta_0)$
is uniformly integrable.  The Optional Sampling Theorem now applies to
give the conditional expectation property of $\tilde U^{(n)}(\beta_0)$.  For
square integrability, note
\(
    \sup_{1 \leq m \leq n}
    \E \| U_{\tau_m} \|^2
        \leq
        \E \left[
           \sup_t
           \| U_{t \wedge \tau_n} (\beta_0) \|^2
        \right].
\)

Since it only depends on values at jump times, the
quadratic variation of $\tilde U^{(n)}(\beta_0)$ at time $\alpha$ is
equal to the quadratic variation of $U(\beta_0)$ at time
$\tau_{\lfloor \alpha n \rfloor}$.  Therefore, since quadratic and predictable
variation have the same limit when it exists
\cite[Prop. 1]{rebolledo1980central}, assumption A\ref{A:integrated-cov-limit}
implies that
\(
    \langle \frac{1}{\sqrt{n}} \tilde U^{(n)}(\beta_0) \rangle_\alpha
        \toP
            \Sigma_\alpha(\beta_0).
\)
When $\frac{1}{\sqrt{n}} \tilde U^{(n)}(\beta_0)$ satisfies a Lindeberg condition,
the process converges in distribution to a Gaussian process with covariance
function $\Sigma(\beta_0).$

The Lindeberg condition is that for any positive $\varepsilon$,
\[
    \frac{1}{n}
    \sum_{i,j}
    \int_{0}^{\tau_n}
        \| H_s(i,j) \|^2
        \, 1\{\| H_s(i,j) \| > \sqrt{n} \varepsilon \}
        \, d\Lambda_s(i,j)
        \toP
        0.
\]
With $K = \sup_{t,i,j} \| x_t(i,j)\|$ as above, the integral is bounded by
\(
    4 \, K^2 \, 1\{n^{-1/2} K > \varepsilon / 2\}
    \cdot
    \frac{\Lambda_{\tau_n}}{n}.
\)
Since $\E K^2 < \infty$, the first term converges to zero in probability.
Since $\E \Lambda_{\tau_n} = \E N_{\tau_n} = n$, the product of the two also
converges to zero in probability.  Thus, the Lindeberg condition is satisfied.
The weak convergence result now follows from Rebolledo's Martingale Central
Limit Theorem~\cite{rebolledo1980central}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{T:score-fisher}, Part \textit{(\ref{I:fisher-part})}]
Part~\textit{(\ref{I:fisher-part})} is a consequence of equicontinuity
and Lenglart's Inequality~\cite{lenglart1977relation}.  As in the proof of
part~\textit{(\ref{I:score-part})}, set $K = \sup_{t,i,j} \| x_t(i,j) \|$.

Recall \eqref{E:score-compensator} and \eqref{E:var-estimate}.
When $\alpha \in [0, 1]$,
\begin{align*}
    \left\|
        \tfrac{1}{n} I_{\lfloor \alpha \tau_n \rfloor}(\hat \beta_n)
        -
        \Sigma_{\alpha} (\beta_0)
    \right\|
        &\leq
        \left\|
            \frac{1}{n}
            \int_0^{\lfloor \alpha \tau_n \rfloor}
                \{
                    V_s(\hat \beta_n, i)
                    -
                    V_s(\beta_0, i)
                \} \, dN_s(i)
        \right\| \\
        &\quad+
        \left\|
            \frac{1}{n}
            \sum_i
            \int_0^{\lfloor \alpha \tau_n \rfloor}
                V_s(\beta_0, i) \, dM_s(i)
        \right\| \\
        &\quad+
        \left\|
            \frac{1}{n}
            \sum_i
            \int_0^{\lfloor \alpha \tau_n \rfloor}
                V_s(\beta_0)
                \, d\Lambda_s(i)
            -
            \Sigma_{\alpha}(\beta_0)
        \right\|.
\end{align*}
The first term is uniformly bounded by 
\(
    \sup_{n',i}
        \|
            V_{\tau_{n'}}(\hat \beta_n, i)
            -
            V_{\tau_{n'}}(\beta_0, i)
        \|,
\)
which converges to zero since $\hat \beta_n \toP \beta_0$ and
$\{ V_{\tau_{n'}}(\cdot, i) \}$ is an equicontinuous family
(assumption~A\ref{A:var-equicont}).
The third term converges to zero by assumption~A\ref{A:integrated-cov-limit}.
Lenglart's Inequality and assumption~A\ref{A:message-times-finite} imply that for
any postive $\rho$ and $\delta$,
\begin{multline*}
    \mathbb{P}\left\{
        \sup_{t \in [0,\tau_n]}
        \left\|
            \frac{1}{n}
            \sum_{i}
            \int_{0}^{t}
                V_s(\beta_0, i) \, dM_s(i)
        \right\|
        \geq \rho
    \right\} \\
    \leq
    \frac{\delta}{\rho^2}
    +
    \mathbb{P}\left\{
        \frac{1}{n^2}
        \sum_{i}
        \int_{0}^{\tau_n}
            \| V_s (\beta_0, i) \|^2
            \, d\Lambda_s(i)
        \geq
        \delta
    \right\}.
\end{multline*}
(see Fleming and Harrington's Cor.~3.4.1 for a related proof
\cite{fleming1991counting}).  The sum in the second term is bounded by
$\frac{K^4}{n} \cdot \frac{\Lambda_{\tau_n}}{n}$.  Since $n^{-1/2} K^2 \toP 0$
and $\E \Lambda_{\tau_n} = n$, the right hand side of the inequality converges
to $\frac{\delta}{\rho^2}$.  Since $\delta$ is arbitrary, the right hand side
must converge to zero.  This proves that the second term in the bound on
\(
    \left\|
        \tfrac{1}{n} I_{\lfloor \alpha \tau_n \rfloor}(\hat \beta_n)
        -
        \Sigma_{\alpha} (\beta_0)
    \right\|
\)
converges to zero uniformly in $\alpha$, concluding the proof.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{T:consistency}]
We follow Haberman's approach to proving
consistency, which relies on Kantorovich's analysis
of Newton's method.  Tapia gives an elementary proof of the Kantorovich
Thereom~\cite{haberman1977maximum,kantorovich1952functional,tapia1971kantorovich}.
We state a weak form of the the result as a lemma.

\begin{lemma}[Kantorovich Theorem]
    Let $P(x) = 0$ be a general system of nonlinear equations, where $P$ is
    a map between two Banach spaces.  Let $P'(x)$ denote the Jacobian
    (Fr\'echet differential) of $P$ at $x$, assumed to exist in $D_0$,
    a convex open neighborhood of $x_0$.  Assume the following:
    \begin{enumerate}[(i)]
        \item $\| [P'(x_0)]^{-1} \| \leq B$,
        \item $\| [P'(x_0)]^{-1} P(x_0) \| \leq \eta$,
        \item $\| P'(x) - P'(y) \| \leq K \| x - y \|$,\quad
            for all $x$ and $y$ in $D_0$,
    \end{enumerate}
    with $h = B K \eta \leq \tfrac{1}{2}$.
    
    Let $\Omega_\ast = \{ x : \| x - x_0 \| \leq 2 \eta \}$.
    If $\Omega_\ast \subset D_0$, then the Newton iterates,
    $x_{k+1} = x_k - [P'(x_k)]^{-1} P(x_k)$, are well defined, remain
    in $\Omega_\ast$, and converge to $x^\ast$ in $\Omega_\ast$ such
    that $P(x^\ast) = 0$.  In addition,
    \[
        \| x^\ast - x_k \|
            \leq
                \frac{\eta}{h}
                \frac{(2h)^{2^k}}{2^k},
        \qquad
        k = 0, 1, 2, \ldots.
    \]
\end{lemma}

Set $U_t(\cdot)$ and $I_t(\cdot)$ to be the gradient and negative
Hessian of the log partial likelihood, as defined in
(\ref{E:log-pl-gradient}--\ref{E:log-pl-neg-hessian}).  Since
$I_t(\beta)$ is a sum of rank-one matrices with positive weights,
it is positive semi-definite, and $\log \mathit{PL}_t(\cdot)$ is
a concave function.  By the assumption that $\| \Sigma_1(\cdot) \|$
is bounded away from zero in a neighborhood of $\beta_0$, for $n$
sufficiently large, if $\log \mathit{PL}_t(\cdot)$ has a local minimum
in that neighborhood then it must be the unique global maximum.

We find the local maximum by applying Newton's method to the
gradient of $\tfrac{1}{n} \log \mathit{PL}_{\tau_n}(\cdot)$, taking
$\beta_0$ as the initial iterate.  Define
\[
    Z_n = -[I_{\tau_n}(\beta_0)]^{-1} U_{\tau_n}(\beta_0).
\]
Theorem~\ref{T:score-fisher} and the
assumptions of the theorem imply that $[I_{\tau_n}(\beta_0)]^{-1}$
exists for $n$ large enough so that $Z_n$ is well-defined.  Moreover,
(i) $Z_n \toP 0$, and
(ii) $\sqrt{n} \, Z_n \tod \Normal(0, \, [\Sigma_1(\beta_0)]^{-1})$.
The first Newton iterate, $\beta_{n,1}$ is equal to $\beta_0 - Z_n$.

Apply Kantorovich's Theorem to bound $\| \hat \beta_n - \beta_0 \|$
and $\| \hat \beta_n - \beta_{n,1} \|$.  By assumption, there exists
a neighborhood of $\beta_0$, say $D_0$, and finite $K$ and $B$, such that 
\(
    \|
        \frac{1}{n} I_{\tau_n} (\beta)
        -
        \frac{1}{n} I_{\tau_n}(\beta')
    \|
    \leq
    K
    \|
        \beta
        -
        \beta'
    \|
\)
and
\(
    \| \frac{1}{n} [I_{\tau_n}(\beta_0)]^{-1} \| \leq B
\)
for $\beta, \beta' \in D_0$.
Define $\eta_n = \| Z_n \|$ and $h_n = B K \eta_n$, noting that $h_n$ and
$\eta_n$ are size $\OhP(n^{-1/2})$.  Thus, for $n$ large enough, the
Newton iterates converge to $\hat \beta_n$.  Moreover,
\begin{enumerate}[(i)]
    \item $\| \hat \beta_n - \beta_0 \| \leq 2 \, \eta_n \toP 0$,
    \item 
        \(
            \sqrt{n} \, \| \hat \beta_n - (\beta_0 - Z_n) \|
            \leq
            2 \sqrt{n} \, \eta_n \, h_n
            \toP 0.
        \)
\end{enumerate}
Thus, $\hat \beta_n \toP \beta_0$, and $\sqrt{n} (\hat \beta_n - \beta_0)$
and $\sqrt{n} \, Z_n$ converge weakly to the same limit.

\end{proof}


\section{Proofs of Multiple Recipient Results}\label{S:multiple-recipient-proofs}
 
\begin{proof}[Proof of Theorem~\ref{T:log-pl-multiple-approx-error}]

For subset $J \subseteq \mathcal{J}_t(i)$, set
\begin{gather}
    X_t(i, J) = \sum_{j \in J} x_t(i,j), \\
    w_t(\beta, i, J)
        = \exp\{ \beta^\trans X_t(i,J) \}.
\end{gather}
As a slight abuse of notation, when $j$ is an element of $\mathcal{J}_t(i)$,
take ``$w_t(\beta, i, j)$'' to mean $w_t(\beta, i, \{j\})$.

Define weights
\begin{gather}
    W_t(\beta, i; L)
        = \sum_{\substack{J \subseteq \mathcal{J}_t(i), \\
                          |J| = L}}
              w_t(\beta,i,J), \\
    \widetilde W_t(\beta, i; L)
        =
            \Big[ \sum_{j \in \mathcal{J}_t(i)} w_t(\beta, i, j) \Big]^L.
\end{gather}

Define gradients
\begin{gather}
    E_t(\beta, i; L)
        = \nabla \big[ \log W_t(\beta, i; L) \big]
        =
        \frac{1}{W_t(\beta, i; L)}
        \sum_{\substack{J \subseteq \mathcal{J}_t(i), \\
                        |J| = L}}\!
            w_t(i,J)
            \,
            X_t(i,J), \\
    \widetilde E_t(\beta, i; L)
        = \nabla \big[ \log \widetilde W_t(\beta, i; L) \big]
        =
        L
        \cdot
        \frac{
            \sum_{j \in \mathcal{J}_t(i)}
                w_t(\beta, i, j) \, x_t(i,j)
        }{
            \sum_{j \in \mathcal{J}_t(i)}
                w_t(\beta, i, j)
        }.
\end{gather}
The second is the expectation of $\sum_{l = 1}^L x_t(i, j_l)$ when
$j_1, \ldots, j_L$ are drawn independently and identically from
$\mathcal{J}_t(i)$ with weights $w_t(\beta, i, \cdot)$; the first is the same
expectation, conditional on the event that $j_1, \ldots, j_L$ are all unique.
Let $\tilde{\mathbb{P}}_{t,\beta,i;L}$ and $\mathbb{P}_{t,\beta,i;L}$
denote the two probability laws for $j_1, \ldots, j_L$, and let
$\tilde{\mathbb{E}}_{t,\beta,i;L}$ and $\mathbb{E}_{t,\beta,i;L}$ denote
expectations with respect to them, so that
$E_t(\beta,i;L) = \mathbb{E}_{t,\beta,i;L} \big[ \sum_{l=1}^L x_t(i,j_l)\big]$
and
\(
    \widetilde E_t(\beta,i;L)
    =
    \tilde{\mathbb{E}}_{t,\beta,i;L} \big[ \sum_{l=1}^L x_t(i,j_l)\big].
\)

The Hessians are
\begin{align}
    \begin{split}
    V_t(\beta,i;L)
        &=
        \nabla^2 \big[  \log W_t(\beta, i; L) \big] \\
        &=
        \frac{1}{W_t(\beta,i;L)}
        \sum_{\substack{J \subseteq \mathcal{J}_t(i), \\
                        |J| = L}}
            w_t(\beta,i,J)
            \Big[
                X_t(i,J)
                -
                E_t(\beta,i;L)
            \Big]^{\otimes 2},
    \end{split} \\
    \begin{split}
    \widetilde V_t(\beta,i;L)
        &=
        \nabla^2 \big[ \log \widetilde W_t(\beta,i;L) \big] \\
        &=
        L
        \cdot
        \frac{
            \sum_{j \in \mathcal{J}_t(i)}
                w_t(\beta, i, j)
                \Big[ x_t(i,j) - \tfrac{1}{L} \widetilde E_t(\beta,i;L) \Big]^{\otimes 2}
        }{
            \sum_{j \in \mathcal{J}_t(i)}
                w_t(\beta, i, j)
        }.
    \end{split}
\end{align}
The first is the covariance matrix of $\sum_{l=1}^L x_t(i,j_l)$ under
$\mathbb{P}_{t,\beta,i;L}$; the second is the covariance matrix of the same
quantity under $\tilde{\mathbb{P}}_{t,\beta,i;L}$.

Let $\hat \beta_t$ maximize $\log \mathit{PL}_t(\cdot)$ and let
$\tilde \beta_t$ maximize $\log \widetilde{\mathit{PL}}_t(\cdot)$.  We
have
\[
    \| \tilde \beta_t - \beta_0 \|
        \leq
        \| \hat \beta_t - \beta_0\|
        +
        \| \hat \beta_t - \tilde \beta_t \|.
\]
We know $\hat \beta_t$ is a $\sqrt{n}$-consistent estimator of
$\beta_0$.  To get consistency of $\tilde \beta_t$, we show
$\tilde \beta_t$ is close to $\hat \beta_t$.

Write
\[
    \nabla \big[ \log \widetilde{\mathit{PL}}_t(\beta) \big]
        =
        \nabla \big[ \log \mathit{PL}_t(\beta )\big]
        +
        \sum_{t_m \leq t}
            E_{t_m}\!(\beta, i_m; |J_m|)
            -
            \widetilde{E}_{t_m}\!(\beta, i_m; |J_m|),
\]
so that
\[
    \nabla \big[ \log \widetilde{\mathit{PL}}_t(\hat \beta_t) \big]
        =
        \sum_{t_m \leq t}
            E_{t_m}\!(\hat \beta_t, i_m; |J_m|)
            -
            \widetilde{E}_{t_m}\!(\hat \beta_t, i_m; |J_m|).
\]
Recalling the representation in terms of probability laws $\mathbb{P}$ and
$\tilde{\mathbb{P}}$,
\[
    E_{t}(\beta, i; L) - \widetilde{E}_t(\beta, i; L)
        =
        \mathbb{E}_{t,\beta,i;L}
            \Big[ \sum_{l=1}^L x_t(i,j_l) \Big]
        -
        \widetilde{\mathbb{E}}_{t,\beta,i;L}
            \Big[ \sum_{l=1}^L x_t(i,j_l) \Big].
\]
We bound the difference by coupling $\mathbb{P}$ and $\widetilde{\mathbb{P}}$.
We define probability law $\mathbb{P}^\ast_{t,\beta,i;L}$ and associated
random variables $j_1, \ldots, j_L$ and
$\tilde \jmath_1, \ldots, \tilde \jmath_L$, such that marginally
$j_1, \ldots, j_L$ are distributed according to $\mathbb{P}_{t,\beta,i;L}$
and $\tilde \jmath_1, \ldots, \tilde \jmath_L$ are distributed according
to $\tilde{\mathbb{P}}_{t,\beta,i;L}$.  Then,
\begin{align*}
    \Big\| E_{t}(\beta, i; L) &- \widetilde{E}_t(\beta, i; L) \Big\| \\
        &=
            \Big\|
            \mathbb{E}_{t,\beta,i;L}^\ast
            \Big[
                \sum_{l=1}^L x_t(i,j_l)
                -
                \sum_{l=1}^L x_t(i, \tilde \jmath_l)
            \Big] 
            \Big\| \\
        &\leq
            2 L
            \cdot
            \Big[
                \sup_{j \in \mathcal{J}_t(i)}
                \| x_t(i,j) \|
            \Big]
            \cdot
            \mathbb{P}^\ast_{t,\beta,i;L}
            \Big\{
                (j_1, \ldots, j_L)
                    \neq
                    (\tilde \jmath_1, \ldots, \tilde \jmath_L)
            \Big\}
\end{align*}
The coupling is as follows:
\begin{enumerate}
    \item Draw $(\tilde \jmath_1, \ldots, \tilde \jmath_L)$ according to
        $\tilde{\mathbb{P}}_{t,\beta,i;L}$.
    \item If $(\tilde \jmath_1, \ldots, \tilde \jmath_L)$ are all unique,
        set $(j_1, \ldots, j_L) = (\tilde \jmath_1, \ldots, \tilde \jmath_L)$,
        otherwise draw $(j_1, \ldots, j_L)$ independently according to
        $\mathbb{P}_{t,\beta,i;L}$.
\end{enumerate}
Now,
\begin{align*}
    \mathbb{P}^\ast_{t,\beta,i;L}
    \Big\{
        (j_1, \ldots, j_L)
            \neq
            (\tilde \jmath_1, \ldots, \tilde \jmath_L)
    \Big\}
        &=
        \mathbb{P}^\ast_{t,\beta,i;L}
        \Big\{
            \text{$\tilde \jmath_1, \ldots, \tilde \jmath_L$ has a duplicate}
        \} \\
        &\leq
        \sum_{k \leq l}
            \mathbb{P}^\ast_{t,\beta,i;L}
            \Big\{
                \tilde \jmath_k = \tilde \jmath_l
            \Big\} \\
        &=
            {L \choose 2}
            \sum_{j \in \mathcal{J}_t(i)}
                \Big[
                    \frac{
                        w_t(\beta, i, j)
                    }{
                        \sum_{j' \in \mathcal{J}_t(i)} w_t(\beta, i, j')
                    }
                \Big]^2.
\end{align*}
Set $K = \sup_t \| x_t(i,j) \|$.  Then, 
\[
    \exp\{-K \, \| \beta \|\}
        \leq w_t(\beta,i,j)
        \leq \exp\{K \| \, \beta \|\},
\]
so that
\[
    \mathbb{P}^\ast_{t,\beta,i;L}
    \Big\{
        (j_1, \ldots, j_L)
            \neq
            (\tilde \jmath_1, \ldots, \tilde \jmath_L)
    \Big\}
        \leq
        {L \choose 2}
        \cdot
        \frac{\exp\{4 K \, \| \beta \|\}}{| \mathcal{J}_t(i) |},
\]
and
\[
    \Big\| E_{t}(\beta, i; L) - \widetilde{E}_t(\beta, i; L) \Big\|
        \leq
        K \, L^2 \, (L - 1)
        \,
        \frac{\exp\{4 K \, \| \beta \|\}}{| \mathcal{J}_t(i) |}.
\]
By a similar argument,
\[
    \Big\| V_{t}(\beta, i; L) - \widetilde{V}_t(\beta, i; L) \Big\|
        \leq
        2\, K^2 \, L^3 \, (L - 1)
        \,
        \frac{\exp\{4 K \, \| \beta \|\}}{| \mathcal{J}_t(i) |}.
\]
Thus,
\[
    \Big\|
        \nabla\big[ \log \widetilde{\mathit{PL}}_t(\hat \beta _t ) \big]
    \Big\|
        \leq
            K
            \exp\{4 K \| \beta \|\}
            \cdot
            \sum_{t_m \leq t}
                \frac{|J_m|^2 \, (|J_m| - 1)}{|\mathcal{J}_{t_m}(i_m)|},
\]
and
\begin{multline*}
    \Big\|
        \nabla^2\big[ \log \widetilde{\mathit{PL}}_t(\beta) \big]
        -
        \nabla^2\big[ \log \mathit{PL}_t(\beta) \big]
    \Big\| \\
        \leq
            2K^2
            \exp\{4 K \| \beta \|\}
            \cdot
            \sum_{t_m \leq t}
                \frac{|J_m|^3 \, (|J_m| - 1)}{|\mathcal{J}_{t_m}(i_m)|}.
\end{multline*}


Set $\tau_n = \inf\{ t : N_t > n \}$, $L = \sup_m |J_m|$, and
\[
    \eta_n =
    \Big\|
        \Big[
            \nabla^2\big[
                \log \widetilde{\mathit{PL}}_{\tau_n}(\hat \beta_{\tau_n})
            \big]
        \Big]^{-1}
        \Big[
            \nabla\big[
                \log \widetilde{\mathit{PL}}_{\tau_n}(\hat \beta_{\tau_n})
            \big]
        \Big]
    \Big\|.
\]
If the limit of 
\(
    \Big\|
        \frac{1}{n} \nabla^2
        \big[ \log \widetilde{\mathit{PL}}_t(\hat \beta_{\tau_n}) \big]
    \Big\|
\)
is bounded away from zero, then there exists a finite $C$ such that for any
$\varepsilon > 0$, when $n$ is sufficiently large,
\[
    \eta_n
        \leq
            C K L^3 \exp\{4 K \| \beta \|\}
            \cdot
            \frac{1}{n}
            \sum_{t_m \leq \tau_n}
                \frac{1}{|\mathcal{J}_{t_m}(i_m)|}
            +
            \varepsilon.
\]

\end{proof}


\section{Bias-Corrected Estimates}

This section gives bias-corrected estimates of the parameters in
the Enron model.

\begin{table}[h]
    \tiny
    \input{tables/group-effects-bc}
    \caption{
        Bias-corrected estimated group-level effects and their
        standard errors.
    }
    \label{T:group-effects-bc}
\end{table}

\begin{figure}[h]
    \includegraphics[scale=0.6]{figures/reciprocation-bc}
    \caption{
        \textsc{Bias-Corrected Reciprocation Effect.}
    }\label{F:reciprocation-bc}
\end{figure}

\begin{table}[h]
    \input{tables/gender-effects-dynamic-bc}
    \label{T:gender-effects-dynamic-bc}    
    \caption{
        Bias-corrected estimated gender-level sending preferences.
    }
\end{table}

\bibliographystyle{imsart-number}
\bibliography{iproc-sources}

\end{document}
