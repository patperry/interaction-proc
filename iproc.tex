\documentclass[aoas,preprint]{imsart}
\pdfoutput=1    % Make arXiv happy

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath,amssymb}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{hypernat}
\RequirePackage{graphicx}
\RequirePackage{enumerate}

\setattribute{journal}{name}{}  % Suppress text "Submitted to..."

\startlocaldefs
\usepackage{iproc-macros}
\endlocaldefs


\begin{document}

\begin{frontmatter}

\title{
    A model for repeated interactions with applications to email traffic
    analysis\protect\thanksref{T1}
}

\runtitle{A point process model for graphs}
\thankstext{T1}{Supported by grant\ldots}

\begin{aug}
    \author{%
        \fnms{Patrick O.} \snm{Perry}\corref{}%
        \ead[label=e1]{patperry@seas.harvard.edu}%
    }%
    \and%
    \author{%
        \fnms{Patrick J.} \snm{Wolfe}%
        \ead[label=e2]{wolfe@stat.harvard.edu}%
    }%

    \runauthor{P.\ O.\ Perry and P.\ J.\ Wolfe}

    \affiliation{Harvard University}

    \address{
        Statistics and Information Sciences Laboratory \\
        Harvard University \\
        33 Oxford Street \\
        Cambridge, MA 02138 \\
        \printead{e1}\\
        \phantom{E-mail:\ }\printead*{e2}
    }
\end{aug}

\begin{abstract}
Abstract.
\end{abstract}

\begin{keyword}[class=AMS]
    \kwd[Primary ]{62M30}     % Statistics:Inference from stochastic processes:Spatial processes
    \kwd[; secondary ]{62N01} % Statistics:Survival analysis and censored data:Censored data models
\end{keyword}

\begin{keyword}
    \kwd{random graphs}
    \kwd{networks}
    \kwd{point processes}
    \kwd{inference}
\end{keyword}

\end{frontmatter}


\section{Introduction}

Data derived from observing repeated pairwise interactions are becoming more
and more common. One obvious source are communications networks: either all of
the phone calls made or the emails sent between inviduals in a small
community. More broadly, one can consider the notion of ``pairwise
interaction'' to include

\begin{description}

    \item[migration patterns] where each ``individual'' is a greographical
    location, and each ``interaction'' is a family relocating (rehabituating)
    from one county to another;

    \item[collaborations] where individuals are authors and interactions are
    the works they coauthor; alternatively, individuals are congressmen and
    interactions are coauthored bills; or

    \item[text analysis] where individials are works or bigrams, and
    interactions are co-occurrences in phrases or documents.

\end{description}

For the current treatment, we shall consider directed interactions, with each
interaction having an initiator (sender) and at least one recipient
(receiver). To simplify the development, initially we focus on the
single-recipient case, so that each interaction involves a single
sender-receiver pair. If we consider the first author of a work to be the
initiator, and if we respect word order in a document, then all of the above
interaction scenarios can be included. Mostly, though, we will focus on
communication networks.

Suppose we observe a system of individuals interacting over time. At the end
of the observation period, we have witnessed a stream of interaction events of
the form $(t, i, j)$. This triple denotes that at time $t$, we observed the
interaction $i \to j$, an interaction sent by individual $i$ and received by
individual $j$. We take $[0,T]$ to be the observation time period,
$\mathcal{I} = \{1, \ldots, I \}$ to be the set of senders, and $\mathcal{J} =
\{1, \ldots, J \}$ to be the set of receivers. Oftentimes $\mathcal{I} =
\mathcal{J}$, but not always.

Given covariate information about the senders and receivers, we want to
determine which characteristics and behaviors are associated with (predictive
of) interaction. We want to answer the following questions.

\begin{enumerate}

    \item If two individuals share an attribute, are they more likely to
    interact? In studying human behavior, Sociologists have repeatedly found
    evidence of homophily, the tendency of individuals to associate with
    similar others \cite{mcpherson2001birds}. We would like to know, for
    example, if pairs of people with the same gender interact more often then
    pairs of people with differing genders.

    \item If individual $i$ sends a message to individual $j$, is $j$ likely
    to respond? How much more likely does the interaction $j \to i$ become,
    and how does this depend on time? When I send you a message at 12:00~p.m.\
    on Monday, I certainly do not expect a reply before 12:01~p.m., but I
    might expect one before 12:00~p.m.\ on Tuesday. Maybe I should expect a
    reply before 1:00~p.m. Certainly, if I have not heard from you in two
    weeks, I should not expect a reply the next month. How can statements like
    this be made precise, and how can they be derived from data?

    \item Conversely, if individual $i$ interacts with $j$ at time $t$, is he
    more likely to interact with $j$ at time $t+1$? How much more likely, and
    how does this affect decay over time?

\end{enumerate}

We present a simple modeling framework to facilitate inquiry into all of the
above. The framework is flexible enough to allow various forms of
time-inhomogeneity and dependency, and includes the ability to handle
interactions with multiple recipients. However, we stress that without further
justification, none of our conclusions imply causality---they only show
association.


\section{The Enron Email Data}

Our motivating dataset is a large collection of email messages sent within the
Enron corporation during the last four years of the company's existence. The
Enron email corpus is a large subset of the email messages sent within the
corporation between November 1998 and June 2002. Enron was an energy company
based in Texas that became notorious in late 2001 for its fraudulent
accounting practices. These practices eventually lead to the resignation of
its CEO, to an external audit into its accounting procedures, to massive
readjustments to its earnings statements, and eventually to its bankruptcy.
Later, many of the top executives were prosecuted and convicted of criminal
fraud and insider trading. As part of the investigation, the Federal Energy
Regulatory Commission (FERC) subpoenaed the e-mail correspondences of the top
employees and posted those not related to the criminal trial to their website
(619,446 messages, roughly 92\% of those subpoenaed).

Leslie Kaelbling, a computer scientist at MIT, purchased the corpus from the
FERC with the intention of making it available to the research community. A
team of researchers at SRI International worked to fix a number of apparent
integrity problems with the data, and then it was released to the public by
William Cohen, at CMU. This is the ``March 2, 2004 Version'' of the dataset.
Later, a former Enron employee requested that one message be removed from the
data; this removal prompted the ``August 21, 2009 Version,'' widely regarded
to be the authoritative version \cite{cohen2009enron}.

Zhou et al. have gone through substantial work to preprocess the Enron corpus
into a useful form \cite{zhou2007strategies}. We rely on their preprocessed
version for our analysis. They reduce the data to the set of e-mails sent by
high-ranking Enron executives, eliminating messages sent by non-employees,
support staff and administrative assistants. They also filter out messages
with missing timestamps. After this culling process, there are 21,635 messages
sent by 156 employees between November 13, 1998 and June 21, 2002. Each email
message has a message body consisting of text, along with header fields
including \texttt{Date}, \texttt{From}, \texttt{To}, \texttt{CC},
\texttt{BCC}, and \texttt{Subject}. The \texttt{From} field always lists a
unique e-mail address, but the \texttt{To}, \texttt{CC}, and \texttt{BCC}
fields sometimes specify multiple recipients. We make no distinction between
\texttt{To}, \texttt{CC}, and \texttt{BCC}, combining them all to determine
the recipients of a message.

We can associate a set of static covariates for each of the 156 individuals in
the dataset. Zhou et al. have extracted the employees' names, departments, and
titles. We use the forenames of the employees to associate genders to the
employees (e.g, John is male, Susan is female). For ambiguously-gendered names
like Dana, Robin, and Sean, we use personal pronouns in the messages to help
code the genders. We code the department as one of three categories: Legal,
Trading, or Other. Finally, we code the position as Senior (CEO, CFO, COO,
Director, Managing Director, VP, President) or Junior (Administrator, Analyst,
Assistant, Attorney, Counsel, Employee, Manager, Specialist, Trader).


\section{Contingency Tables}

The contingency table is an established method for analyzing homophily and
group-level behavior.

\section{Pitfalls of Table-Based Analyses}

The table-based analysis does not account for variability in individual
behavior, ignores the times of the messages, and does not handle messages with
multiple recipients.

\subsection{Individual Inhomogeneity}
\subsection{Time Dependence}
\subsection{Multiple Recipients}


\section{A Point Process Model}

Every interaction process can be encoded by a set of counting measures.
For this section, assume that each message has a single recipient.
Let $i$ be a sender in $\mathcal{I}$, let $j$ be a receiver in $\mathcal{J}$,
and let $t$ be a positive time in $\reals_+$.  Denote the number of
messages sent from $i$ to $j$ in $(0,t]$ by $N_t(i,j)$, the
number of messages sent from $i$ to any receiver in $(0,t]$ by $N_t(i)$, and
the number of messages sent from any sender to any receiver in $(0,t]$ by
$N_t$.  Thus,
\[
    N_t(i) = \sum_{j\in \mathcal{J}} N_t(i,j),
    \qquad
    N_t = \sum_{i\in \mathcal{I}} N_t(i).
\]
Each of these, when considered as a function of $t$, is right-continuous with
left-hand limits.

% Suppose $(t_1, i_1, j_1), \ldots, (t_M, i_M, j_M)$ is the sequence of observed
% messages, where tuple $(t,i,j)$ encodes that at time $t$, sender $i$ sent a
% message to receiver $j$.  Define the following message sets:
% \begin{align*}
%     \mathcal{M}_t(i,j)
%         &= \{ m : t_m \leq t, \, i_m = i, \, j_m = j \}, \\
%     \mathcal{M}_t(i)
%         &= \cup_{j \in \mathcal{J}} \mathcal{M}_t(i,j), \\
%     \mathcal{M}_t
%         &= \cup_{i \in \mathcal{I}} \mathcal{M}_t(i).
% \end{align*}
% It follows that
% \begin{align*}
%     N_t(i,j) &= |\mathcal{M}_t(i,j)|, \\
%     N_t(i) &= |\mathcal{M}_t(i)|, \\
%     N_t &= |\mathcal{M}_t|.
% \end{align*}

% The reverse procedure works as well: given $N_\cdot(\cdot, \cdot)$, there
% is a sequence of messages and a set function $\mathcal{M}_\cdot(\cdot,\cdot)$
% such that $\mathcal{M}_s(i,j) \subseteq \mathcal{M}_t(i,j)$ for $s \leq t$,
% consistent with the equation $N_t(i,j) = |\mathcal{M}_t(i,j)|$.  If the
% times are all distinct then this construction is unique; otherwise it is
% unique up to time-fixing permutations of message indices.

To model the interaction process, we take the message events to be random.
Formally, suppose that the interaction process is defined on a
probability space $(\Omega, \mathcal{F}, \mathbb{P})$.  Let
$\big\{ \mathcal{F}_t : t \geq 0 \big\}$ be a filtration of $\mathcal{F}$
such that $N_t(i,j)$ is $\mathcal{F}_t$-measurable when
$(t,i,j) \in \reals_+ \times \mathcal{I} \times \mathcal{J}$.  Further,
assume that $N_0 = 0$, that $N_t < \infty$ a.s. for all $t$, and that 
$N_\cdot$ is piecewise constant with jumps of size $1$.  Thus, the message
arrival times are all distinct.

The process $N(i,j)$ is a local submartingale and thus has a 
nondecreasing predictable compensator, $\Lambda(i,j)$.  Predictability
means roughly that $\Lambda_t(i,j)$ is known just before time $t$.  The
compensator satisfies $\Lambda_0(i,j) = 0$ and is such that
\(
    N(i,j) - \Lambda(i,j)
\)
is a local martingale.  Assume $\Lambda(i,j)$ is absolutely continuous,
so that there exists a predictable stochastic intensity process $\lambda(i,j)$
such that
\[
    \Lambda_t = \int_0^t \lambda_s(i,j) \, ds.
\]
The intuition is
\[
    \lambda_t(i,j) \, dt
        =
        \mathbb{P}\{
            \text{$i$ sends $j$ a message in $[t,t+dt)$}
        \}.
\]
See Martinussen and Scheike for more details~\cite{martinussen2006dynamic}.

Suppose sender $i$ only sends messages to a subset of the receivers.  Denote
this subset by $\mathcal{J}(i)$.  For sender $i$, receiver $j$, and positive
time $t$, let $x_t(i,j)$ be a predictable vector of covariates in $\reals^p$.
Let $\beta$ be an unknown parameter vector in $\reals^p$ and let
$\lambda_t(i)$ be the baseline intensity for sender $i$, deterministic but
unknown.  We assume $\lambda_t(i,j)$ has the form
\begin{equation}
    \lambda_t(i,j)
        = 
        \begin{cases}
            \lambda_t(i)
            \cdot
            \exp\{ \langle x_t(i,j), \, \beta \rangle \}
                &\text{when $j \in \mathcal{J}(i)$,} \\
            0
                &\text{otherwise.}
        \end{cases}
\end{equation}
This is a version of the Cox proportional hazards
model~\cite{cox1972regression}.

The likelihood of the interaction process is a function of the unknown
parameter vector $\beta$ and the sender-specific baseline
intensities $\lambda(1), \ldots, \lambda(I)$.  After observing the process
up to time $t$, the likelihood is
\begin{multline*}
    L_t\big(\beta, \lambda(1), \ldots, \lambda(I)\big) \\
        =
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \lambda_s(i,j) \, dN_s(i,j)
            -
            \int_0^t
                \lambda_s(i,j) \, ds
        \bigg\}.
\end{multline*}
This factors into two terms, one which depends on all of the parameters,
and one which depends on $\beta$ alone.  We estimate $\beta$ by maximizing
the latter term, the partial likelihood, denoted $\mathit{PL}_t(\beta)$.
Define
\begin{gather*}
    w_{\beta,t}(i,j) = \exp\{ \langle x_t(i,j), \, \beta \rangle \},
    \qquad
    W_{\beta,t}(i) = \sum_{j \in \mathcal{J}(i)} w_{\beta,t}(i,j), \\
    \tilde \lambda_{\beta,t}(i)
        = \lambda_t(i) \cdot W_{\beta,t}(i).
\end{gather*}
The factorization is
\begin{align*}
    \begin{split}
    L_t\big(\beta, \lambda(1), \ldots&, \lambda(I)\big) \\
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \int_0^t
                \log \tilde \lambda_{\beta,s}(i) \, dN_s(i,j)
            -
            \int_0^t
                \tilde \lambda_{\beta,s}(i) \, ds
        \bigg\}
        \cdot
        \mathit{PL}_t(\beta),
    \end{split} \\
    \mathit{PL}_t(\beta)
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \frac{w_{\beta,s}(i,j)}
                          {W_{\beta,s}(i)}
                \, dN_s(i,j)
        \bigg\}.
\end{align*}

\clearpage

\section{Multiple recipients}

Let $J_n$ be an increasing sequence of positive integers, set
$\mathcal{J}_n = \{ 1, \ldots, J_n \}$, and
let $x_{n,j}$ such that $1 \leq j \leq J_n$ be a triangular array of
$p$-dimensional covariate vectors.  Let $l_m$ be a sequence of positive
integers and $\mathcal{R}_{n,m}$ such that
$1 \leq m \leq n$ be another triangular array, where
$\mathcal{R}_{n,m} \in \mathcal{J}_n^{l_m}$.  The vector
\(
    \mathcal{R}_{n,m}
    =
    \big(
        \mathcal{R}_{n,m}(1), \ldots, \mathcal{R}_{n,m}(l_m)
    \big)
\)
can be considered an ordered sample of size $l_m$ from $\mathcal{J}_n$.

Consider three probability laws for $\mathcal{R}_{n,m}$ indexed by
$\beta$.  For all three, the variables $\mathcal{R}_{n,m}$ are independent.
First, when $j \in \mathcal{J}_n$ define
\[
    \pi_{\beta, n}(j)
    =
    \frac{\exp\big\{\langle x_{n,j}, \beta \rangle\big\}}
         {\sum_{j' \in \mathcal{J}_n}
            \exp\big\{\langle x_{n,j'}, \beta \rangle\big\}}.
\]
Under $\tilde{\mathbb{P}}_\beta$, the components of $\mathcal{R}_{n,m}$ are
drawn with replacement according to $\pi_{\beta, n}(\cdot)$:
\begin{equation}
    \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
    =
    \prod_{k = 1}^{l_m}
        \pi_{\beta, n}(R(k)).
\end{equation}
Under both $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$, the components
of $\mathcal{R}_{n,m}$ are unique.
The law $\mathbb{P}_\beta$ designates that 
$\mathcal{R}_{n,m}$ is drawn from $\tilde{\mathbb{P}}_\beta$
conditionally on having unique components;
the law $\mathbb{P}^1_\beta$ designates that the components of
$\mathcal{R}_{n,m}$ are drawn one at a time without replacement according to
weights $\pi_{\beta,n}(\cdot)$.  Formally, define $|\cdot|$ to be the number
of unique components in a vector.  Then,
\begin{align}
    \mathbb{P}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
        &=
        \frac{
            \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
            I_{\{|R| = l_m\}}
        }{
            \sum_{R' \in \mathcal{J}_n^{l_m}}
                \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R' \big\}
                I_{\{|R'| = l_m\}}                
        } \\
\intertext{and}
    \mathbb{P}^1_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
        &=
        \frac{1}{l_m !}
        \sum_{\sigma}
        \prod_{k=1}^{l_m}
            \frac{
                \pi_{\beta,n}\big(R(\sigma(k))\big)
            }{
                1 - \sum_{k' = 1}^{k - 1} \pi_{\beta,n}\big(R(\sigma(k'))\big)
            },
\end{align}
where $\sigma$ ranges over all permutations of $\{ 1, ..., l_m \}$.


We wish to estimate $\beta$ after observing covariates
$\{ x_{n,j} : 1 \leq j \leq J_n \}$ and samples
$\{\mathcal{R}_{n,m} : 1 \leq m \leq n\}$.  In the setting driven by
$\tilde{\mathbb{P}}_\beta$, the maximum likelihood estimate is easy to
obtain.  In $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$, though, the
likelihood and its derivatives have combinatorially many terms
and can be prohibitively expensive to optimize.

The likelihood corresponding to $\tilde{\mathbb{P}}_\beta$ is given by
\[
    \tilde{L}_n(\beta ; R_1, \ldots, R_n)
    =
    \prod_{m=1}^{n}
        \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R_m \}.
\]
Let $\hat \beta_n$ be the value that maximizes
\(
    \tilde{L}_n( \cdot\, ; \mathcal{R}_{n,1}, \ldots, \mathcal{R}_{n,n}).
\)
As $n$ increases, under $\tilde{\mathbb{P}}_\beta$ standard likelihood
theory shows that $\hat \beta_n$ converges in probability to $\beta$ and
that $\sqrt{n}(\beta - \hat \beta_n)$ converges in distribution to a
mean-zero Gaussian random variable.  This fails to hold generally under
$\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$.  However, it turns out
that if $J_n$ diverges sufficiently fast, then the bias in $\hat \beta_n$ is
of order $\tfrac{1}{\sqrt{n}}$, and $\sqrt{n}(\beta - \hat \beta_n)$ still
has a Gaussian limit.

\begin{theorem}
    Under $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$,
    if $\max_{j \in \mathcal{J}_n} \|x_{n,j}\|_2$,
    \(
        \frac{1}{n} \sum_{m=1}^n l_m^2,
    \)
    and
    \(
        \frac{n}{J_n^2}
    \)
    are uniformly bounded in $n$, then $\hat \beta_n$ is a
    $\sqrt{n}$-consistent estimate of $\beta$.
\end{theorem}


\bibliographystyle{imsart-number}
\bibliography{iproc-sources}

\end{document}
