\documentclass[aoas,preprint]{imsart}
\pdfoutput=1    % Make arXiv happy

\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath,amssymb}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{hypernat}
\RequirePackage{graphicx}
\RequirePackage{enumerate}

\setattribute{journal}{name}{}  % Suppress text "Submitted to..."

\startlocaldefs
\usepackage{iproc-macros}
\endlocaldefs


\begin{document}

\begin{frontmatter}

\title{
    A model for repeated interactions with applications to email traffic
    analysis\protect\thanksref{T1}
}

\runtitle{A point process model for graphs}
\thankstext{T1}{Supported by grant\ldots}

\begin{aug}
    \author{%
        \fnms{Patrick O.} \snm{Perry}\corref{}%
        \ead[label=e1]{patperry@seas.harvard.edu}%
    }%
    \and%
    \author{%
        \fnms{Patrick J.} \snm{Wolfe}%
        \ead[label=e2]{wolfe@stat.harvard.edu}%
    }%

    \runauthor{P.\ O.\ Perry and P.\ J.\ Wolfe}

    \affiliation{Harvard University}

    \address{
        Statistics and Information Sciences Laboratory \\
        Harvard University \\
        33 Oxford Street \\
        Cambridge, MA 02138 \\
        \printead{e1}\\
        \phantom{E-mail:\ }\printead*{e2}
    }
\end{aug}

\begin{abstract}
Abstract.
\end{abstract}

\begin{keyword}[class=AMS]
    \kwd[Primary ]{62M30}     % Statistics:Inference from stochastic processes:Spatial processes
    \kwd[; secondary ]{62N01} % Statistics:Survival analysis and censored data:Censored data models
\end{keyword}

\begin{keyword}
    \kwd{random graphs}
    \kwd{networks}
    \kwd{point processes}
    \kwd{inference}
\end{keyword}

\end{frontmatter}


\section{Introduction}

Data derived from observing repeated pairwise interactions are becoming more
and more common. One obvious source are communications networks: either all of
the phone calls made or the emails sent between inviduals in a small
community. More broadly, one can consider the notion of ``pairwise
interaction'' to include

\begin{description}

    \item[migration patterns] where each ``individual'' is a greographical
    location, and each ``interaction'' is a family relocating (rehabituating)
    from one county to another;

    \item[collaborations] where individuals are authors and interactions are
    the works they coauthor; alternatively, individuals are congressmen and
    interactions are coauthored bills; or

    \item[text analysis] where individials are works or bigrams, and
    interactions are co-occurrences in phrases or documents.

\end{description}

For the current treatment, we shall consider directed interactions, with each
interaction having an initiator (sender) and at least one recipient
(receiver). To simplify the development, initially we focus on the
single-recipient case, so that each interaction involves a single
sender-receiver pair. If we consider the first author of a work to be the
initiator, and if we respect word order in a document, then all of the above
interaction scenarios can be included. Mostly, though, we will focus on
communication networks.

Suppose we observe a system of individuals interacting over time. At the end
of the observation period, we have witnessed a stream of interaction events of
the form $(t, i, j)$. This triple denotes that at time $t$, we observed the
interaction $i \to j$, an interaction sent by individual $i$ and received by
individual $j$. We take $[0,T]$ to be the observation time period,
$\mathcal{I} = \{1, \ldots, I \}$ to be the set of senders, and $\mathcal{J} =
\{1, \ldots, J \}$ to be the set of receivers. Oftentimes $\mathcal{I} =
\mathcal{J}$, but not always.

Given covariate information about the senders and receivers, we want to
determine which characteristics and behaviors are associated with (predictive
of) interaction. We want to answer the following questions.

\begin{enumerate}

    \item If two individuals share an attribute, are they more likely to
    interact? In studying human behavior, Sociologists have repeatedly found
    evidence of homophily, the tendency of individuals to associate with
    similar others \cite{mcpherson2001birds}. We would like to know, for
    example, if pairs of people with the same gender interact more often then
    pairs of people with differing genders.

    \item If individual $i$ sends a message to individual $j$, is $j$ likely
    to respond? How much more likely does the interaction $j \to i$ become,
    and how does this depend on time? When I send you a message at 12:00~p.m.\
    on Monday, I certainly do not expect a reply before 12:01~p.m., but I
    might expect one before 12:00~p.m.\ on Tuesday. Maybe I should expect a
    reply before 1:00~p.m. Certainly, if I have not heard from you in two
    weeks, I should not expect a reply the next month. How can statements like
    this be made precise, and how can they be derived from data?

    \item Conversely, if individual $i$ interacts with $j$ at time $t$, is he
    more likely to interact with $j$ at time $t+1$? How much more likely, and
    how does this affect decay over time?

\end{enumerate}

We present a simple modeling framework to facilitate inquiry into all of the
above. The framework is flexible enough to allow various forms of
time-inhomogeneity and dependency, and includes the ability to handle
interactions with multiple recipients. However, we stress that without further
justification, none of our conclusions imply causality---they only show
association.


\section{The Enron Email Data}

Our motivating dataset is a large collection of email messages sent within the
Enron corporation during the last four years of the company's existence. The
Enron email corpus is a large subset of the email messages sent within the
corporation between November 1998 and June 2002. Enron was an energy company
based in Texas that became notorious in late 2001 for its fraudulent
accounting practices. These practices eventually lead to the resignation of
its CEO, to an external audit into its accounting procedures, to massive
readjustments to its earnings statements, and eventually to its bankruptcy.
Later, many of the top executives were prosecuted and convicted of criminal
fraud and insider trading. As part of the investigation, the Federal Energy
Regulatory Commission (FERC) subpoenaed the e-mail correspondences of the top
employees and posted those not related to the criminal trial to their website
(619,446 messages, roughly 92\% of those subpoenaed).

Leslie Kaelbling, a computer scientist at MIT, purchased the corpus from the
FERC with the intention of making it available to the research community. A
team of researchers at SRI International worked to fix a number of apparent
integrity problems with the data, and then it was released to the public by
William Cohen, at CMU. This is the ``March 2, 2004 Version'' of the dataset.
Later, a former Enron employee requested that one message be removed from the
data; this removal prompted the ``August 21, 2009 Version,'' widely regarded
to be the authoritative version \cite{cohen2009enron}.

Zhou et al. have gone through substantial work to preprocess the Enron corpus
into a useful form \cite{zhou2007strategies}. We rely on their preprocessed
version for our analysis. They reduce the data to the set of e-mails sent by
high-ranking Enron executives, eliminating messages sent by non-employees,
support staff and administrative assistants. They also filter out messages
with missing timestamps. After this culling process, there are 21,635 messages
sent by 156 employees between November 13, 1998 and June 21, 2002. Each email
message has a message body consisting of text, along with header fields
including \texttt{Date}, \texttt{From}, \texttt{To}, \texttt{CC},
\texttt{BCC}, and \texttt{Subject}. The \texttt{From} field always lists a
unique e-mail address, but the \texttt{To}, \texttt{CC}, and \texttt{BCC}
fields sometimes specify multiple recipients. We make no distinction between
\texttt{To}, \texttt{CC}, and \texttt{BCC}, combining them all to determine
the recipients of a message.

We can associate a set of static covariates for each of the 156 individuals in
the dataset. Zhou et al. have extracted the employees' names, departments, and
titles. We use the forenames of the employees to associate genders to the
employees (e.g, John is male, Susan is female). For ambiguously-gendered names
like Dana, Robin, and Sean, we use personal pronouns in the messages to help
code the genders. We code the department as one of three categories: Legal,
Trading, or Other. Finally, we code the position as Senior (CEO, CFO, COO,
Director, Managing Director, VP, President) or Junior (Administrator, Analyst,
Assistant, Attorney, Counsel, Employee, Manager, Specialist, Trader).


\section{Contingency Tables}

The contingency table is an established method for analyzing homophily and
group-level behavior.

\section{Pitfalls of Table-Based Analyses}

The table-based analysis does not account for variability in individual
behavior, ignores the times of the messages, and does not handle messages with
multiple recipients.

\subsection{Individual Inhomogeneity}
\subsection{Time Dependence}
\subsection{Multiple Recipients}


\section{A Point Process Model}\label{S:point-process-model}

Every interaction process can be encoded by a set of counting measures.
For this section, assume that each message has a single recipient.
Let $i$ be a sender in $\mathcal{I}$, let $j$ be a receiver in $\mathcal{J}$,
and let $t$ be a positive time in $\reals_+$.  Denote the number of
messages sent from $i$ to $j$ in $(0,t]$ by $N_t(i,j)$, the
number of messages sent from $i$ to any receiver in $(0,t]$ by $N_t(i)$, and
the number of messages sent from any sender to any receiver in $(0,t]$ by
$N_t$.  Thus,
\[
    N_t(i) = \sum_{j\in \mathcal{J}} N_t(i,j),
    \qquad
    N_t = \sum_{i\in \mathcal{I}} N_t(i).
\]
Each of these, when considered as a function of $t$, is right-continuous with
left-hand limits.

% Suppose $(t_1, i_1, j_1), \ldots, (t_M, i_M, j_M)$ is the sequence of observed
% messages, where tuple $(t,i,j)$ encodes that at time $t$, sender $i$ sent a
% message to receiver $j$.  Define the following message sets:
% \begin{align*}
%     \mathcal{M}_t(i,j)
%         &= \{ m : t_m \leq t, \, i_m = i, \, j_m = j \}, \\
%     \mathcal{M}_t(i)
%         &= \cup_{j \in \mathcal{J}} \mathcal{M}_t(i,j), \\
%     \mathcal{M}_t
%         &= \cup_{i \in \mathcal{I}} \mathcal{M}_t(i).
% \end{align*}
% It follows that
% \begin{align*}
%     N_t(i,j) &= |\mathcal{M}_t(i,j)|, \\
%     N_t(i) &= |\mathcal{M}_t(i)|, \\
%     N_t &= |\mathcal{M}_t|.
% \end{align*}

% The reverse procedure works as well: given $N_\cdot(\cdot, \cdot)$, there
% is a sequence of messages and a set function $\mathcal{M}_\cdot(\cdot,\cdot)$
% such that $\mathcal{M}_s(i,j) \subseteq \mathcal{M}_t(i,j)$ for $s \leq t$,
% consistent with the equation $N_t(i,j) = |\mathcal{M}_t(i,j)|$.  If the
% times are all distinct then this construction is unique; otherwise it is
% unique up to time-fixing permutations of message indices.

To model the interaction process, we take the message events to be random.
Formally, suppose that the interaction process is defined on a
probability space $(\Omega, \mathcal{F}, \mathbb{P})$.  Let
$\big\{ \mathcal{F}_t : t \geq 0 \big\}$ be a filtration of $\mathcal{F}$
such that $N_t(i,j)$ is $\mathcal{F}_t$-measurable when
$(t,i,j) \in \reals_+ \times \mathcal{I} \times \mathcal{J}$.  Further,
assume that $N_0 = 0$, that $N_t < \infty$ a.s. for all $t$, and that 
$N_\cdot$ is piecewise constant with jumps of size $1$.  Thus, the message
arrival times are all distinct.

The process $N(i,j)$ is a local submartingale and thus has a 
nondecreasing predictable compensator, $\Lambda(i,j)$.  Predictability
means roughly that $\Lambda_t(i,j)$ is known just before time $t$.  The
compensator satisfies $\Lambda_0(i,j) = 0$ and is such that
\(
    N(i,j) - \Lambda(i,j)
\)
is a local martingale.  Assume $\Lambda(i,j)$ is absolutely continuous,
so that there exists a predictable stochastic intensity process $\lambda(i,j)$
such that
\[
    \Lambda_t = \int_0^t \lambda_s(i,j) \, ds.
\]
The intuition is
\[
    \lambda_t(i,j) \, dt
        =
        \mathbb{P}\{
            \text{$i$ sends $j$ a message in $[t,t+dt)$}
        \}.
\]
See Martinussen and Scheike for more details~\cite{martinussen2006dynamic}.

Suppose sender $i$ only sends messages to a subset of the receivers.  Denote
this subset by $\mathcal{J}(i)$.  For sender $i$, receiver $j$, and positive
time $t$, let $x_t(i,j)$ be a predictable vector of covariates in $\reals^p$.
Let $\beta$ be an unknown parameter vector in $\reals^p$ and let
$\lambda_t(i)$ be the baseline intensity for sender $i$, deterministic but
unknown.  We assume $\lambda_t(i,j)$ has the form
\begin{equation}
    \lambda_t(i,j)
        = 
        \begin{cases}
            \lambda_t(i)
            \cdot
            \exp\{ \langle x_t(i,j), \, \beta \rangle \}
                &\text{when $j \in \mathcal{J}(i)$,} \\
            0
                &\text{otherwise.}
        \end{cases}
\end{equation}
This is a version of the Cox proportional hazards
model~\cite{cox1972regression}.

The likelihood of the interaction process is a function of the unknown
parameter vector $\beta$ and the sender-specific baseline
intensities $\lambda(1), \ldots, \lambda(I)$.  After observing the process
up to time $t$, the likelihood is
\begin{multline*}
    L_t\big(\beta, \lambda(1), \ldots, \lambda(I)\big) \\
        =
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \lambda_s(i,j) \, dN_s(i,j)
            -
            \int_0^t
                \lambda_s(i,j) \, ds
        \bigg\}.
\end{multline*}
This factors into two terms, one which depends on all of the parameters,
and one which depends on $\beta$ alone.  We estimate $\beta$ by maximizing
the latter term, the partial likelihood, denoted $\mathit{PL}_t(\beta)$.
Define
\begin{gather*}
    w_{\beta,t}(i,j) = \exp\{ \langle x_t(i,j), \, \beta \rangle \},
    \qquad
    W_{\beta,t}(i) = \sum_{j \in \mathcal{J}(i)} w_{\beta,t}(i,j), \\
    \tilde \lambda_{\beta,t}(i)
        = \lambda_t(i) \cdot W_{\beta,t}(i).
\end{gather*}
The factorization is
\begin{align*}
    \begin{split}
    L_t\big(\beta, \lambda(1), \ldots&, \lambda(I)\big) \\
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \int_0^t
                \log \tilde \lambda_{\beta,s}(i) \, dN_s(i,j)
            -
            \int_0^t
                \tilde \lambda_{\beta,s}(i) \, ds
        \bigg\}
        \cdot
        \mathit{PL}_t(\beta),
    \end{split} \\
    \mathit{PL}_t(\beta)
        &=
        \exp\bigg\{
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \log \frac{w_{\beta,s}(i,j)}
                          {W_{\beta,s}(i)}
                \, dN_s(i,j)
        \bigg\}.
\end{align*}

Suppose $(t_1, i_1, j_1), \ldots, (t_M, i_M, j_M)$ is the sequence of observed
messages, where tuple $(t,i,j)$ indicates that at time $t$, sender $i$ sent a
message to receiver $j$.  Define the following message sets:
\begin{align*}
  \mathcal{M}_t(i,j)
    &= \{ m : t_m \leq t, \, i_m = i, \, j_m = j \}, \\
  \mathcal{M}_t(i)
    &= \cup_{j \in \mathcal{J}} \mathcal{M}_t(i,j), \\
  \mathcal{M}_t
    &= \cup_{i \in \mathcal{I}} \mathcal{M}_t(i).
\end{align*}
The partial likelihood factors a a product of terms, one for each sender:
\begin{align*}
    \mathit{PL}_t(\beta)
        &=
        \,\,
        \prod_{i \in \mathcal{I}}
            \,\,
            \mathit{PL}_t(\beta, i), \\
    \mathit{PL}_t(\beta, i)
        &=
        \!\!\!\!
        \prod_{m \in \mathcal{M}_t(i)}
            \!\!\!
            \frac{w_{\beta, t_m} (i, j_m)}
                 {W_{\beta, t_m}}.
\end{align*}
This factorization allows us to compute $\log \mathit{PL}_t(\beta)$ and
its derivatives by computing the sender-specific terms in parallel and
then adding them together.

The sender-specific log-partial likelihood and its derivatives are
\begin{align*}
    \log \mathit{PL}_t(\beta, i)
        &=
        \!\!\!\!
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!\!        
            \langle x_{t_m}\!(i, j_m), \, \beta \rangle
            \,
            -
            \,
            \log W_{\beta, t_m}\!(i), \\
    \nabla [ \log \mathit{PL}_t(\beta, i) ]
        &=
        \!\!\!\!        
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!\!
            x_{t_m}\!(i,j_m)
            \,
            -
            \,
            \frac{1}{W_{t_m}\!(i)}
                \sum_{j \in \mathcal{J}(i)}
                    w_{t_m}\!(i,j) \,
                    x_{t_m}\!(i,j), \\
    \begin{split}
    \nabla^2 [ \log \mathit{PL}_t(\beta, i) ]
        &=
        -
        \!\!\!\!
        \sum_{m \in \mathcal{M}_t(i)}
            \!\!
            \bigg\{
            \frac{1}{W_{t_m}\!(i)}
            \sum_{j \in \mathcal{J}(i)}
                w_{t_m}\!(i,j) \,
                \big[
                    x_{t_m}\!(i,j)
                \big]^{\otimes 2} \\
        &\qquad\qquad\qquad-
            \Big[
                \frac{1}{W_{t_m}\!(i)}
                \sum_{j \in \mathcal{J}(i)}
                    w_{t_m}\!(i,j) \,
                    x_{t_m}\!(i,j)
            \Big]^{\otimes 2}
            \bigg\},
    \end{split}
\end{align*}
where $a^{\otimes 2} = a a^\trans$.  When $x_t(i,j)$ is constant over time,
there are sufficient statistics for $\beta$ and these formulas reduce.
Otherwise, computing $\log \mathit{PL}_t(\beta,i)$ and its derivatives
requires iterating over all messages, potentially taking time
$\Oh(|\mathcal{M}_t(i)| \cdot |\mathcal{J}(i)| \cdot p^2)$.  
The serial computation time for computing the full partial likelihood
and its derivatives is $\Oh(M \cdot J \cdot p^2)$.
For the Enron data, $M \approx 20,\!000$, $J \approx 150$,
and $p \approx 200$.  This is relatively small by modern standards.  Often
$M$ on the order of $10^9$ and $J$ on the order of $10^6$.  Computations
taking time $\Oh(M \cdot J)$ are prohibitive for large datasets.

Taking advantage of sparsity, the serial computation time reduces to
$\Oh(M \cdot p^2 + I \cdot J)$.  Decompose $x$ into its static
(non time-varying) and dynamic parts:
\[
    x_t(i,j)
        = x_0(i,j) + d_t(i,j).
\]
Typically, the dynamic part, $d_t(i,j)$, is sparse, with at most $\bar p$
nonzero components.  Moreover $d_t(i,j)$ is zero for most $(i,j)$ pairs
--- often $d_t(i,j)$ is zero unless $i$ and $j$ have exchanged
messages in the past.  Let
\[
    \mathcal{\bar J}(i)
        =
        \{
            j \in \mathcal{J}(i) : d_t(i,j) \neq 0 
            \,\,
            \text{for some $t$}
        \}.
\]
Assume $|\mathcal{\bar J}(i)| \ll |\mathcal{J}(i)|$ and that 
computing $d_t(i,j)$ takes amortized time $\Oh(\bar p)$ for each
$(t,i,j)$ triple.

Notice
\begin{align*}
    w_{\beta,t}(i,j)
        &=
            w_{\beta,0}(i,j)
            \cdot
            \exp\{ \langle d_t(i,j), \beta \rangle \}, \\
    W_{\beta,t}(i)
        &=
            W_{\beta,0}(i)
            +
            \sum_{j \in \mathcal{\bar J}(i)}
                w_{\beta,t}(i,j) - w_{\beta,0}(i,j)
\end{align*}
It takes time $\Oh(|\mathcal{J}(i)| \cdot p)$ to compute the values of
$W_{\beta,0}(i)$ and $w_{\beta,0}(i,j)$ for any $i$ and all
$j$ in $\mathcal{J}(i)$; once these values are known, computing the values
of $W_{\beta,t}(i)$ and $w_{\beta,t}(i,j)$ takes time
\(
    \Oh(|\mathcal{\bar J}(i)| \cdot \bar p).
\)
Since
\[
    \sum_{m\in \mathcal{M}_t(i)}
        x_{t_m}\!(i,j_m)
        =
            \sum_{j \in \mathcal{J}(i)}
                \mathcal{M}_t(i,j) \, x_0(i,j)
            \,\,\,
            +
            \sum_{m \in \mathcal{M}_t(i)}
                d_{t_m}\!(i,j),
\]
%computing sum on the left hand side takes time
%\(
%    \Oh(|\mathcal{M}_t(i)| \cdot \bar p + |\mathcal{J}(i)| \cdot p);
%\)
computing $\log \mathit{PL}_t(\beta, i)$ takes time
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p
        +
        |\mathcal{J}(i)| \cdot p
    ).
\)

The second term in the gradient is
\begin{multline*}
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta,t_m}\!(i)}
        \sum_{j \in \mathcal{J}(i)}
            w_{\beta,t_m}\!(i,j)
            \,
            x_{t_m}\!(i,j) \\
    =
    \sum_{j \in \mathcal{J}(i)}
    \bigg[
        \sum_{m \in \mathcal{M}_t(i)}
            \frac{w_{\beta,t_m}\!(i,j)}{W_{\beta,t_m}\!(i)}
    \bigg]
    \cdot
    x_0(i,j) \\
    +
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta, t_m}\!(i)}
        \sum_{j \in \mathcal{\bar J}(i)}
            w_{\beta,t_m}\!(i,j)
            \,
            d_{t_m}\!(i,j).
\end{multline*}
Also,
\[
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{w_{\beta,t_m}\!(i,j)}{W_{\beta,t_m}\!(i)}
    =
        w_{\beta,0}(i,j)
        \cdot
        \sum_{m \in \mathcal{M}_t(i)}
            \frac{
                \exp\{\langle d_{t_m}\!(i,j), \, \beta \rangle \}
            }{
                W_{\beta,t_m}\!(i)
            };
\]
the latter term is constant when $j \notin \mathcal{\bar J}(i)$.
Organizing the computations as suggested by these formula,
the additional overhead for computing the gradient of
$\log \mathit{PL}_t(\beta,i)$ is
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p
        +
        |\mathcal{J}(i)| \cdot p
    )
\).
This is the same computational complexity as for computing
$\log \mathit{PL}_t(\beta,i)$.

To compute the Hessian, write
\begin{multline*}
    \big[ x_t(i,j) \big]^{\otimes 2}
    =
    \big[ x_0(i,j) \big]^{\otimes 2}
    +
    \big[ d_t(i,j) \big]^{\otimes 2} \\
    + 
    \big[
        x_0(i,j) \otimes d_t(i,j)
    \big]
    +
    \big[
        x_0(i,j) \otimes d_t(i,j)
    \big]^\trans
\end{multline*}
Once we have the gradient of $\log \mathit{PL}_t(\beta,i)$,
the additional term needed to compute the Hessian is
\[
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta,t_m}\!(i)}
        \sum_{j \in \mathcal{J}(i)}
            w_{\beta,t_m}\!(i,j) \,
            \big[
                x_{t_m}\!(i,j)
            \big]^{\otimes 2}.
\]
The part involving
$\big[ x_0(i,j) \big]^{\otimes 2} + \big[ d_t(i,j) \big]^{\otimes 2}$
can be gotten similarly to the term from the gradient in time
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p^2
        +
        |\mathcal{J}(i)| \cdot p^2
    ).
\)
The cross term factors as
\begin{multline*}
    \sum_{m \in \mathcal{M}_t(i)}
        \frac{1}{W_{\beta,t_m}\!(i)}
        \sum_{j \in \mathcal{J}(i)}
            w_{\beta,t_m}\!(i,j) \,
            \big[
                x_{0}(i,j)
                \otimes
                d_{t_m}\!(i,j)
            \big] \\
        =
        \sum_{j \in \mathcal{J}(i)}
            x_{0}(i,j)
            \otimes
            \bigg[
                \sum_{m \in \mathcal{M}_t(i)}
                    \frac{w_{\beta,t_m}\!(i,j)}{W_{\beta,t_m}\!(i)}
                    \,
                    d_{t_m}\!(i,j)
            \bigg].
\end{multline*}
Thus, the total computation time for computing $\log \mathit{PL}_t(\beta,i)$,
its gradient, and its hessian, is
\(
    \Oh(
        |\mathcal{M}_t(i)| \cdot |\mathcal{\bar J}(i)| \cdot \bar p^2
        +
        |\mathcal{J}(i)| \cdot p^2
    ).
\)

Suppose $|\mathcal{J}(i)| \leq J$, $|\mathcal{\bar J}(i)| \leq \bar J$,
and $|\mathcal{M}_t(i)| \leq M$.  By organizing the computations as described
above, we can compute $\log \mathit{PL}_t(\beta)$ and its first
two derivatives in serial time
\(
    \Oh(
        | \mathcal{I} |
        \cdot
        M
        \cdot
        \bar J
        \cdot
        \bar p^2
        +
        |\mathcal{I}| \cdot J \cdot p^2
    ).
\)
With $\Oh(|\mathcal{I}|)$ processors, we can compute these quantities in time
\(
    \Oh(
        M \cdot \bar J \cdot \bar p^2
        +
        J \cdot p^2
        +
        |\mathcal{I}| \cdot p^2
    ).
\)

\clearpage

\section{Multiple recipients}

To this point, we have assumed that each message has a single recipient.
Often, messages have multiple recipients.  In the Enron dataset,
approximately 30\% of messages have more than one recipient, with
a few messages having more than fifty recipients.  One way of dealing
with multiple recipients is, for example, treating a message with three
recipients as three separate single-recipient messages.  A less ad-hoc
approach is to explicitly take multiple recipients into account at the
modeling stage.

Introduce $\lambda_t(i ; R)$ as the baseline intensity at time $t$ for
messages sent by sender $i$ with $R$ recipients.  Let $J$ be any
subset of $\mathcal{J}(i)$ and model the rate at which $i$ sends messages to
recipient set $J$ as
\begin{equation}\label{E:intensity-multiple}
    \lambda_t(i,J)
        =
        \lambda_t(i ; |J|)
        \cdot
        \exp\Big\{
            \sum_{j \in J}
                \langle x_t(i,j), \, \beta \rangle
        \Big\}.
\end{equation}
We have chosen this form for ease of interpretability.  At time
$t$, changing receiver $j$'s covariates from $x_t(i,j)$ to
$x'_t(i,j)$ results in scaling the the rate at
which $i$ sends him messages by
\(
    \exp\{ \langle x'_t(i,j) - x_t(i,j), \beta \rangle \};
\)
importantly, this does not depend on $\lambda_t(i; R)$.

The partial likelihood is nearly the same as in the single-recipient
setting from Section~\ref{S:point-process-model}.
Let $(t_1, i_1, J_1), \ldots, (t_M, i_M, J_M)$ be the sequence of
observed messages, with tuple $(t, i, J)$ indicating that at time $t$
sender $i$ sent a message with recipient set
$J$, a subset of $\mathcal{J}$.  Define sets
\begin{align*}
    \mathcal{M}_t(i, J)
        &= \{ m : t_m \leq t, i_m = i, J_m = J \}, \\
    \mathcal{M}_t(i)
        &= \cup_{J \subseteq \mathcal{J}} \mathcal{M}_t(i, J), \\
    \mathcal{M}_t
        &= \cup_{i \in \mathcal{I}} \mathcal{M}_t(i).
\end{align*}
When $i \in \mathcal{I}$, $j \in \mathcal{J}(i)$, and $R \geq 1$, define
\[
    w_{\beta,t}(i,j) = \exp\{ \langle x_t(i,j), \, \beta \rangle \},
    \quad
    W_{\beta,t}(i; R)
    =
    \!\!
    \sum_{\substack{J \subseteq \mathcal{J}(i)\\ |J| = R}}
        \prod_{j \in J}
            w_{\beta,t}(i,j).
\]
The log partial likelihood at time $t$ is
\begin{equation}\label{E:log-pl-multiple}
    \log \mathit{PL}_t(\beta)
        =
        \sum_{i \in \mathcal{I}}
        \log \mathit{PL}_t(\beta, i),
\end{equation}
where the sender-specific part is
\begin{equation}\label{E:sender-log-pl-multiple}
    \log \mathit{PL}_t(\beta, i)
        =
        \sum_{m \in \mathcal{M}_t(i)}
        \Big[
            \sum_{j \in J_m}
                \langle x_{t_m}\!(i, j), \, \beta \rangle
        \Big]
            -
            \log W_{\beta,t_m} (i ; |J_m|).
\end{equation}
Consistency and asymptotic normality of the maximum partial likelihood
estimator follow from standard arguments.  We state this result and
sketch its proof below.

\subsection{The general case}
Let $\mathcal{I}$ be a set of senders and $\mathcal{J}$ be a set of
receivers.  
For each sender $i$, let $\bar \lambda_t(i)$ be a non-negative predictable
process called the baseline intensity of sender $i$; let
$\mathcal{J}_t(i)$ be a predictable subset of $\mathcal{J}$ called the 
recipient set of sender $i$.
For each sender-receiver pair $(i,j)$, let $x_t(i,j)$ in $\reals^p$ be a
predictable vector of covariates.  Let $\beta_0$ in $\reals^p$
be an unknown vector of coefficients.

Suppose $N$ is a multivariate counting process on 
$\reals_+ \times \mathcal{I} \times \mathcal{J}$,
with stochastic intensity
\begin{equation}
    \lambda_t(i,j)
        =
        \bar \lambda_t(i)
        \exp\{ \beta_0^\trans x_t(i, j) \}
        \cdot
        1{\{j \in \mathcal{J}_t(i)\}}.
\end{equation}
The integer $N_t(i,j)$ is the number of events marked $(i,j)$ having
times in  $[0,t]$, with $N_0(i,j)$ assumed to be zero.
Interpret event $(t, i, j)$ as a message sent from $i$ to $j$ at
time $t$.  Define
\(
    N_t(i) = \sum_{j \in \mathcal{J}} N_t(i,j)
\)
and
\(
    N_t = \sum_{i \in \mathcal{I}} N_t(i).
\)

  
If $\{ (t_m, i_m, j_m) \}$ is the set of observed messages, then
the log partial likelihood at time $t$, evaluated at $\beta$, is
\begin{equation}
    \log
    \mathit{PL}_t(\beta)
        =
        \sum_{t_m \leq t}
        \bigg\{
            \beta^\trans x_{t_m}\!(i_m, j_m)
            -
            \log\big[
                \!\!\!\!
                \sum_{j \in \mathcal{J}_{t_m}\!(i_m)}
                    \exp\{ \beta^\trans x_{t_m}\!(i_m, j)
            \big]
        \bigg\}.
\end{equation}
Define weights
\begin{gather}
    w_{t}(\beta, i,j)
        =
        \exp\{ \beta^\trans x_t(i,j) \}
        \cdot
        1\{ j \in \mathcal{J}_t(i)\}, \\
    W_{t}(\beta, i)
        =
        \sum_{j \in \mathcal{J}} w_{t}(\beta, i,j).
\end{gather}
Note the inner sum in $\log \mathit{PL}_t(\beta)$ is
$W_{t_m}\!(\beta, i_m)$.  The function 
$\log W_{t}(\cdot, i)$ has gradient $E_{t}(\cdot, i)$ and Hessian 
$V_{t}(\cdot, i)$, given by
\begin{gather}
    E_{t}(\beta, i)
        =
        \frac{1}{W_{t}(\beta, i)}
        \sum_{j \in \mathcal{J}_t(i)}
            w_{t}(\beta, i,j) \, x_{t}(i,j), \\
    V_{t}(\beta, i)
        =
        \frac{1}{W_{t}(\beta, i)}
        \sum_{j \in \mathcal{J}_t(i)}
            w_{t}(\beta, i,j) 
            \Big[ x_{t}(i,j) - E_{t}(\beta, i)\Big]^{\otimes 2},
\end{gather}
where $a^{\otimes 2} = a \otimes a = a a^\trans$.
The gradient and negative Hessian of $\log \mathit{PL}_t(\cdot)$ are
\begin{gather}
    U_t(\beta)
        =
        \nabla \big[ \log \mathit{PL}_t(\beta) \big]
        =
        \sum_{t_m \leq t}
            x_{t_m}(i_m, j_m) - E_{t_m}(\beta, i_m), \\
    I_t(\beta)
        =
        -\nabla^2 \big[ \log \mathit{PL}_t(\beta) \big]
        =
        \sum_{t_m \leq t}
            V_{t_m}(\beta, i_m).
\end{gather}
We call $U_t(\beta)$ the unnormalized score function and $I_t(\beta)$
the Fisher information matrix.  Let $\hat \beta_t$ be the value that
maximizes $\log \mathit{PL}_t(\cdot)$, called the maximum partial
likelihood estimator (MPLE).  Set $\hat U_t = U_t(\hat \beta_t)$ and
$\hat I_t = I_t(\hat \beta_t)$ to be the estimated score and Fisher
information.

Typically, one proves consistency of $\hat \beta_t$ by keeping $t$ fixed
and letting $|\mathcal{I}|$ go to infinity
\cite{andersen1993statistical,cook2007statistical,fleming1991counting,
martinussen2006dynamic}.  This makes sense in the context of clinical trial
data, where $\mathcal{I}$ corresponds to the set of patients.  In interaction
data, $|\mathcal{I}|$ is either fixed, or $t$ and $|\mathcal{I}|$ increase
simultaneously.  It is more natural to look at what happens as time increases.
Increasing sender sets can be handled by assuming $|\mathcal{I}|$ is infinite
and for most senders $\bar \lambda_t(i)$ is zero during the initial
part of the observation period; increasing $|\mathcal{J}|$ can be handled
similarly through $\mathcal{J}_t(i)$.

Let $\tau_n$ be an increasing sequence of stopping times and $\mathcal{B}$ be
a neighborhood of $\beta_0$.  Assume the following conditions hold:
\begin{enumerate}[(1)]
    \item \textbf{The covariates are locally uniformly bounded.}  That is,
    \[
        \sup_{\substack{i \in \mathcal{I} \\ j \in \mathcal{J}}} x_t(i,j)
        \,\,\text{is locally bounded.}
    \]
    \item \textbf{The stopping times are finite.}  For each $n$,
    \[
        \mathbb{P}\{\tau_n < \infty\} = 1.
    \]
    \item \textbf{The cumulative baseline intensity has at most linear growth.}
    \[
        \limsup_{n\to\infty}
            \frac{1}{n} \sum_{i \in \mathcal{I}}
            \int_0^{\tau(n)} \bar \lambda_s(i) \, ds < \infty.
    \]
    \item \textbf{The integrated covariance function is well-behaved.}
    For $\beta \in \mathcal{B}$ and $\alpha \in [0,1]$, as $ n \to \infty$,
    \[
        \frac{1}{n}
        \sum_{i \in \mathcal{I}}
        \int_0^{\tau_{\lfloor \alpha n \rfloor}}
            V_s(\beta,i)
            \, W_s(\beta,i)
            \, \bar \lambda(i)
            \, ds
        \toP
        \Sigma_\alpha(\beta),
    \]
    where $\Sigma_\alpha(\cdot)$ is locally bounded and Lipschitz.
    
    \item \textbf{The covariates satisfy a Lindeberg condition.}
    For any multivariate predictable process $\pi$ satisfying
    $\pi_t(i, j) \geq 0$ and $\sum_{j \in \mathcal{J}} \pi_t(i, j) = 1$,
    there exists a $\delta > 0$ such that 
    \[
        \frac{1}{\sqrt{n}}
            \sup_{\substack{i \in \mathcal{I} \\ 0 \leq t \leq \tau_n}}
            \| y_{t}(i) \|
            \cdot
            I_{\{
                \sum_{j \in \mathcal{J}_s(i)} \beta_0^\trans  x_t(i,j)
                \,>\, -\delta \, \| y_t(i) \|
            \}}
        \toP
        0,
    \]
    where $y_t(i) = \sum_{j \in \mathcal{J}} \pi_t(i, j) \, x_t(i,j)$.
\end{enumerate}

The process $N_t(i,j)$ has compensator
\begin{equation}
    \Lambda_t(i,j)
        =
            \int_0^t \lambda_s(i,j) \, ds;
\end{equation}
similarly, processes $N_t(i)$ and $N_t$ have compensators
$\Lambda_t(i) = \sum_{j \in \mathcal{J}} \Lambda_t(i,j)$
and $\Lambda_t = \sum_{i \in \mathcal{I}} \Lambda_t(i)$.  Define local
martintagles $M_t(i,j) = N_t(i,j) - \Lambda_t(i,j)$,
$M_t(i) = N_t(i) - \Lambda_t(i)$, and 
$M_t = N_t - \Lambda_t$.  Since $N_t(i,j)$ is a counting process,
$\Lambda_t(i,j)$ is locally bounded 
\cite[Thm.~IV.12]{meyer1976cours}
and $M_t(i,j)$ is locally square-integrable
\cite[Thm.~2.3.1]{fleming1991counting}.

The score function evaluated at the true parameter vector has a simple
representation in terms of these martingales.  Set
$H_t(i,j) = x_t(i,j) - E_t(\beta_0,i)$.  Since $x$ is locally uniformly
bounded, $H$ is as well.  Using the identity
\(
    \sum_{j \in \mathcal{J}}
    \int_0^t
        H_s(i,j) \,
        d\Lambda_s(i,j)
    =
    0,
\)
the score can be written as
\begin{align*}
    U_t(\beta_0)
        &=
        \sum_{i \in \mathcal{I}}
        \sum_{j \in \mathcal{J}}
        \int_0^t
            H_s(i,j) \, dN_s(i,j) \\
        &=
        \sum_{i \in \mathcal{I}}
        \sum_{j \in \mathcal{J}}
        \int_0^t
            H_s(i,j) \, dM_s(i,j).
\end{align*}
Each term in the sum is locally square integrable, with predictable
covariations
\begin{align*}
    \begin{split}
        \bigg\langle
            \int
                H_s(i,j) \, dM_s(i,j)
        &, \, \,
            \int
                H_s(i',j') \, dM_s(i',j')
        \bigg\rangle_t \\
        &=
            \int_0^t
                H_s(i,j) \otimes H_s(i',j') \,
                d\big\langle M(i,j), M(i',j')\big\rangle_s
    \end{split} \\
        &=
            \int_0^t
                \big[ H_s(i,j) \big]^{\otimes 2} \,
                d\Lambda_s(i,j)
            \cdot
            I_{\{ i = i', j = j' \}}
\end{align*}
\cite[Thm.~2.4.3]{fleming1991counting}.  There exists a sequence
of stopping times localizing all $H(i,j)$ and $M(i,j)$
simultaneously, so $U(\beta_0)$ is locally square integrable
with predictable variation
\begin{align*}
    \big\langle U(\beta_0) \big\rangle_t
        &=
            \sum_{i \in \mathcal{I}}
            \sum_{j \in \mathcal{J}}
            \int_0^t
                \big[ H_s(i,j) \big]^{\otimes 2} \,
                d\Lambda_s(i,j) \\
        &=
            \sum_{i \in \mathcal{I}}
            \int_0^t
                V_s(\beta_0, i) \,
                d\Lambda_s(i). \\
\intertext{Note}
    I_t(\beta)
        &=
            \sum_{i \in \mathcal{I}}
            \int_0^t
                V_s(\beta, i) \,
                dN_s(i).
\end{align*}

At this point, we rescale time to make the message arrivals uniform.  Define
the sequence of stopping times $\tau_n = \sup\{ t : N_t < n \}$, and let
$\mathcal{F}_{\tau_n}$ be the $\sigma$-algebra of events prior to $\tau_n$.
The idea is to change time from the original scale to a scale on which
$\tau_{n'} - \tau_n$ is proportional to $n' - n$.

Since $N_{t \wedge \tau_n}$ is a bounded right-continuous nonnegative
submartingale, the Doob-Meyer decomposition ensures
$M_{t \wedge \tau_n}$ is a uniformly integrable martingale.  Thus, the
Optional Sampling Theorem gives
$\E[M_{\tau_{n}} | \mathcal{F}_{\tau_{n-1}} ] = M_{\tau_{n-1}}$.
Similar properties hold for $M(i)$, $M(i,j)$, $\Lambda(i)$, and
$\Lambda(i,j)$.

Define the sequence of random time changes $t_n : [0,1] \to [0, \tau_n]$ by
sending $\tfrac{k}{n}$ to $\tau_k$ and interpolating linearly otherwise:
\[
    t_n(\alpha)
        =
        \tau_{\lfloor \alpha n \rfloor}
        +
        (\alpha n - \lfloor \alpha n \rfloor)
        (\tau_{\lfloor \alpha n \rfloor + 1}
         -
         \tau_{\lfloor \alpha n \rfloor}).
\]
We claim $M_{t_n(\alpha)}$ is a local square-integrable martingale.
It inherits local square-integrability from $M_{t \wedge \tau_n}$.  For
the martingale property, say $\alpha \leq \alpha'$.

\clearpage

Define the sequence of stopping times $\tau_n = \sup\{ t : N_t < n \}$.
Rescale time with the message arrivals and normalize the score. 
Specifically, when $n > 0$ and $\alpha \in [0,1]$, define
\begin{equation}
    U^{(n)}_{\alpha}(\beta_0)
    = 
    n^{-1/2} U_{\lfloor \alpha \tau_n \rfloor}(\beta_0)
\end{equation}
By the Optional Stopping Theorem, $U_{t \wedge \tau_n}(\beta_0)$ is a local
square-integrable martingale, so $U^{(n)}_{\alpha}(\beta_0)$ is, as well.
Its predictable variation is
\begin{equation}
     \langle U^{(n)}(\beta_0) \rangle_\alpha
     =
     \frac{1}{n}
     \sum_{i \in \mathcal{I}}
     \int_0^{\alpha \tau_n}
        V_s(\beta_0, i)
        \, d\Lambda_s(i).
\end{equation}




 Actually, the following Lemma shows
that these quantities are all square-integrable.

\begin{lemma}
    If $N$ is an arbitrary bounded counting process with compensator
    $\Lambda$, then $M = N - \Lambda$ is a square-integrable
    martingale and $\Lambda$ is square-integrable.
\end{lemma}
\begin{proof}
    The proof involves several results from Fleming and Harrington
    \cite{fleming1991counting}. Since $N$ is a bounded counting process,
    $\Lambda$ is locally bounded (Thm 2.3.2, p. 61) and
    integrable (Thm 1.4.1, p. 37).  Thus, $M$ is locally square integrable
    (Thm 2.3.1, p. 61).  Since $M$ is also right-continuous,
    $\E M^2_t \leq \E \Lambda_t$, and hence $M$ is square-integrable.
    Since $N$ is bouned and $\Lambda$ is integrable,
    this inequality also gives
    \(
        \E \Lambda_t^2
            \leq \E \Lambda_t + 2 \E N_t \Lambda_t - \E N_t^2
            < \infty,
    \)
    so $\Lambda$ is square-integrable.
\end{proof}

\begin{lemma}
    If
    \(
        \E\left[ \sup_{t,i,j} \| x_{t \wedge \tau_n} (i,j) \|_2^4 \right]
        <
        \infty,
    \)
    then $U_{t \wedge \tau_n}(\beta_0)$ and
    \(
        U_{t \wedge \tau_n}^2(\beta_0)
        -
        \langle U(\beta_0) \rangle_{t \wedge \tau_n}
    \)
    are uniformly integrable martingales.
\end{lemma}


The normalized stopped score
\(
    U^{(n)}_{t}(\beta_0)
    = 
    n^{-1/2} U_{t \wedge \tau_n}(\beta_0)
\) has
predictable variation
\[
     \langle U^{(n)}(\beta_0) \rangle_t
     =
     \frac{1}{n}
     \sum_{i \in \mathcal{I}}
     \int_0^{t \wedge \tau_n}
        V_s(\beta_0, i)
        \, d\Lambda_s(i).
\]
We have already shown that $U_t(\beta_0)$ is a local square-integrable
martingale.  Under the fourth moment condition on $x$, the stopped score,
$U_{t \wedge \tau_n}(\beta_0)$ is a square-integrable martingale.


\clearpage

The discrete-time process
$\tilde M_n = M_{\tau_n}$ is a local martingale with respect to 
filtration $\mathcal{\tilde F}_n = \mathcal{F}_{\tau_n}$ since
the Optional Sampling Theorem gives
$\mathbb{E}[M_{\tau_{n+1}} \mid \mathcal{F}_{\tau_n}] = M_{\tau_n}$.
In particular, this gives that $\tilde \Lambda_n = \Lambda_{\tau_n}$ is
a local $\mathcal{\tilde F}_n$-submartingale with compensator $n$.
Define $\tilde M_n(i)$ and $\tilde M_n(i,j)$ similarly to $\tilde M_n$;
these are also local $\mathcal{\tilde F}_n$-martingales.

The score has an alternative representation in terms of the time-scaled
processes:
\[
    \tilde U_n(\beta_0)
        = U_{\tau_n}(\beta_0)
        =
        \sum_{i \in \mathcal{I}}
        \sum_{j \in \mathcal{J}}
        \sum_{m = 1}^{n}
            \cdot
            \int_{\tau_{m-1}}^{\tau_m}
                H_{s}(i,j)
                \, dM_{s}(i,j).
\]
Its predictable variation is
\[
    \langle \tilde U(\beta_0) \rangle_n
        =
        \sum_{i \in \mathcal{I}}
        \sum_{m = 1}^{n}
            \int_{\tau_{m-1}}^{\tau_m}
                V_{s}(\beta_0, i)
                \, d\Lambda_s(i,j)
\]
for some non-negative increasing $\mathcal{\tilde F}_m$-predictable
process $\tilde \Lambda_{m}$.

At this point, we rescale time to make the message arrivals uniform.  Define
the sequence of stopping times $\tau_n = \sup\{ t : N_t < n \}$, and let
$\mathcal{F}_{\tau_n}$ be the $\sigma$-algebra of events prior to $\tau_n$.
The idea is to change time from the original scale to a scale on which
$\tau_n = n$.
We will need the following lemmas.  

\begin{lemma}
    Let $N$ be a any local submartingale with localizing sequence $\tau_n$
    and compensator $A$.  If $T$ is a stopping time, then, $N_{t\wedge T}$ is
    a local submartingale with localizing sequence $\tau_n$
    and compensator $A_{t \wedge T}$.
\end{lemma}
\begin{proof}
    The Doob-Meyer decomposition ensures
    $N_{t \wedge \tau_n} - A_{t \wedge \tau_n}$ is a martingale.  Thus,
    the Optional Stopping Theorem guarantees that the stopped processes
    $N_{t \wedge T \wedge \tau_n}$ and
    $N_{t \wedge T \wedge \tau_n} - A_{t \wedge T \wedge \tau_n}$ are
    a submartingale and a martingale, respectively.  
    A stopped predictable process is predictable
    \cite[Prop. 2.4]{jacod1987limit}, so that $A_{t \wedge T \wedge \tau_n}$
    must be the compensator of $N_{t \wedge T \wedge \tau_n}$.
    Hence,
    $N_{t \wedge T}$ is a local submartingale with
    localizing sequence $\tau_n$ and compensator $A_{t \wedge T}$.
\end{proof}

\begin{lemma}\label{L:expect-stopped}
    Let $N$ be a counting process with locally bounded compensator $A$, and
    let $T$ be a stopping time.  If $N_{t \wedge T}$ is uniformly integrable,
    then $\mathbb{E} N_T = \mathbb{E} A_T$.  Moreover,
    $\mathbb{E} A_T < \infty$.
\end{lemma}
\begin{proof}
    Set $M = N - A$.  Let $\tau_n$ be a sequence of stopping times
    localizing $M$, with $A_{t \wedge \tau_n}$ bounded.    We claim
    $M_{t \wedge T \wedge \tau_n}$ is uniformly integrable:
    \begin{align*}
        \lim_{K \to \infty} \sup_t \,
            \mathbb{E}\Big[
                &|M_{t \wedge T \wedge \tau_n}|
                \cdot
                I_{\{|M_{t \wedge T \wedge \tau_n}| > K\}}
            \Big] \\
        \begin{split}
        &\leq
            \lim_{K \to \infty} \sup_t
            \mathbb{E}\Big[
                2 \cdot |N_{t \wedge T \wedge \tau_n}|
                \cdot
                I_{\{|N_{t \wedge T \wedge \tau_n}| > K/2 \}} \\
                &\qquad\qquad\qquad\quad+
                2 \cdot |A_{t \wedge T \wedge \tau_n}|
                \cdot
                I_{\{|A_{t \wedge T \wedge \tau_n}| > K/2 \}}
            \Big]
        \end{split} \\
        &= 0.
    \end{align*}
    The Optional Sampling Theorem gives that
    \(
        \mathbb{E}\big[ M_{T \wedge \tau_n} \big] = 0,
    \)
    equivalently
    \(
        \mathbb{E}\big[ N_{T \wedge \tau_n} \big]
        =
        \mathbb{E}\big[ A_{T \wedge \tau_n} \big].
    \)
    Two applications of the Monotone Convergence Theorem give
    \(
        \mathbb{E}\big[ N_{T} \big]
        =
        \lim_{n \to \infty}
        \mathbb{E}\big[ N_{T \wedge \tau_n} \big]
        =
        \lim_{n \to \infty}        
        \mathbb{E}\big[ A_{T \wedge \tau_n} \big]
        =
        \mathbb{E}\big[ A_{T} \big].
    \)
    Finiteness of $\mathbb{E} A_{T}$ follows from the
    uniform integrability assumption on $N_{t \wedge T}$ since
    \(
        \mathbb{E} N_T
            = \lim_{t \to \infty} \mathbb{E} N_{t \wedge T}
            \leq \sup_t \mathbb{E} N_{t \wedge T}.
    \)
\end{proof}

\begin{lemma}
    Let $N$ be a counting process with locally bounded compensator $A$, and
    let $T$ and $T'$ be stopping times with $T \leq T'$.  Let $M = N - A$
    and let $\mathcal{F}_T$ be the $\sigma$-algebra of events prior to $T$.
    If $N_{t \wedge T'}$ is uniformly integrable,
    then $\mathbb{E} [ M_{T'} \mid \mathcal{F}_T ] = M_T$.
\end{lemma}

\begin{proof}
    Let $\tau_n$ localize $M$ and be such that $A$ is bounded.
    As in the proof of Lemma~\ref{L:expect-stopped}, the stopped process
    $M_{t \wedge T' \wedge \tau_n}$ is uniformly integrable.
    By the Optional Sampling Theorem,
    \(
        \mathbb{E} [ M_{T'^\wedge \tau_n} \mid \mathcal{F}_T ]
            = M_{T \wedge \tau_n}.
    \)
    Since
    \(
        |M_{t \wedge T' \wedge \tau_n}|
        \leq
        |N_{T'}|
        +
        |A_{T'}|
    \)
    and $| M_{T \wedge \tau_n}| \leq |N_{T'}| + |A_{T'}|$,
    two applications of the Dominated Convergence Theorem give
    \(
        \mathbb{E} [ M_{T'} \mid \mathcal{F}_T ]
        =
        \lim_{n \to \infty}
        \mathbb{E} [ M_{T'^\wedge \tau_n} \mid \mathcal{F}_T ]
        =
        \lim_{n \to \infty}
        M_{T \wedge \tau_n}
        =
        M_T.
    \)
\end{proof}

\clearpage



Let $J_n$ be an increasing sequence of positive integers, set
$\mathcal{J}_n = \{ 1, \ldots, J_n \}$, and
let $x_{n,j}$ such that $1 \leq j \leq J_n$ be a triangular array of
$p$-dimensional covariate vectors.  Let $l_m$ be a sequence of positive
integers and $\mathcal{R}_{n,m}$ such that
$1 \leq m \leq n$ be another triangular array, where
$\mathcal{R}_{n,m} \in \mathcal{J}_n^{l_m}$.  The vector
\(
    \mathcal{R}_{n,m}
    =
    \big(
        \mathcal{R}_{n,m}(1), \ldots, \mathcal{R}_{n,m}(l_m)
    \big)
\)
can be considered an ordered sample of size $l_m$ from $\mathcal{J}_n$.

Consider three probability laws for $\mathcal{R}_{n,m}$ indexed by
$\beta$.  For all three, the variables $\mathcal{R}_{n,m}$ are independent.
First, when $j \in \mathcal{J}_n$ define
\[
    \pi_{\beta, n}(j)
    =
    \frac{\exp\big\{\langle x_{n,j}, \beta \rangle\big\}}
         {\sum_{j' \in \mathcal{J}_n}
            \exp\big\{\langle x_{n,j'}, \beta \rangle\big\}}.
\]
Under $\tilde{\mathbb{P}}_\beta$, the components of $\mathcal{R}_{n,m}$ are
drawn with replacement according to $\pi_{\beta, n}(\cdot)$:
\begin{equation}
    \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
    =
    \prod_{k = 1}^{l_m}
        \pi_{\beta, n}(R(k)).
\end{equation}
Under both $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$, the components
of $\mathcal{R}_{n,m}$ are unique.
The law $\mathbb{P}_\beta$ designates that 
$\mathcal{R}_{n,m}$ is drawn from $\tilde{\mathbb{P}}_\beta$
conditionally on having unique components;
the law $\mathbb{P}^1_\beta$ designates that the components of
$\mathcal{R}_{n,m}$ are drawn one at a time without replacement according to
weights $\pi_{\beta,n}(\cdot)$.  Formally, define $|\cdot|$ to be the number
of unique components in a vector.  Then,
\begin{align}
    \mathbb{P}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
        &=
        \frac{
            \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
            I_{\{|R| = l_m\}}
        }{
            \sum_{R' \in \mathcal{J}_n^{l_m}}
                \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R' \big\}
                I_{\{|R'| = l_m\}}                
        } \\
\intertext{and}
    \mathbb{P}^1_\beta\big\{ \mathcal{R}_{n,m} = R \big\}
        &=
        \frac{1}{l_m !}
        \sum_{\sigma}
        \prod_{k=1}^{l_m}
            \frac{
                \pi_{\beta,n}\big(R(\sigma(k))\big)
            }{
                1 - \sum_{k' = 1}^{k - 1} \pi_{\beta,n}\big(R(\sigma(k'))\big)
            },
\end{align}
where $\sigma$ ranges over all permutations of $\{ 1, ..., l_m \}$.


We wish to estimate $\beta$ after observing covariates
$\{ x_{n,j} : 1 \leq j \leq J_n \}$ and samples
$\{\mathcal{R}_{n,m} : 1 \leq m \leq n\}$.  In the setting driven by
$\tilde{\mathbb{P}}_\beta$, the maximum likelihood estimate is easy to
obtain.  In $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$, though, the
likelihood and its derivatives have combinatorially many terms
and can be prohibitively expensive to optimize.

The likelihood corresponding to $\tilde{\mathbb{P}}_\beta$ is given by
\[
    \tilde{L}_n(\beta ; R_1, \ldots, R_n)
    =
    \prod_{m=1}^{n}
        \tilde{\mathbb{P}}_\beta\big\{ \mathcal{R}_{n,m} = R_m \}.
\]
Let $\hat \beta_n$ be the value that maximizes
\(
    \tilde{L}_n( \cdot\, ; \mathcal{R}_{n,1}, \ldots, \mathcal{R}_{n,n}).
\)
As $n$ increases, under $\tilde{\mathbb{P}}_\beta$ standard likelihood
theory shows that $\hat \beta_n$ converges in probability to $\beta$ and
that $\sqrt{n}(\beta - \hat \beta_n)$ converges in distribution to a
mean-zero Gaussian random variable.  This fails to hold generally under
$\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$.  However, it turns out
that if $J_n$ diverges sufficiently fast, then the bias in $\hat \beta_n$ is
of order $\tfrac{1}{\sqrt{n}}$, and $\sqrt{n}(\beta - \hat \beta_n)$ still
has a Gaussian limit.

\begin{theorem}
    Under $\mathbb{P}_\beta$ and $\mathbb{P}^1_\beta$,
    if $\max_{j \in \mathcal{J}_n} \|x_{n,j}\|_2$,
    \(
        \frac{1}{n} \sum_{m=1}^n l_m^2,
    \)
    and
    \(
        \frac{n}{J_n^2}
    \)
    are uniformly bounded in $n$, then $\hat \beta_n$ is a
    $\sqrt{n}$-consistent estimate of $\beta$.
\end{theorem}


\bibliographystyle{imsart-number}
\bibliography{iproc-sources}

\end{document}
